{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and initialize TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import spectral\n",
    "import shelve\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Configuration ---\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 600 # Set a large number, EarlyStopping will find the best\n",
    "LEARNING_RATE = 1e-4\n",
    "PATIENCE = 30 # For Early Stopping\n",
    "IGNORE_VALUE = -1 # Integer value to represent 'N' or ignored labels\n",
    "\n",
    "# Task names (consistent keys/output layer names)\n",
    "TASK_NAMES = ['plant', 'age', 'part', 'health', 'lifecycle']\n",
    "OUTPUT_NAMES = [f\"{task}_output\" for task in TASK_NAMES]\n",
    "\n",
    "# Loss weights (weighting between tasks, start equal)\n",
    "LOSS_WEIGHTS = {\n",
    "    'plant_output': 1.0,\n",
    "    'age_output': 1.0,\n",
    "    'part_output': 1.0,\n",
    "    'health_output': 1.0,\n",
    "    'lifecycle_output': 1.0\n",
    "}\n",
    "\n",
    "print(f\"Using TensorFlow version: {tf.__version__}\")\n",
    "# Optional: Configure GPU memory growth if needed\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Configured memory growth for {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALER_FILEPATH = 'data/pkl/scaler_10-23-25_0.pkl'\n",
    "CHECKPOINT_FILEPATH = 'data/checkpoints/best_spectral_model.weights_10-23-25_0.h5'\n",
    "MODEL_FILEPATH = 'data/checkpoints/model_10-23-25_0.keras'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load full spectral library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('util/')\n",
    "import importlib\n",
    "\n",
    "import util_scripts as util\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "MONGO_DBR_URI = os.getenv('MONGO_DBR_URI')\n",
    "\n",
    "reload_data_driver = False\n",
    "\n",
    "if (reload_data_driver):\n",
    "\n",
    "    records = []\n",
    "\n",
    "    # Create a new client and connect to the server\n",
    "    client = MongoClient(MONGO_DBR_URI, server_api=ServerApi('1'))\n",
    "\n",
    "    # Send a ping to confirm a successful connection\n",
    "    try:\n",
    "        client.admin.command('ping')\n",
    "        print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    db = client[\"upwins_db\"]\n",
    "    view_name = \"spectral_library\"\n",
    "    spectral_library = db[view_name]\n",
    "\n",
    "    records = spectral_library.find()\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_pickle('data/pkl/library_with_Genus_species.pkl')\n",
    "\n",
    "else:\n",
    "    df = pd.read_pickle('data/pkl/library_with_Genus_species.pkl')\n",
    "    #df = pd.read_pickle('data/pkl/library.pkl')\n",
    "\n",
    "sc = util.SpectralCollection(df)\n",
    "wl_lib = sc.wl\n",
    "name = sc.name\n",
    "spectra = sc.spectra\n",
    "print(sc.spectra.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Nano imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fname = 'data/morven_4000/raw_4000_or_ref.img'\n",
    "#fname_hdr = 'data/morven_4000/raw_4000_or_ref.hdr'\n",
    "\n",
    "fname = 'data/morven_9-2025/raw_55691_or_ref.img'\n",
    "fname_hdr = 'data/morven_9-2025/raw_55691_or_ref.hdr'\n",
    "\n",
    "#fname = 'data/5-8-2025/100133_Allied_05_08_2025_2015_06_04_17_50_15/raw_0_ref'\n",
    "#fname_hdr = 'data/5-8-2025/100133_Allied_05_08_2025_2015_06_04_17_50_15/raw_0_ref.hdr'\n",
    "\n",
    "# Open the image and read into an array\n",
    "im = spectral.envi.open(fname_hdr, fname)\n",
    "wl_img = np.asarray(im.bands.centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the image into memory\n",
    "im.Arr = im.load()\n",
    "print(f'Shape of Im.Arr = {im.Arr.shape}')\n",
    "im.List = np.reshape(im.Arr, (im.nrows*im.ncols, im.nbands))\n",
    "print(f'Shape of im.List = {im.List.shape}')\n",
    "\n",
    "valid_pixel_mask = np.sum(im.List, axis=1)>0\n",
    "\n",
    "dataList = im.List[valid_pixel_mask, :]\n",
    "print(f'Shape of dataList = {dataList.shape}')\n",
    "nr = im.nrows\n",
    "nc = im.ncols\n",
    "nb = im.nbands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL: Get target bands for resampling from ROI (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_filepath = 'data/pkl/rois_labeled/crisfield/Crisfield_October_Training_ROIs_Img14_0513_Genus_spec.pkl'\n",
    "\n",
    "with open(roi_filepath, 'rb') as f:\n",
    "    roiData = pickle.load(f)\n",
    "    roi_df = roiData.df # a DataFrame holding all the data for the ROI\n",
    "\n",
    "roi_spectra = roi_df.to_numpy()[:,4:]\n",
    "roi_spectra = roi_spectra.astype(np.float32)\n",
    "\n",
    "wl_roi = roi_df.columns.to_numpy()[4:]\n",
    "\n",
    "print(\"ROI target bands count: \", len(wl_roi))\n",
    "\n",
    "# set wl_img = to wl_roi for use in the rest of the code\n",
    "wl_img = wl_roi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample library to match input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BandResampler, which is a function that resamples spectra from one source to match a different source.\n",
    "# See: https://www.spectralpython.net/class_func_ref.html?highlight=resampling#spectral.algorithms.resampling.BandResampler\n",
    "# Inputs: the first input is the wavelengths for the spectra that you are going to resample, the second input is the wavelengths that you want to resample to.\n",
    "resampler = spectral.BandResampler(wl_lib, wl_img)\n",
    "spectra_resampled = resampler(spectra.T).T\n",
    "\n",
    "print(f'The shape of the spectral library is {spectra_resampled.shape}.')\n",
    "print(f'({spectra_resampled.shape[0]} spectra with {spectra_resampled.shape[1]} bands.)')\n",
    "\n",
    "spectra = spectra_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare ROI data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def find_roi_files(root_dir):\n",
    "    string_list = ['.pkl', 'roi']\n",
    "    \n",
    "    matching_files = []\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        for filename in files:\n",
    "            if all(string in filename.lower() for string in string_list):\n",
    "                matching_files.append(os.path.join(root, filename))\n",
    "    return matching_files\n",
    "\n",
    "# Project codes for labeling ROI data\n",
    "# **IMPORTANT**: ROIs should be named using the **same** naming convention used to label ASD files \n",
    "\n",
    "plant_codes = {\n",
    "    'Ammo_bre': ['Ammophila', 'breviligulata', 'American Beachgrass', 'grass', 'https://en.wikipedia.org/wiki/Ammophila_breviligulata'],\n",
    "    'Chas_lat': ['Chasmanthium', 'latifolium', 'River Oats', 'grass', 'https://en.wikipedia.org/wiki/Chasmanthium_latifolium'],\n",
    "    'Pani_ama': ['Panicum', 'amarum', 'Coastal Panic Grass', 'grass', 'https://en.wikipedia.org/wiki/Panicum_amarum'],\n",
    "    'Pani_vir': ['Panicum', 'virgatum', 'Switch Grass', 'grass', 'https://en.wikipedia.org/wiki/Panicum_virgatum'],\n",
    "    'Soli_sem': ['Solidago', 'sempervirens', 'Seaside Goldenrod', 'succulent', 'https://en.wikipedia.org/wiki/Chasmanthium_latifolium'],\n",
    "    'Robi_his': ['Robinia', 'hispida', 'Bristly locust', 'shrub', 'https://en.wikipedia.org/wiki/Robinia_hispida'],\n",
    "    'More_pen': ['Morella', 'pennsylvanica', 'Bristly locust', 'shrub', 'https://en.wikipedia.org/wiki/Myrica_pensylvanica'],    \n",
    "    'Rosa_rug': ['Rosa', 'rugosa', 'Sandy Beach Rose', 'shrub', 'https://en.wikipedia.org/wiki/Rosa_rugosa'],\n",
    "    'Cham_fas': ['Chamaecrista', 'fasciculata', 'Partridge Pea', 'legume', 'https://en.wikipedia.org/wiki/Chamaecrista_fasciculata'],\n",
    "    'Soli_rug': ['Solidago', 'rugosa', 'Wrinkleleaf goldenrod', 'shrub', 'https://en.wikipedia.org/wiki/Solidago_rugosa'],\n",
    "    'Bacc_hal': ['Baccharis', 'halimifolia', 'Groundseltree', 'shrub', 'https://en.wikipedia.org/wiki/Baccharis_halimifolia'],\n",
    "    'Iva_fru_': ['Iva', 'frutescens', 'Jesuits Bark ', 'shrub', 'https://en.wikipedia.org/wiki/Iva_frutescens'],\n",
    "    'Ilex_vom': ['Ilex', 'vomitoria', 'Yaupon Holly', 'evergreen shrub', 'https://en.wikipedia.org/wiki/Ilex_vomitoria'],\n",
    "    'Genus_spe': ['Genus', 'species', 'vegetation', 'background', '']\n",
    "}  \n",
    "age_codes = {  \n",
    "    'PE': ['Post Germination Emergence', 'PE'],\n",
    "\t#'RE': ['Re-emergence', 'RE'],\n",
    "    #'RE': ['Year 1 growth', '1G'],\n",
    "\t#'E': ['Emergence (from seed)', 'E'],\n",
    "    'E': ['Post Germination Emergence', 'PE'],\n",
    "\t#'D': ['Dormant', 'D'],\n",
    "\t'1G': ['Year 1 growth', '1G'],\n",
    "    '2G': ['Year 2 growth', '2G'],\n",
    "\t#'1F': ['Year 1 Flowering', '1F'],\n",
    "    'J': ['Juvenile', 'J'],\n",
    "\t'M': ['Mature', 'M']\n",
    "}\n",
    "principal_part_codes = {  \n",
    "    'MX': ['Mix', 'MX'],\n",
    "    #'S': ['Seed', 'SE'],\n",
    "\t#'SA': ['Shoot Apex', 'SA'],\n",
    "    'SA': ['Internode Stem', 'ST'],\n",
    "\t'L': ['Leaf/Blade', 'L'],\n",
    "\t#'IS': ['Internode Stem', 'IS'],\n",
    "    'ST': ['Internode Stem', 'ST'],\n",
    "    'SP': ['Sprout', 'SP'],\n",
    "\t#'CS': ['Colar Sprout', 'CS'],\n",
    "    'CS': ['Sprout', 'SP'],\n",
    "\t#'RS': ['Root Sprout', 'RS'],\n",
    "    'RS': ['Sprout', 'SP'],\n",
    "\t'LG': ['Lignin', 'LG'],\n",
    "\t'FL': ['Flower', 'FL'],\n",
    "    #'B': ['Blade', 'B'],\n",
    "\t'B': ['Leaf/Blade', 'L'],\n",
    "    'FR': ['Fruit', 'FR'],\n",
    "\t#'S': ['Seed', 'SE'], #moved above because 'S' is in other codes; this is an old code\n",
    "    'SE': ['Seed', 'SE'],\n",
    "\t#'St': ['Stalk', 'St']\n",
    "}\n",
    "health_codes = {\n",
    "    'MH': ['Healthy/Unhealthy Mix', 'MH'],\n",
    "\t'DS': ['Drought Stress', 'DS'],\n",
    "\t'SS': ['Salt Stress (soak)', 'SS'],\n",
    "    'SY': ['Salt Stress (spray)', 'SY'],\n",
    "\t'S': ['Stressed', 'S'],\n",
    "    'LLRZ': ['LLRZ Lab Stress', 'LLRZ'],\n",
    "\t#'D': ['Dormant', 'D'],\n",
    "    'R': ['Rust', 'R'],\n",
    "    'H': ['Healthy', 'H']\n",
    "}\n",
    "\n",
    "lifecycle_codes = { \n",
    "\t'D': ['Dormant', 'D'],\n",
    "    'RE': ['Re-emergence', 'RE'],\n",
    "    'FLG': ['Flowering', 'FLG'],\n",
    "    'FRG': ['Fruiting', 'FRG'],\n",
    "    \"FFG\": ['Fruiting and Flowering', 'FFG'],\n",
    "    'N': ['Neither', 'N']\n",
    "}\n",
    "\n",
    "# data lists\n",
    "\n",
    "d_spectra = []\n",
    "d_plant = []\n",
    "d_part = []\n",
    "d_health = []\n",
    "d_age = []\n",
    "d_lifecycle = []\n",
    "\n",
    "yd_all_dict = {\n",
    "    'plant': d_plant,\n",
    "    'age': d_age,\n",
    "    'part': d_part,\n",
    "    'health': d_health,\n",
    "    'lifecycle': d_lifecycle\n",
    "}\n",
    "\n",
    "code_category_dict = {\n",
    "    'plant': plant_codes,\n",
    "    'age': age_codes,\n",
    "    'part': principal_part_codes,\n",
    "    'health': health_codes,\n",
    "    'lifecycle': lifecycle_codes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find ROI data\n",
    "\n",
    "roi_files = find_roi_files('data/pkl/rois_labeled')\n",
    "\n",
    "print(f\"Number of ROI files found: {len(roi_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare ROI data\n",
    "\n",
    "# ==============================================================================\n",
    "# Helper Function for Stratified Sampling\n",
    "# ==============================================================================\n",
    "def stratified_sample_with_min_per_roi(spectra_df, min_per_roi=50, total_samples=300):\n",
    "    \"\"\"\n",
    "    Selects a subset of labeled ROI pixels from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        spectra_df (pd.DataFrame): DataFrame containing spectral data and a 'roi_name' column.\n",
    "        min_per_roi (int): The minimum number of pixels to select from each ROI.\n",
    "        total_samples (int): The total number of pixels to select.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the selected subset of ROI pixels.\n",
    "    \"\"\"\n",
    "    # Group by ROI and sample a minimum number of pixels from each.\n",
    "    # If an ROI has fewer pixels than min_per_roi, all its pixels are taken.\n",
    "    guaranteed_samples = spectra_df.groupby('roi_name').apply(\n",
    "        lambda x: x.sample(n=min(len(x), min_per_roi))\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # If a total sample size is specified and it's larger than the guaranteed sample\n",
    "    if total_samples and total_samples > len(guaranteed_samples):\n",
    "        remaining_to_select = total_samples - len(guaranteed_samples)\n",
    "        \n",
    "        # Create a pool of remaining pixels by excluding those already selected\n",
    "        # We use the DataFrame index to identify unique rows\n",
    "        remaining_pixels_df = spectra_df.drop(guaranteed_samples.index)\n",
    "\n",
    "        # If there are enough remaining pixels, sample from them\n",
    "        if remaining_to_select > 0 and not remaining_pixels_df.empty:\n",
    "            num_to_sample_from_remaining = min(remaining_to_select, len(remaining_pixels_df))\n",
    "            additional_samples = remaining_pixels_df.sample(n=num_to_sample_from_remaining)\n",
    "            \n",
    "            # Combine the guaranteed and additional samples\n",
    "            final_selection_df = pd.concat([guaranteed_samples, additional_samples])\n",
    "        else:\n",
    "            final_selection_df = guaranteed_samples\n",
    "    else:\n",
    "        final_selection_df = guaranteed_samples\n",
    "\n",
    "    return final_selection_df\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Data Processing and Sampling Logic\n",
    "# ==============================================================================\n",
    "\n",
    "# --- STAGE 1: Aggregate all ROI data from all files ---\n",
    "\n",
    "# This list will hold DataFrames of processed ROI data from each file\n",
    "all_rois_data_list = []\n",
    "resample_rois = True\n",
    "\n",
    "# Assuming roi_files is a list of your file paths\n",
    "for roi_filename in roi_files:\n",
    "    # Unpickling the dictionary\n",
    "    with open(roi_filename, 'rb') as f:\n",
    "        roiData = pickle.load(f)\n",
    "        roi_df = roiData.df  # a DataFrame holding all the data for the ROI\n",
    "\n",
    "    roi_spectra = roi_df.to_numpy()[:, 4:].astype(np.float32)\n",
    "    roi_spectra_names = roi_df['Name'].to_numpy() # These are the labels for each pixel\n",
    "\n",
    "    if resample_rois:\n",
    "        wl_roi = roi_df.columns.to_numpy()[4:]\n",
    "        resampler_roi = spectral.BandResampler(wl_roi, wl_img)\n",
    "        print(\"Number of bands: \", len(wl_roi))\n",
    "        roi_spectra_resampled = resampler_roi(roi_spectra.T).T\n",
    "        roi_spectra = roi_spectra_resampled\n",
    "\n",
    "    if \"crisfield\" in roi_filename.lower():\n",
    "        print(\"Running pixel-wise normalization for 'crisfield'\")\n",
    "        min_vals_pixel = np.min(roi_spectra, axis=1, keepdims=True)\n",
    "        max_vals_pixel = np.max(roi_spectra, axis=1, keepdims=True)\n",
    "        range_vals_pixel = max_vals_pixel - min_vals_pixel\n",
    "        range_vals_pixel[range_vals_pixel == 0] = 1\n",
    "        roi_spectra = (roi_spectra - min_vals_pixel) / range_vals_pixel\n",
    "\n",
    "    # Create a temporary DataFrame for the current file's data\n",
    "    temp_df = pd.DataFrame(roi_spectra)\n",
    "    temp_df['roi_name'] = roi_spectra_names\n",
    "    \n",
    "    # Add the processed data to our master list\n",
    "    all_rois_data_list.append(temp_df)\n",
    "    \n",
    "    print(f\"Processed and aggregated data from {roi_filename}\")\n",
    "\n",
    "# Concatenate all data into a single master DataFrame\n",
    "master_roi_df = pd.concat(all_rois_data_list, ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ROI Data Aggregation Complete.\")\n",
    "print(f\"Total number of ROI pixels collected: {len(master_roi_df)}\")\n",
    "print(\"Pixel distribution before sampling:\")\n",
    "print(master_roi_df['roi_name'].value_counts().to_string())\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- STAGE 2: Perform Stratified Sampling on the aggregated data ---\n",
    "\n",
    "# Define your sampling parameters\n",
    "MIN_PIXELS_PER_ROI = 50 \n",
    "TOTAL_ROI_PIXELS_TO_SELECT = 300\n",
    "\n",
    "print(f\"Starting stratified sampling with min {MIN_PIXELS_PER_ROI}/ROI and a target total of {TOTAL_ROI_PIXELS_TO_SELECT}...\")\n",
    "\n",
    "selected_rois_df = stratified_sample_with_min_per_roi(\n",
    "    master_roi_df,\n",
    "    min_per_roi=MIN_PIXELS_PER_ROI,\n",
    "    total_samples=TOTAL_ROI_PIXELS_TO_SELECT\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Stratified Sampling Complete.\")\n",
    "print(f\"Total number of pixels selected: {len(selected_rois_df)}\")\n",
    "print(\"Pixel distribution after sampling:\")\n",
    "print(selected_rois_df['roi_name'].value_counts().to_string())\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- STAGE 3: Populate final data structures with the selected subset ---\n",
    "\n",
    "# Extract the selected spectra and names for final processing\n",
    "selected_spectra = selected_rois_df.drop(columns=['roi_name']).to_numpy()\n",
    "selected_names = selected_rois_df['roi_name'].to_numpy()\n",
    "\n",
    "for i in range(len(selected_spectra)):\n",
    "    roi_spectrum = selected_spectra[i]\n",
    "    name = selected_names[i]\n",
    "\n",
    "    # Append the selected spectrum\n",
    "    d_spectra.append(roi_spectrum)\n",
    "    \n",
    "    # --- This block is your original metadata parsing logic ---\n",
    "    if name == 'Genus_spe_N_N_N_N':\n",
    "        new_name = 'Genus_spe_MX_N_N_N'\n",
    "        print(f\"Renaming {name} to {new_name}\")\n",
    "        name = new_name\n",
    "\n",
    "    if name[-1] != '_':\n",
    "        name = name + '_'\n",
    "    \n",
    "    class_data_dict = {}\n",
    "    \n",
    "    for cat, codes in code_category_dict.items():\n",
    "        class_data_dict[cat] = 'N'\n",
    "        for key, value in codes.items():\n",
    "            if cat == 'plant':\n",
    "                if (name[:8].lower() == key.lower()) or (name[:9].lower() == key.lower()):\n",
    "                    class_data_dict[cat] = value[0] + '_' + value[1]\n",
    "            else:\n",
    "                if '_' + key + '_' in name:\n",
    "                    class_data_dict[cat] = value[1]\n",
    "    \n",
    "    # Append the parsed metadata for the selected spectrum\n",
    "    for key in yd_all_dict:\n",
    "        yd_all_dict[key].append(class_data_dict[key])\n",
    "\n",
    "print(\"Final `d_spectra` and `yd_all_dict` have been populated with the sampled data.\")\n",
    "\n",
    "d_spectra = np.asarray(d_spectra)\n",
    "print(d_spectra.shape)\n",
    "\n",
    "for key in yd_all_dict:\n",
    "    yd_all_dict[key] = np.asarray(yd_all_dict[key])\n",
    "    print(key, yd_all_dict[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add selected ROI spectra to library spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_to_select = 250  # Here (250 or 300) randomly selected ROI spectra are added to the full spectral library\n",
    "\n",
    "# total_indices = d_spectra.shape[0]\n",
    "\n",
    "# selected_indices = np.random.choice(total_indices, size=num_to_select, replace=False)\n",
    "\n",
    "# selected_d_spectra = d_spectra[selected_indices]\n",
    "# print(\"\\nNew d_spectra shape:\", selected_d_spectra.shape)\n",
    "\n",
    "# selected_yd_all_dict = {}\n",
    "# for key in yd_all_dict:\n",
    "#     # Select the corresponding rows/elements from the original array\n",
    "#     selected_yd_all_dict[key] = yd_all_dict[key][selected_indices]\n",
    "#     print(f\"New {key} shape: {selected_yd_all_dict[key].shape}\")\n",
    "\n",
    "selected_d_spectra = d_spectra\n",
    "selected_yd_all_dict = yd_all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra = np.concatenate((spectra, selected_d_spectra))\n",
    "\n",
    "sc.name = np.concatenate((sc.name, selected_yd_all_dict['plant']))\n",
    "sc.age = np.concatenate((sc.age, selected_yd_all_dict['age']))\n",
    "sc.principle_part = np.concatenate((sc.principle_part, selected_yd_all_dict['part']))\n",
    "sc.health = np.concatenate((sc.health, selected_yd_all_dict['health']))\n",
    "sc.lifecycle = np.concatenate((sc.lifecycle, selected_yd_all_dict['lifecycle']))\n",
    "print(f\"New spectra shape: {spectra.shape}\")\n",
    "print(f\"New sc.name shape: {sc.name.shape}\")\n",
    "print(f\"New sc.age shape: {sc.age.shape}\")\n",
    "print(f\"New sc.principle_part shape: {sc.principle_part.shape}\")\n",
    "print(f\"New sc.health shape: {sc.health.shape}\")\n",
    "print(f\"New sc.lifecycle shape: {sc.lifecycle.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_integer_labels(data_array, label_array):\n",
    "    \"\"\"\n",
    "    Assigns integer values from a label array to corresponding values in a data array.\n",
    "\n",
    "    Args:\n",
    "        data_array:  The array containing the string or categorical data (e.g., Yn_o, Ypp_o, etc.).\n",
    "        label_array: The array of unique labels (e.g., y_plant_labels, y_part_labels, etc.).\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array containing the integer representations of the data based on the labels.\n",
    "    \"\"\"\n",
    "    mapping = {label: i for i, label in enumerate(label_array)}\n",
    "    integer_array = np.array([mapping[x] for x in data_array])\n",
    "    return integer_array\n",
    "\n",
    "\n",
    "X_all = spectra\n",
    "\n",
    "y_plant_labels = np.unique(sc.name)\n",
    "y_part_labels = np.unique(sc.principle_part)\n",
    "y_health_labels = np.unique(sc.health)\n",
    "y_age_labels = np.unique(sc.age)\n",
    "y_lifecycle_labels = np.unique(sc.lifecycle)\n",
    "\n",
    "label_maps = {'plant': y_plant_labels, 'age': y_age_labels, 'part': y_part_labels, 'health': y_health_labels, 'lifecycle': y_lifecycle_labels}\n",
    "\n",
    "# Store the label_maps dictionary in a shelve key-value store\n",
    "with shelve.open('data/shelve/label_maps_store') as db:\n",
    "    db['label_maps'] = label_maps\n",
    "\n",
    "# Number of classes for each task\n",
    "n_plant_classes = len(y_plant_labels)\n",
    "n_age_classes = len(y_age_labels)\n",
    "n_part_classes = len(y_part_labels)\n",
    "n_health_classes = len(y_health_labels)\n",
    "n_lifecycle_classes = len(y_lifecycle_labels)\n",
    "\n",
    "Yn_int = assign_integer_labels(sc.name, y_plant_labels)\n",
    "Yp_int = assign_integer_labels(sc.principle_part, y_part_labels)\n",
    "Yh_int = assign_integer_labels(sc.health, y_health_labels)\n",
    "Ya_int = assign_integer_labels(sc.age, y_age_labels)\n",
    "Yl_int = assign_integer_labels(sc.lifecycle, y_lifecycle_labels)\n",
    "\n",
    "def replace_n_with_ignore_val(integer_array, label_array):\n",
    "    \"\"\"\n",
    "    Replaces integer values with -1 in an integer array where the corresponding\n",
    "    label in the label array is 'N'.\n",
    "\n",
    "    Args:\n",
    "        integer_array: The integer-encoded data array (e.g., Yn_int, Ypp_int, etc.).\n",
    "        label_array: The array of unique labels (e.g., y_plant_labels, y_part_labels, etc.).\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array with values replaced by -1 where the label is 'N'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        n_index = np.where(label_array == 'N')[0][0]  # Find the index of 'N'\n",
    "        integer_array[integer_array == n_index] = IGNORE_VALUE  # Replace values\n",
    "        return integer_array\n",
    "    except IndexError:\n",
    "        print(\"'N' not found in the label array.  No replacements made.\")\n",
    "        return integer_array\n",
    "\n",
    "\n",
    "y_plant = replace_n_with_ignore_val(Yn_int, y_plant_labels)\n",
    "y_part = replace_n_with_ignore_val(Yp_int, y_part_labels)\n",
    "y_age = replace_n_with_ignore_val(Ya_int, y_age_labels)\n",
    "y_health = replace_n_with_ignore_val(Yh_int, y_health_labels)\n",
    "\n",
    "y_lifecycle = Yl_int\n",
    "\n",
    "y_all_dict_original = {'plant': y_plant, 'part': y_part, 'age': y_age, 'health': y_health, 'lifecycle': y_lifecycle}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Splitting (Train/Validation/Test on features and ORIGINAL labels) ---\n",
    "indices = np.arange(len(X_all))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.15, random_state=42)\n",
    "train_indices, val_indices = train_test_split(train_indices, test_size=0.1765, random_state=42)\n",
    "X_train, X_val, X_test = X_all[train_indices], X_all[val_indices], X_all[test_indices]\n",
    "y_train_dict_orig = {task: y_all_dict_original[task][train_indices] for task in TASK_NAMES}\n",
    "y_val_dict_orig = {task: y_all_dict_original[task][val_indices] for task in TASK_NAMES}\n",
    "y_test_dict_orig = {task: y_all_dict_original[task][test_indices] for task in TASK_NAMES}\n",
    "print(f\"Data split sizes: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
    "\n",
    "test_indices_library = test_indices[test_indices < len(sc.spectra)]\n",
    "print(\"Number of library spectra in test set: \", len(test_indices_library))\n",
    "test_indices_rois = test_indices[test_indices >= len(sc.spectra)]\n",
    "print(\"Number of ROI spectra in test set: \", len(test_indices_rois))\n",
    "\n",
    "X_test_library = X_all[test_indices_library]\n",
    "X_test_rois = X_all[test_indices_rois]\n",
    "\n",
    "y_test_dict_library = {task: y_all_dict_original[task][test_indices_library] for task in TASK_NAMES}\n",
    "y_test_dict_rois = {task: y_all_dict_original[task][test_indices_rois] for task in TASK_NAMES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preprocessing (Standardization) ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "scaler_filename = SCALER_FILEPATH\n",
    "with open(scaler_filename, 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "print(f\"Scaler saved to {scaler_filename}\")\n",
    "\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Reshape for Conv1D: (batch, steps, channels=1) ---\n",
    "X_train_scaled = X_train_scaled[..., np.newaxis]\n",
    "X_val_scaled = X_val_scaled[..., np.newaxis]\n",
    "X_test_scaled = X_test_scaled[..., np.newaxis]\n",
    "#X_train_scaled = X_train[..., np.newaxis]\n",
    "#X_val_scaled = X_val[..., np.newaxis]\n",
    "#X_test_scaled = X_test[..., np.newaxis]\n",
    "\n",
    "print(f\"Feature shapes: Train={X_train_scaled.shape}, Val={X_val_scaled.shape}, Test={X_test_scaled.shape}\")\n",
    "\n",
    "# --- Prepare Labels for Keras Fit/Evaluate (Replace IGNORE_VALUE with 0) ---\n",
    "y_train_dict_keras = {f\"{task}_output\": np.maximum(0, y_train_dict_orig[task]) for task in TASK_NAMES}\n",
    "y_val_dict_keras = {f\"{task}_output\": np.maximum(0, y_val_dict_orig[task]) for task in TASK_NAMES}\n",
    "y_test_dict_keras = {f\"{task}_output\": np.maximum(0, y_test_dict_orig[task]) for task in TASK_NAMES}\n",
    "print(f\"Example Keras 'age_output' labels for training: {y_train_dict_keras['age_output'][:20]}\")\n",
    "\n",
    "# --- Create Sample Weight Dictionaries ---\n",
    "sample_weights_train = {f\"{task}_output\": (y_train_dict_orig[task] != IGNORE_VALUE).astype(np.float32) for task in TASK_NAMES}\n",
    "sample_weights_val = {f\"{task}_output\": (y_val_dict_orig[task] != IGNORE_VALUE).astype(np.float32) for task in TASK_NAMES}\n",
    "sample_weights_test = {f\"{task}_output\": (y_test_dict_orig[task] != IGNORE_VALUE).astype(np.float32) for task in TASK_NAMES}\n",
    "print(f\"Example train sample weights for 'age_output': {sample_weights_train['age_output'][:20]}\")\n",
    "\n",
    "\n",
    "X_test_library_scaled = scaler.transform(X_test_library)\n",
    "X_test_library_scaled = X_test_library_scaled[..., np.newaxis]\n",
    "\n",
    "X_test_rois_scaled = scaler.transform(X_test_rois)\n",
    "X_test_rois_scaled = X_test_rois_scaled[..., np.newaxis]\n",
    "\n",
    "y_test_dict_library_keras = {f\"{task}_output\": np.maximum(0, y_test_dict_library[task]) for task in TASK_NAMES}\n",
    "y_test_dict_rois_keras = {f\"{task}_output\": np.maximum(0, y_test_dict_rois[task]) for task in TASK_NAMES}\n",
    "\n",
    "sample_weights_test_library = {f\"{task}_output\": (y_test_dict_library[task] != IGNORE_VALUE).astype(np.float32) for task in TASK_NAMES}\n",
    "sample_weights_test_rois = {f\"{task}_output\": (y_test_dict_rois[task] != IGNORE_VALUE).astype(np.float32) for task in TASK_NAMES}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a 1D CNN to capture local patterns (like absorption peaks or slopes across adjacent bands)\n",
    "\n",
    "def build_spectral_cnn(input_shape, n_plant, n_age, n_part, n_health, n_lifecycle):\n",
    "    inputs = keras.Input(shape=input_shape, name='spectrum_input')\n",
    "\n",
    "    # --- Shared Feature Extractor (1D CNN Backbone) ---\n",
    "    x = layers.Conv1D(filters=32, kernel_size=7, activation='relu', padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=3)(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    x = layers.Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=3)(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    x = layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=3)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    shared_features = layers.Flatten()(x)\n",
    "\n",
    "    # Optional shared dense layer\n",
    "    shared_features = layers.Dense(128, activation='relu')(shared_features)\n",
    "    shared_features = layers.BatchNormalization()(shared_features)\n",
    "    shared_features = layers.Dropout(0.5)(shared_features)\n",
    "\n",
    "    # --- Output Heads (One per task) ---\n",
    "    plant_output = layers.Dense(64, activation='relu')(shared_features)\n",
    "    plant_output = layers.Dense(n_plant, activation='softmax', name='plant_output')(plant_output)\n",
    "\n",
    "    age_output = layers.Dense(32, activation='relu')(shared_features)\n",
    "    age_output = layers.Dense(n_age, activation='softmax', name='age_output')(age_output)\n",
    "\n",
    "    part_output = layers.Dense(32, activation='relu')(shared_features)\n",
    "    part_output = layers.Dense(n_part, activation='softmax', name='part_output')(part_output)\n",
    "\n",
    "    health_output = layers.Dense(32, activation='relu')(shared_features)\n",
    "    health_output = layers.Dense(n_health, activation='softmax', name='health_output')(health_output)\n",
    "\n",
    "    lifecycle_output = layers.Dense(32, activation='relu')(shared_features)\n",
    "    lifecycle_output = layers.Dense(n_lifecycle, activation='softmax', name='lifecycle_output')(lifecycle_output)\n",
    "\n",
    "    # --- Build the Model ---\n",
    "    model = keras.Model(\n",
    "        inputs=inputs,\n",
    "        outputs={\n",
    "            'plant_output': plant_output,\n",
    "            'age_output': age_output,\n",
    "            'part_output': part_output,\n",
    "            'health_output': health_output,\n",
    "            'lifecycle_output': lifecycle_output\n",
    "        },\n",
    "        name=\"spectral_multi_task_cnn\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "input_shape = X_train_scaled.shape[1:]\n",
    "model = build_spectral_cnn(input_shape, n_plant_classes, n_age_classes, n_part_classes, n_health_classes, n_lifecycle_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "losses = {name: 'sparse_categorical_crossentropy' for name in OUTPUT_NAMES}\n",
    "metrics = {name: 'sparse_categorical_accuracy' for name in OUTPUT_NAMES}\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=losses,\n",
    "    loss_weights=LOSS_WEIGHTS,\n",
    "    metrics=metrics, # Standard metrics\n",
    "    weighted_metrics=metrics # Weighted metrics\n",
    ")\n",
    "\n",
    "print(\"\\nModel Compiled.\")\n",
    "print(f\"Losses: {model.loss}\")\n",
    "print(f\"Metrics: {model.metrics_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NN if weights are not loaded in the previous step\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=PATIENCE,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_dict_keras,           # Labels dictionary (keys match output names)\n",
    "    sample_weight=sample_weights_train, # Sample weights dictionary (keys match output names)\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val_scaled, y_val_dict_keras, sample_weights_val), # Also pass val weights\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Finished.\")\n",
    "\n",
    "model.save_weights(CHECKPOINT_FILEPATH)\n",
    "model.save(MODEL_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_maps = {\n",
    "    'plant': y_plant_labels,\n",
    "    'age': y_age_labels,\n",
    "    'part': y_part_labels,\n",
    "    'health': y_health_labels,\n",
    "    'lifecycle': y_lifecycle_labels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\nEvaluating on Test Set with Best Model (using sample weights)...\")\n",
    "\n",
    "results = model.evaluate(\n",
    "    X_test_scaled,\n",
    "    y_test_dict_keras,\n",
    "    sample_weight=sample_weights_test,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=0,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "print(f\"Overall Loss (Weighted Sum): {results['loss']:.4f}\")\n",
    "\n",
    "print(\"\\nTest Weighted Metrics (Accuracy ignoring invalid samples):\")\n",
    "for name in OUTPUT_NAMES:\n",
    "    metric_key = f\"{name}_weighted_sparse_categorical_accuracy\"\n",
    "    if metric_key in results:\n",
    "        print(f\"  {name.replace('_output', '').capitalize()}: {results[metric_key]:.4f}\")\n",
    "    else:\n",
    "         metric_key_alt = f\"weighted_{name}_sparse_categorical_accuracy\"\n",
    "         if metric_key_alt in results:\n",
    "              print(f\"  {name.replace('_output', '').capitalize()} (alt key): {results[metric_key_alt]:.4f}\")\n",
    "         else:\n",
    "              print(f\"  {name.replace('_output', '').capitalize()}: Weighted metric key not found in results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Test Set from Spectral Library only\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\nEvaluating on Test Set (Library) with Best Model (using sample weights)...\")\n",
    "\n",
    "results = model.evaluate(\n",
    "    X_test_library_scaled,\n",
    "    y_test_dict_library_keras,\n",
    "    sample_weight=sample_weights_test_library,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=0,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "print(f\"Overall Loss (Weighted Sum): {results['loss']:.4f}\")\n",
    "\n",
    "print(\"\\nTest Weighted Metrics (Accuracy ignoring invalid samples):\")\n",
    "for name in OUTPUT_NAMES:\n",
    "    metric_key = f\"{name}_weighted_sparse_categorical_accuracy\"\n",
    "    if metric_key in results:\n",
    "        print(f\"  {name.replace('_output', '').capitalize()}: {results[metric_key]:.4f}\")\n",
    "    else:\n",
    "         metric_key_alt = f\"weighted_{name}_sparse_categorical_accuracy\"\n",
    "         if metric_key_alt in results:\n",
    "              print(f\"  {name.replace('_output', '').capitalize()} (alt key): {results[metric_key_alt]:.4f}\")\n",
    "         else:\n",
    "              print(f\"  {name.replace('_output', '').capitalize()}: Weighted metric key not found in results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Test Set from ROIs only\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\nEvaluating on Test Set (ROIs) with Best Model (using sample weights)...\")\n",
    "\n",
    "results = model.evaluate(\n",
    "    X_test_rois_scaled,\n",
    "    y_test_dict_rois_keras,\n",
    "    sample_weight=sample_weights_test_rois,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=0,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "print(f\"Overall Loss (Weighted Sum): {results['loss']:.4f}\")\n",
    "\n",
    "print(\"\\nTest Weighted Metrics (Accuracy ignoring invalid samples):\")\n",
    "for name in OUTPUT_NAMES:\n",
    "    metric_key = f\"{name}_weighted_sparse_categorical_accuracy\"\n",
    "    if metric_key in results:\n",
    "        print(f\"  {name.replace('_output', '').capitalize()}: {results[metric_key]:.4f}\")\n",
    "    else:\n",
    "         metric_key_alt = f\"weighted_{name}_sparse_categorical_accuracy\"\n",
    "         if metric_key_alt in results:\n",
    "              print(f\"  {name.replace('_output', '').capitalize()} (alt key): {results[metric_key_alt]:.4f}\")\n",
    "         else:\n",
    "              print(f\"  {name.replace('_output', '').capitalize()}: Weighted metric key not found in results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Print Labeled Codes and Counts in Test Set for Each Species (checking for a balanced test set)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for i, val in enumerate(y_plant_labels):\n",
    "    \n",
    "    selected_indices = np.arange(len(y_test_dict_orig['plant']))\n",
    "    selected_indices = selected_indices[y_test_dict_orig['plant'] == i]\n",
    "\n",
    "    y_test_dict_keras_filtered = {key : y_test_dict_keras[key][selected_indices] for key in y_test_dict_keras.keys()}\n",
    "    sample_weights_test_filtered = {key : sample_weights_test[key][selected_indices] for key in sample_weights_test.keys()}\n",
    "\n",
    "    #print labels for values for all tasks of y_test_dict_keras['plant_output'][selected_indices]\n",
    "\n",
    "    #counts_dict = dict(zip(np.unique(y_test_dict_keras_filtered, return_counts=True)))\n",
    "\n",
    "    value_counts = {}\n",
    "\n",
    "    for key, arr in y_test_dict_keras_filtered.items():\n",
    "        value_counts[key] = Counter(arr)\n",
    "\n",
    "    print(\"Labeled Codes and Counts in Test Set for Each Species\")\n",
    "\n",
    "    for key, counts in value_counts.items():\n",
    "\n",
    "        key = key.replace('_output', '')\n",
    "        \n",
    "        print(f\" '{key}':\")\n",
    "        for value, count in counts.items():\n",
    "            print(f\"    {label_maps[key][value]}: {count} {'spectra' if key == 'plant' else ''}\")\n",
    "\n",
    "    results = model.evaluate(\n",
    "        X_test_scaled[selected_indices],\n",
    "        y_test_dict_keras_filtered,\n",
    "        sample_weight=sample_weights_test_filtered,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=0,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTest Set Evaluation Results for plant={val}:\")\n",
    "    print(f\"Overall Loss (Weighted Sum): {results['loss']:.4f}\")\n",
    "\n",
    "    print(\"\\nTest Weighted Metrics (Accuracy ignoring invalid samples):\")\n",
    "    for name in OUTPUT_NAMES:\n",
    "        metric_key = f\"{name}_weighted_sparse_categorical_accuracy\"\n",
    "        if metric_key in results:\n",
    "            print(f\"  {name.replace('_output', '').capitalize()}: {results[metric_key]:.4f}\")\n",
    "        else:\n",
    "            metric_key_alt = f\"weighted_{name}_sparse_categorical_accuracy\"\n",
    "            if metric_key_alt in results:\n",
    "                print(f\"  {name.replace('_output', '').capitalize()} (alt key): {results[metric_key_alt]:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {name.replace('_output', '').capitalize()}: Weighted metric key not found in results.\")\n",
    "    print(\"\\n____________________________________________________________\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Show confusion matrices for each task\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np # Ensure numpy is imported\n",
    "\n",
    "# --- Predictions on the Test Set ---\n",
    "print(\"\\nGenerating predictions on the test set...\")\n",
    "predictions = model.predict(X_test_scaled, batch_size=BATCH_SIZE)\n",
    "print(\"Predictions generated.\")\n",
    "\n",
    "# Ensure predictions is a dictionary if it's not already (Keras usually returns dict for multi-output)\n",
    "if not isinstance(predictions, dict):\n",
    "     # If the model output is a list, convert it back to a dict based on output names\n",
    "     output_layer_names = model.output_names # Or use OUTPUT_NAMES if they match exactly\n",
    "     predictions = dict(zip(output_layer_names, predictions))\n",
    "\n",
    "\n",
    "# # --- Create a mapping from task name to its labels ---\n",
    "# # Make sure these label arrays are accessible here\n",
    "# label_maps = {\n",
    "#     'plant': y_plant_labels,\n",
    "#     'age': y_age_labels,\n",
    "#     'part': y_part_labels,\n",
    "#     'health': y_health_labels,\n",
    "#     'lifecycle': y_lifecycle_labels\n",
    "# }\n",
    "\n",
    "# --- Generate and Plot Confusion Matrices ---\n",
    "print(\"\\nGenerating Confusion Matrices and Classification Reports...\")\n",
    "\n",
    "for task in TASK_NAMES:\n",
    "    output_name = f\"{task}_output\" # e.g., 'plant_output'\n",
    "\n",
    "    print(f\"\\n--- Task: {task.capitalize()} ---\")\n",
    "\n",
    "    # 1. Get True Labels (Original, includes integer mapping for 'N')\n",
    "    y_true_all = y_test_dict_orig[task]\n",
    "\n",
    "    # 2. Get Predicted Labels (Integers)\n",
    "    if output_name not in predictions:\n",
    "        print(f\"Warning: Output key '{output_name}' not found in model predictions. Skipping task '{task}'.\")\n",
    "        continue\n",
    "    y_pred_probs = predictions[output_name]\n",
    "    y_pred_all = np.argmax(y_pred_probs, axis=-1) # Get the class index with the highest probability\n",
    "\n",
    "    # 3. Get Sample Weights\n",
    "    weights = sample_weights_test[output_name]\n",
    "\n",
    "    # 4. Get the corresponding class names (labels)\n",
    "    original_labels = label_maps[task] # e.g., y_lifecycle_labels\n",
    "\n",
    "    # 5. *Conditional* Filtering and Label Definition\n",
    "    if task == 'lifecycle':\n",
    "        # --- Lifecycle Task: Include 'N' ---\n",
    "        # We don't filter samples based on weight, as weight=0 specifically marks 'N' here\n",
    "        # We want to see how 'N' is classified.\n",
    "        y_true_valid = y_true_all\n",
    "        y_pred_valid = y_pred_all\n",
    "\n",
    "        # Define the labels for the CM to include *all* original categories\n",
    "        # Note: y_true_valid might contain IGNORE_VALUE (-1) if 'N' was mapped to it.\n",
    "        # The confusion_matrix function handles this gracefully if -1 isn't in `labels`.\n",
    "        # So, we define `valid_indices` based on the original mapping.\n",
    "        valid_indices = list(range(len(original_labels))) # 0, 1, 2,... N_classes-1\n",
    "        valid_string_labels = list(original_labels) # Includes 'N' string\n",
    "\n",
    "        print(f\"Including 'N' category for {task.capitalize()}. Evaluating on {len(y_true_valid)} samples.\")\n",
    "\n",
    "    else:\n",
    "        # --- Other Tasks: Exclude 'N' ---\n",
    "        # Filter out ignored samples using the weights\n",
    "        valid_mask = (weights == 1.0) # Mask for samples that should NOT be ignored\n",
    "        y_true_valid = y_true_all[valid_mask]\n",
    "        y_pred_valid = y_pred_all[valid_mask]\n",
    "\n",
    "        # Define the labels for the CM *excluding* the 'N' category\n",
    "        try:\n",
    "            # Find the integer index originally assigned to 'N'\n",
    "            n_index = np.where(original_labels == 'N')[0][0]\n",
    "            valid_indices = [i for i in range(len(original_labels)) if i != n_index]\n",
    "            valid_string_labels = [label for i, label in enumerate(original_labels) if i != n_index]\n",
    "        except IndexError:\n",
    "             # 'N' category doesn't exist for this task, include all labels\n",
    "             print(f\"Note: 'N' category not found in labels for task '{task}'. Including all labels.\")\n",
    "             valid_indices = list(range(len(original_labels)))\n",
    "             valid_string_labels = list(original_labels)\n",
    "\n",
    "        print(f\"Excluding 'N' category for {task.capitalize()}. Evaluating on {len(y_true_valid)} samples.\")\n",
    "\n",
    "\n",
    "    # 6. Check if there are any valid samples left for this task\n",
    "    if len(y_true_valid) == 0:\n",
    "        print(f\"No samples to evaluate for task '{task}' after filtering (if applicable). Skipping CM.\")\n",
    "        continue\n",
    "\n",
    "    # 7. Determine the final set of labels/indices *present* in the data for the CM\n",
    "    # This is important because even if we define all labels, some might not\n",
    "    # appear in the specific y_true_valid/y_pred_valid subset.\n",
    "    present_true_labels = np.unique(y_true_valid)\n",
    "    present_pred_labels = np.unique(y_pred_valid)\n",
    "\n",
    "    # Combine present labels, but only keep those that were in our initial 'valid_indices' list\n",
    "    # (This prevents including the IGNORE_VALUE index (-1) if 'N' was mapped to it for lifecycle)\n",
    "    all_present_indices_in_data = np.unique(np.concatenate((present_true_labels, present_pred_labels)))\n",
    "    final_cm_indices = [idx for idx in valid_indices if idx in all_present_indices_in_data]\n",
    "    final_cm_labels = [label for idx, label in zip(valid_indices, valid_string_labels) if idx in final_cm_indices]\n",
    "\n",
    "\n",
    "    # Handle case where after filtering, no valid classes remain\n",
    "    if not final_cm_indices:\n",
    "         print(f\"No valid classes found in true/predicted labels for task '{task}' for CM. Skipping CM.\")\n",
    "         continue\n",
    "\n",
    "    # 8. Calculate Confusion Matrix using the *final* determined indices\n",
    "    # `labels=final_cm_indices` ensures the matrix axes match `final_cm_labels`\n",
    "    cm = confusion_matrix(y_true_valid, y_pred_valid, labels=final_cm_indices)\n",
    "\n",
    "    # 9. Plot Confusion Matrix\n",
    "    plt.figure(figsize=(max(8, len(final_cm_labels)*0.8), max(6, len(final_cm_labels)*0.6))) # Adjust size based on num labels\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=final_cm_labels,\n",
    "                yticklabels=final_cm_labels)\n",
    "    plt.title(f'Confusion Matrix - Task: {task.capitalize()}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 10. Print Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    try:\n",
    "        # Use zero_division=0 or 1 for handling classes with no true samples\n",
    "        report = classification_report(y_true_valid, y_pred_valid,\n",
    "                                   labels=final_cm_indices,\n",
    "                                   target_names=final_cm_labels,\n",
    "                                   zero_division=0)\n",
    "        print(report)\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not generate classification report for {task}: {e}\")\n",
    "        print(\"This might happen if predicted values contain labels not present in true values after filtering.\")\n",
    "        print(f\"Unique True values considered: {np.unique(y_true_valid)}\")\n",
    "        print(f\"Unique Pred values considered: {np.unique(y_pred_valid)}\")\n",
    "        print(f\"Indices used for report: {final_cm_indices}\")\n",
    "        print(f\"Labels used for report: {final_cm_labels}\")\n",
    "\n",
    "\n",
    "print(\"\\nFinished generating visualizations.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
