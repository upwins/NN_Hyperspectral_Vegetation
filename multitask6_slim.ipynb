{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and initialize TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import spectral\n",
    "import seaborn as sns\n",
    "\n",
    "import shelve\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION AND SETUP\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Constants for Model and Training ---\n",
    "TASK_NAMES = ['plant', 'age', 'part', 'health', 'lifecycle']\n",
    "OUTPUT_NAMES = [f\"{task}_output\" for task in TASK_NAMES]\n",
    "IGNORE_VALUE = -1\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "BATCH_SIZE = 32\n",
    "PRETRAIN_EPOCHS = 800         # 400 # Max epochs for pre-training (EarlyStopping will find the best)\n",
    "FINETUNE_EPOCHS = 200         # 200 # Max epochs for fine-tuning\n",
    "PATIENCE = 30                 # Patience for Early Stopping\n",
    "LEARNING_RATE = 1e-4          # Pre-training learning rate\n",
    "FT_LEARNING_RATE = 1e-4       # Fine-tuning learning rate\n",
    "\n",
    "# --- Configuration Flags ---\n",
    "EXPORT_TO_CSV = True          # Flag to save evaluation reports as CSV files\n",
    "\n",
    "# --- Loss weights (for weighting the contribution of each task to the total loss) ---\n",
    "LOSS_WEIGHTS = {\n",
    "    'plant_output': 1.0,\n",
    "    'age_output': 1.0,\n",
    "    'part_output': 1.0,\n",
    "    'health_output': 1.0,\n",
    "    'lifecycle_output': 1.0\n",
    "}\n",
    "\n",
    "# --- Directory Setup ---\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "os.makedirs('scalers', exist_ok=True)\n",
    "\n",
    "# --- Suppress TensorFlow and other warnings for cleaner output ---\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "print(f\"Using TensorFlow version: {tf.__version__}\")\n",
    "# Optional: Configure GPU memory growth if needed\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Configured memory growth for {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load full spectral library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('util/')\n",
    "import importlib\n",
    "\n",
    "import util_scripts as util\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "MONGO_DBR_URI = os.getenv('MONGO_DBR_URI')\n",
    "\n",
    "reload_data_driver = False\n",
    "\n",
    "if (reload_data_driver):\n",
    "\n",
    "    records = []\n",
    "\n",
    "    # Create a new client and connect to the server\n",
    "    client = MongoClient(MONGO_DBR_URI, server_api=ServerApi('1'))\n",
    "\n",
    "    # Send a ping to confirm a successful connection\n",
    "    try:\n",
    "        client.admin.command('ping')\n",
    "        print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    db = client[\"upwins_db\"]\n",
    "    view_name = \"spectral_library\"\n",
    "    spectral_library = db[view_name]\n",
    "\n",
    "    records = spectral_library.find()\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_pickle('data/pkl/library_with_Genus_species.pkl')\n",
    "\n",
    "else:\n",
    "    df = pd.read_pickle('data/pkl/library_with_Genus_species.pkl')\n",
    "    #df = pd.read_pickle('data/pkl/library.pkl')\n",
    "\n",
    "sc = util.SpectralCollection(df)\n",
    "wl_lib = sc.wl\n",
    "name = sc.name\n",
    "spectra = sc.spectra\n",
    "print(sc.spectra.shape)\n",
    "\n",
    "plant_array_lib = sc.name\n",
    "age_array_lib = sc.age\n",
    "part_array_lib = sc.principle_part\n",
    "health_array_lib = sc.health\n",
    "lifecycle_array_lib = sc.lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Get target bands for resampling from Imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fname = 'data/morven_4000/raw_4000_or_ref.img'\n",
    "#fname_hdr = 'data/morven_4000/raw_4000_or_ref.hdr'\n",
    "\n",
    "fname = 'data/morven_9-2025/raw_36286_or_ref.img'\n",
    "fname_hdr = 'data/morven_9-2025/raw_36286_or_ref.hdr'\n",
    "\n",
    "#fname = 'data/5-8-2025/100133_Allied_05_08_2025_2015_06_04_17_50_15/raw_0_ref'\n",
    "#fname_hdr = 'data/5-8-2025/100133_Allied_05_08_2025_2015_06_04_17_50_15/raw_0_ref.hdr'\n",
    "\n",
    "# Open the image and read into an array\n",
    "im = spectral.envi.open(fname_hdr, fname)\n",
    "wl_img = np.asarray(im.bands.centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Load the image into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the image into memory\n",
    "im.Arr = im.load()\n",
    "print(f'Shape of Im.Arr = {im.Arr.shape}')\n",
    "im.List = np.reshape(im.Arr, (im.nrows*im.ncols, im.nbands))\n",
    "print(f'Shape of im.List = {im.List.shape}')\n",
    "\n",
    "valid_pixel_mask = np.sum(im.List, axis=1)>0\n",
    "\n",
    "dataList = im.List[valid_pixel_mask, :]\n",
    "print(f'Shape of dataList = {dataList.shape}')\n",
    "nr = im.nrows\n",
    "nc = im.ncols\n",
    "nb = im.nbands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Get target bands for resampling from ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_filepath = 'data/pkl/rois_labeled/crisfield/Crisfield_October_Training_ROIs_Img14_0513_Iva_fru.pkl'\n",
    "\n",
    "with open(roi_filepath, 'rb') as f:\n",
    "    roiData = pickle.load(f)\n",
    "    roi_df = roiData.df # a DataFrame holding all the data for the ROI\n",
    "\n",
    "roi_spectra = roi_df.to_numpy()[:,4:]\n",
    "roi_spectra = roi_spectra.astype(np.float32)\n",
    "\n",
    "wl_roi = roi_df.columns.to_numpy()[4:]\n",
    "\n",
    "print(\"ROI target bands count: \", len(wl_roi))\n",
    "\n",
    "# set wl_img = to wl_roi for use in the rest of the code\n",
    "wl_img = wl_roi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample library to match target bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BandResampler, which is a function that resamples spectra from one source to match a different source.\n",
    "# See: https://www.spectralpython.net/class_func_ref.html?highlight=resampling#spectral.algorithms.resampling.BandResampler\n",
    "# Inputs: the first input is the wavelengths for the spectra that you are going to resample, the second input is the wavelengths that you want to resample to.\n",
    "\n",
    "resampler = spectral.BandResampler(wl_lib, wl_img)\n",
    "spectra_resampled = resampler(spectra.T).T\n",
    "\n",
    "print(f'The shape of the resampled spectral library is {spectra_resampled.shape}.')\n",
    "print(f'({spectra_resampled.shape[0]} spectra with {spectra_resampled.shape[1]} bands.)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare ROI data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def find_roi_files(root_dir):\n",
    "    string_list = ['.pkl']\n",
    "    \n",
    "    matching_files = []\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        for filename in files:\n",
    "            if all(string in filename.lower() for string in string_list):\n",
    "                matching_files.append(os.path.join(root, filename))\n",
    "    return matching_files\n",
    "\n",
    "# Project codes for labeling ROI data\n",
    "# **IMPORTANT**: ROIs should be named using the **same** naming convention used to label ASD files \n",
    "\n",
    "plant_codes = {\n",
    "    'Ammo_bre': ['Ammophila', 'breviligulata', 'American Beachgrass', 'grass', 'https://en.wikipedia.org/wiki/Ammophila_breviligulata'],\n",
    "    'Chas_lat': ['Chasmanthium', 'latifolium', 'River Oats', 'grass', 'https://en.wikipedia.org/wiki/Chasmanthium_latifolium'],\n",
    "    'Pani_ama': ['Panicum', 'amarum', 'Coastal Panic Grass', 'grass', 'https://en.wikipedia.org/wiki/Panicum_amarum'],\n",
    "    'Pani_vir': ['Panicum', 'virgatum', 'Switch Grass', 'grass', 'https://en.wikipedia.org/wiki/Panicum_virgatum'],\n",
    "    'Soli_sem': ['Solidago', 'sempervirens', 'Seaside Goldenrod', 'succulent', 'https://en.wikipedia.org/wiki/Chasmanthium_latifolium'],\n",
    "    'Robi_his': ['Robinia', 'hispida', 'Bristly locust', 'shrub', 'https://en.wikipedia.org/wiki/Robinia_hispida'],\n",
    "    'More_pen': ['Morella', 'pennsylvanica', 'Bristly locust', 'shrub', 'https://en.wikipedia.org/wiki/Myrica_pensylvanica'],    \n",
    "    'Rosa_rug': ['Rosa', 'rugosa', 'Sandy Beach Rose', 'shrub', 'https://en.wikipedia.org/wiki/Rosa_rugosa'],\n",
    "    'Cham_fas': ['Chamaecrista', 'fasciculata', 'Partridge Pea', 'legume', 'https://en.wikipedia.org/wiki/Chamaecrista_fasciculata'],\n",
    "    'Soli_rug': ['Solidago', 'rugosa', 'Wrinkleleaf goldenrod', 'shrub', 'https://en.wikipedia.org/wiki/Solidago_rugosa'],\n",
    "    'Bacc_hal': ['Baccharis', 'halimifolia', 'Groundseltree', 'shrub', 'https://en.wikipedia.org/wiki/Baccharis_halimifolia'],\n",
    "    'Iva_fru_': ['Iva', 'frutescens', 'Jesuits Bark ', 'shrub', 'https://en.wikipedia.org/wiki/Iva_frutescens'],\n",
    "    'Ilex_vom': ['Ilex', 'vomitoria', 'Yaupon Holly', 'evergreen shrub', 'https://en.wikipedia.org/wiki/Ilex_vomitoria'],\n",
    "    'Genus_spe': ['Genus', 'species', 'vegetation', 'background', '']\n",
    "}  \n",
    "age_codes = {  \n",
    "    'PE': ['Post Germination Emergence', 'PE'],\n",
    "\t#'RE': ['Re-emergence', 'RE'],\n",
    "    #'RE': ['Year 1 growth', '1G'],\n",
    "\t#'E': ['Emergence (from seed)', 'E'],\n",
    "    'E': ['Post Germination Emergence', 'PE'],\n",
    "\t#'D': ['Dormant', 'D'],\n",
    "\t'1G': ['Year 1 growth', '1G'],\n",
    "    '2G': ['Year 2 growth', '2G'],\n",
    "\t#'1F': ['Year 1 Flowering', '1F'],\n",
    "    'J': ['Juvenile', 'J'],\n",
    "\t'M': ['Mature', 'M']\n",
    "}\n",
    "principal_part_codes = {  \n",
    "    'MX': ['Mix', 'MX'],\n",
    "    #'S': ['Seed', 'SE'],\n",
    "\t#'SA': ['Shoot Apex', 'SA'],\n",
    "    'SA': ['Internode Stem', 'ST'],\n",
    "\t'L': ['Leaf/Blade', 'L'],\n",
    "\t#'IS': ['Internode Stem', 'IS'],\n",
    "    'ST': ['Internode Stem', 'ST'],\n",
    "    'SP': ['Sprout', 'SP'],\n",
    "\t#'CS': ['Colar Sprout', 'CS'],\n",
    "    'CS': ['Sprout', 'SP'],\n",
    "\t#'RS': ['Root Sprout', 'RS'],\n",
    "    'RS': ['Sprout', 'SP'],\n",
    "\t'LG': ['Lignin', 'LG'],\n",
    "\t'FL': ['Flower', 'FL'],\n",
    "    #'B': ['Blade', 'B'],\n",
    "\t'B': ['Leaf/Blade', 'L'],\n",
    "    'FR': ['Fruit', 'FR'],\n",
    "\t#'S': ['Seed', 'SE'], #moved above because 'S' is in other codes; this is an old code\n",
    "    'SE': ['Seed', 'SE'],\n",
    "\t#'St': ['Stalk', 'St']\n",
    "}\n",
    "health_codes = {\n",
    "    'MH': ['Healthy/Unhealthy Mix', 'MH'],\n",
    "\t'DS': ['Drought Stress', 'DS'],\n",
    "\t'SS': ['Salt Stress (soak)', 'SS'],\n",
    "    'SY': ['Salt Stress (spray)', 'SY'],\n",
    "\t'S': ['Stressed', 'S'],\n",
    "    'LLRZ': ['LLRZ Lab Stress', 'LLRZ'],\n",
    "\t#'D': ['Dormant', 'D'],\n",
    "    'R': ['Rust', 'R'],\n",
    "    'H': ['Healthy', 'H']\n",
    "}\n",
    "\n",
    "lifecycle_codes = { \n",
    "\t'D': ['Dormant', 'D'],\n",
    "    'RE': ['Re-emergence', 'RE'],\n",
    "    'FLG': ['Flowering', 'FLG'],\n",
    "    'FRG': ['Fruiting', 'FRG'],\n",
    "    \"FFG\": ['Fruiting and Flowering', 'FFG'],\n",
    "    'N': ['Neither', 'N']\n",
    "}\n",
    "\n",
    "# data lists\n",
    "\n",
    "d_spectra = []\n",
    "d_plant = []\n",
    "d_part = []\n",
    "d_health = []\n",
    "d_age = []\n",
    "d_lifecycle = []\n",
    "\n",
    "yd_all_dict_str = {\n",
    "    'plant': d_plant,\n",
    "    'age': d_age,\n",
    "    'part': d_part,\n",
    "    'health': d_health,\n",
    "    'lifecycle': d_lifecycle\n",
    "}\n",
    "\n",
    "code_category_dict = {\n",
    "    'plant': plant_codes,\n",
    "    'age': age_codes,\n",
    "    'part': principal_part_codes,\n",
    "    'health': health_codes,\n",
    "    'lifecycle': lifecycle_codes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find ROI data\n",
    "\n",
    "roi_files = find_roi_files('data/pkl/rois_labeled')\n",
    "print(f\"Number of ROI files found: {len(roi_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare ROI data\n",
    "\n",
    "# ==============================================================================\n",
    "# Helper Function for Stratified Sampling\n",
    "# ==============================================================================\n",
    "def stratified_sample_with_min_per_roi(spectra_df, min_per_roi=50, total_samples=300):\n",
    "    \"\"\"\n",
    "    Selects a subset of labeled ROI pixels from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        spectra_df (pd.DataFrame): DataFrame containing spectral data and a 'roi_name' column.\n",
    "        min_per_roi (int): The minimum number of pixels to select from each ROI.\n",
    "        total_samples (int): The total number of pixels to select.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the selected subset of ROI pixels.\n",
    "    \"\"\"\n",
    "    # Group by ROI and sample a minimum number of pixels from each.\n",
    "    # If an ROI has fewer pixels than min_per_roi, all its pixels are taken.\n",
    "    guaranteed_samples = spectra_df.groupby('roi_name').apply(\n",
    "        lambda x: x.sample(n=min(len(x), min_per_roi))\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # If a total sample size is specified and it's larger than the guaranteed sample\n",
    "    if total_samples and total_samples > len(guaranteed_samples):\n",
    "        remaining_to_select = total_samples - len(guaranteed_samples)\n",
    "        \n",
    "        # Create a pool of remaining pixels by excluding those already selected\n",
    "        # We use the DataFrame index to identify unique rows\n",
    "        remaining_pixels_df = spectra_df.drop(guaranteed_samples.index)\n",
    "\n",
    "        # If there are enough remaining pixels, sample from them\n",
    "        if remaining_to_select > 0 and not remaining_pixels_df.empty:\n",
    "            num_to_sample_from_remaining = min(remaining_to_select, len(remaining_pixels_df))\n",
    "            additional_samples = remaining_pixels_df.sample(n=num_to_sample_from_remaining)\n",
    "            \n",
    "            # Combine the guaranteed and additional samples\n",
    "            final_selection_df = pd.concat([guaranteed_samples, additional_samples])\n",
    "        else:\n",
    "            final_selection_df = guaranteed_samples\n",
    "    else:\n",
    "        final_selection_df = guaranteed_samples\n",
    "\n",
    "    return final_selection_df\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Data Processing and Sampling Logic\n",
    "# ==============================================================================\n",
    "\n",
    "# --- STAGE 1: Aggregate all ROI data from all files ---\n",
    "\n",
    "# This list will hold DataFrames of processed ROI data from each file\n",
    "all_rois_data_list = []\n",
    "resample_rois = True\n",
    "\n",
    "for roi_filename in roi_files:\n",
    "    # Unpickling the dictionary\n",
    "    with open(roi_filename, 'rb') as f:\n",
    "        roiData = pickle.load(f)\n",
    "        roi_df = roiData.df  # The DataFrame holding all the data for the ROI\n",
    "\n",
    "    # Create a unique identifier for sampling based on both Name and Color\n",
    "    # This creates a unique integer for each ('Name', 'Color') group\n",
    "    color_group_id = roi_df.groupby(['Name', 'Color']).ngroup().astype(str)\n",
    "    # We append this ID to the original name to create a new, unique ROI name for sampling\n",
    "    roi_df['unique_roi_name'] = roi_df['Name'] + '_' + color_group_id\n",
    "\n",
    "    roi_spectra = roi_df.iloc[:, 4:-1].to_numpy().astype(np.float32)\n",
    "\n",
    "    if resample_rois:\n",
    "        wl_roi = roi_df.columns.to_numpy()[4:-1]\n",
    "        resampler_roi = spectral.BandResampler(wl_roi, wl_img)\n",
    "        print(\"Number of bands: \", len(wl_roi))\n",
    "        roi_spectra = resampler_roi(roi_spectra.T).T\n",
    "\n",
    "    if \"crisfield\" in roi_filename.lower():\n",
    "        print(\"Running pixel-wise normalization for 'crisfield'\")\n",
    "        min_vals_pixel = np.min(roi_spectra, axis=1, keepdims=True)\n",
    "        max_vals_pixel = np.max(roi_spectra, axis=1, keepdims=True)\n",
    "        range_vals_pixel = max_vals_pixel - min_vals_pixel\n",
    "        range_vals_pixel[range_vals_pixel == 0] = 1\n",
    "        roi_spectra = (roi_spectra - min_vals_pixel) / range_vals_pixel\n",
    "\n",
    "    # Create a temporary DataFrame for the current file's data\n",
    "    temp_df = pd.DataFrame(roi_spectra)\n",
    "    \n",
    "    # This takes the raw data from the roi_df columns and assigns it by position.\n",
    "    temp_df['roi_name'] = roi_df['unique_roi_name'].values\n",
    "    temp_df['original_name'] = roi_df['Name'].values\n",
    "    \n",
    "    all_rois_data_list.append(temp_df)\n",
    "    print(f\"Processed and aggregated data from {roi_filename}\")\n",
    "\n",
    "# Concatenate all data into a single master DataFrame\n",
    "master_roi_df = pd.concat(all_rois_data_list, ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ROI Data Aggregation Complete.\")\n",
    "print(f\"Total number of ROI pixels collected: {len(master_roi_df)}\")\n",
    "print(\"Pixel distribution before sampling (groups are Name + Color):\")\n",
    "print(master_roi_df['roi_name'].value_counts())\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- STAGE 2: Perform Stratified Sampling on the aggregated data ---\n",
    "\n",
    "MIN_PIXELS_PER_ROI = 30 \n",
    "TOTAL_ROI_PIXELS_TO_SELECT = 300\n",
    "\n",
    "print(f\"Starting stratified sampling with min {MIN_PIXELS_PER_ROI}/group and a target total of {TOTAL_ROI_PIXELS_TO_SELECT}...\")\n",
    "\n",
    "selected_rois_df = stratified_sample_with_min_per_roi(\n",
    "    master_roi_df,\n",
    "    min_per_roi=MIN_PIXELS_PER_ROI,\n",
    "    total_samples=TOTAL_ROI_PIXELS_TO_SELECT\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Stratified Sampling Complete.\")\n",
    "print(f\"Total number of pixels selected: {len(selected_rois_df)}\")\n",
    "print(\"Pixel distribution after sampling (groups are Name + Color):\")\n",
    "print(selected_rois_df['roi_name'].value_counts())\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- STAGE 3: Populate final data structures with the selected subset ---\n",
    "\n",
    "# Extract the selected spectra and ORIGINAL names for final processing\n",
    "selected_spectra = selected_rois_df.drop(columns=['roi_name', 'original_name']).to_numpy()\n",
    "selected_original_names = selected_rois_df['original_name'].to_numpy()\n",
    "\n",
    "for i in range(len(selected_spectra)):\n",
    "    roi_spectrum = selected_spectra[i]\n",
    "    # Use the ORIGINAL name for metadata parsing\n",
    "    name = selected_original_names[i]\n",
    "\n",
    "    d_spectra.append(roi_spectrum)\n",
    "    \n",
    "    # This block is your original metadata parsing logic, which now works on the original name\n",
    "    if name == 'Genus_spe_N_N_N_N':\n",
    "        name = 'Genus_spe_MX_N_N_N'\n",
    "    \n",
    "    if name == 'Genus_species_MX_N_N_N':\n",
    "        name = 'Genus_spe_MX_N_N_N'\n",
    "    \n",
    "    if name == 'Soli_semp_MX_M_H_FLG':\n",
    "        name = 'Soli_sem_MX_M_H_FLG'\n",
    "    \n",
    "    if name == 'Iva_frut_MX_M_H_N':\n",
    "        name = 'Iva_fru_MX_M_H_N'\n",
    "\n",
    "    if name[-1] != '_':\n",
    "        name = name + '_'\n",
    "    \n",
    "    class_data_dict = {}\n",
    "    # Assuming code_category_dict is defined elsewhere\n",
    "    for cat, codes in code_category_dict.items():\n",
    "        class_data_dict[cat] = 'N'\n",
    "        for key, value in codes.items():\n",
    "            if cat == 'plant':\n",
    "                if (name[:8].lower() == key.lower()) or (name[:9].lower() == key.lower()):\n",
    "                    class_data_dict[cat] = value[0] + '_' + value[1]\n",
    "            else:\n",
    "                if '_' + key + '_' in name:\n",
    "                    class_data_dict[cat] = value[1]\n",
    "    \n",
    "    for key in yd_all_dict_str:\n",
    "        yd_all_dict_str[key].append(class_data_dict[key])\n",
    "\n",
    "print(\"Final `d_spectra` and `yd_all_dict_str` have been populated with the sampled data.\")\n",
    "use_rois = True\n",
    "\n",
    "d_spectra = np.asarray(d_spectra)\n",
    "print(d_spectra.shape)\n",
    "\n",
    "for key in yd_all_dict_str:\n",
    "    yd_all_dict_str[key] = np.asarray(yd_all_dict_str[key])\n",
    "    print(key, yd_all_dict_str[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Label Encoding and 'N' Value Replacement Functions ---\n",
    "\n",
    "def assign_integer_labels(data_array, label_array):\n",
    "    \"\"\"Maps string labels to integers based on a complete list of unique labels.\"\"\"\n",
    "    mapping = {label: i for i, label in enumerate(label_array)}\n",
    "    return np.array([mapping[x] for x in data_array])\n",
    "\n",
    "def replace_n_with_ignore_val(integer_array, label_array):\n",
    "    \"\"\"Replaces the integer corresponding to the label 'N' with the IGNORE_VALUE.\"\"\"\n",
    "    try:\n",
    "        n_index = np.where(label_array == 'N')[0][0]\n",
    "        integer_array_copy = integer_array.copy()\n",
    "        integer_array_copy[integer_array_copy == n_index] = IGNORE_VALUE\n",
    "        return integer_array_copy\n",
    "    except IndexError:\n",
    "        # 'N' was not found in this label set, so no replacement is needed.\n",
    "        return integer_array\n",
    "\n",
    "\n",
    "# --- Processing All Labels for Consistent Encoding ---\n",
    "\n",
    "# Create a comprehensive list of all possible labels for each task\n",
    "y_plant_labels_full = np.unique(np.concatenate([plant_array_lib, yd_all_dict_str['plant']]))\n",
    "y_age_labels_full = np.unique(np.concatenate([age_array_lib, yd_all_dict_str['age']]))\n",
    "y_part_labels_full = np.unique(np.concatenate([part_array_lib, yd_all_dict_str['part']]))\n",
    "y_health_labels_full = np.unique(np.concatenate([health_array_lib, yd_all_dict_str['health']]))\n",
    "y_lifecycle_labels_full = np.unique(np.concatenate([lifecycle_array_lib, yd_all_dict_str['lifecycle']]))\n",
    "\n",
    "# Create human-readable label maps for evaluation reports (excluding 'N' where it's an invalid label)\n",
    "label_maps = {\n",
    "    'plant': np.sort([l for l in y_plant_labels_full if l != 'N']),\n",
    "    'age': np.sort([l for l in y_age_labels_full if l != 'N']),\n",
    "    'part': np.sort([l for l in y_part_labels_full if l != 'N']),\n",
    "    'health': np.sort([l for l in y_health_labels_full if l != 'N']),\n",
    "    'lifecycle': y_lifecycle_labels_full # For lifecycle, 'N' is a valid class, so we don't filter it\n",
    "}\n",
    "\n",
    "# Store the label_maps dictionary in a shelve key-value store\n",
    "with shelve.open('data/shelve/label_maps_store_5') as db:\n",
    "    db['label_maps'] = label_maps\n",
    "\n",
    "# Process the library labels\n",
    "Yn_int_lib = assign_integer_labels(plant_array_lib, y_plant_labels_full)\n",
    "Ya_int_lib = assign_integer_labels(age_array_lib, y_age_labels_full)\n",
    "Yp_int_lib = assign_integer_labels(part_array_lib, y_part_labels_full)\n",
    "Yh_int_lib = assign_integer_labels(health_array_lib, y_health_labels_full)\n",
    "Yl_int_lib = assign_integer_labels(lifecycle_array_lib, y_lifecycle_labels_full)\n",
    "\n",
    "y_library_tasks_orig = {\n",
    "    'plant': replace_n_with_ignore_val(Yn_int_lib, y_plant_labels_full),\n",
    "    'age': replace_n_with_ignore_val(Ya_int_lib, y_age_labels_full),\n",
    "    'part': replace_n_with_ignore_val(Yp_int_lib, y_part_labels_full),\n",
    "    'health': replace_n_with_ignore_val(Yh_int_lib, y_health_labels_full),\n",
    "    'lifecycle': Yl_int_lib  # No 'N' replacement\n",
    "}\n",
    "\n",
    "# Process the nano-imagery labels\n",
    "Yn_int_nano = assign_integer_labels(yd_all_dict_str['plant'], y_plant_labels_full)\n",
    "Ya_int_nano = assign_integer_labels(yd_all_dict_str['age'], y_age_labels_full)\n",
    "Yp_int_nano = assign_integer_labels(yd_all_dict_str['part'], y_part_labels_full)\n",
    "Yh_int_nano = assign_integer_labels(yd_all_dict_str['health'], y_health_labels_full)\n",
    "Yl_int_nano = assign_integer_labels(yd_all_dict_str['lifecycle'], y_lifecycle_labels_full)\n",
    "\n",
    "y_nano_tasks_orig = {\n",
    "    'plant': replace_n_with_ignore_val(Yn_int_nano, y_plant_labels_full),\n",
    "    'age': replace_n_with_ignore_val(Ya_int_nano, y_age_labels_full),\n",
    "    'part': replace_n_with_ignore_val(Yp_int_nano, y_part_labels_full),\n",
    "    'health': replace_n_with_ignore_val(Yh_int_nano, y_health_labels_full),\n",
    "    'lifecycle': Yl_int_nano # No 'N' replacement\n",
    "}\n",
    "\n",
    "print(\"\\nDefining number of classes based on the full label sets...\")\n",
    "\n",
    "n_plant_classes = len(y_plant_labels_full)\n",
    "n_age_classes = len(y_age_labels_full)\n",
    "n_part_classes = len(y_part_labels_full)\n",
    "n_health_classes = len(y_health_labels_full)\n",
    "n_lifecycle_classes = len(y_lifecycle_labels_full)\n",
    "\n",
    "# -- DEBUG --\n",
    "print(f\"Number of classes: Plant={n_plant_classes}, Age={n_age_classes}, Part={n_part_classes}, Health={n_health_classes}, Lifecycle={n_lifecycle_classes}\")\n",
    "print(len(label_maps['plant']), len(label_maps['age']), len(label_maps['part']), len(label_maps['health']), len(label_maps['lifecycle']))\n",
    "\n",
    "# --- DEBUG: Print labels where plant is 'N' ---\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"DEBUG: Checking for 'N' plant labels\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(\"\\n--- Library Labels (plant='N') ---\")\n",
    "lib_n_indices = np.where(plant_array_lib == 'N')[0]\n",
    "for i in lib_n_indices:\n",
    "    print(f\"Index {i}: plant='{plant_array_lib[i]}', age='{age_array_lib[i]}', part='{part_array_lib[i]}', health='{health_array_lib[i]}', lifecycle='{lifecycle_array_lib[i]}'\")\n",
    "\n",
    "print(\"\\n--- Nano-Imagery Labels (plant='N') ---\")\n",
    "nano_n_indices = np.where(yd_all_dict_str['plant'] == 'N')[0]\n",
    "for i in nano_n_indices:\n",
    "    print(f\"Index {i}: plant='{yd_all_dict_str['plant'][i]}', age='{yd_all_dict_str['age'][i]}', part='{yd_all_dict_str['part'][i]}', health='{yd_all_dict_str['health'][i]}', lifecycle='{yd_all_dict_str['lifecycle'][i]}'\")\n",
    "print(\"=\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REUSABLE MODEL & EVALUATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. REUSABLE MODEL & EVALUATION FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def build_multitask_cnn(input_shape, n_plant, n_age, n_part, n_health, n_lifecycle):\n",
    "    \"\"\"Builds the multi-task 1D CNN with named backbone layers for transfer learning.\"\"\"\n",
    "    inputs = keras.Input(shape=input_shape, name='spectrum_input')\n",
    "\n",
    "    # --- Shared Feature Extractor (1D CNN Backbone) ---\n",
    "    x = layers.Conv1D(filters=32, kernel_size=7, activation='relu', padding='same', name='backbone_conv1')(inputs)\n",
    "    x = layers.BatchNormalization(name='backbone_bn1')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=3, name='backbone_pool1')(x)\n",
    "    x = layers.Dropout(0.25, name='backbone_drop1')(x)\n",
    "\n",
    "    x = layers.Conv1D(filters=64, kernel_size=5, activation='relu', padding='same', name='backbone_conv2')(x)\n",
    "    x = layers.BatchNormalization(name='backbone_bn2')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=3, name='backbone_pool2')(x)\n",
    "    x = layers.Dropout(0.25, name='backbone_drop2')(x)\n",
    "\n",
    "    x = layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same', name='backbone_conv3')(x)\n",
    "    x = layers.BatchNormalization(name='backbone_bn3')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=3, name='backbone_pool3')(x)\n",
    "    x = layers.Dropout(0.3, name='backbone_drop3')(x)\n",
    "\n",
    "    x = layers.Flatten(name='backbone_flatten')(x)\n",
    "    shared_features = layers.Dense(128, activation='relu', name='backbone_dense')(x)\n",
    "    shared_features_bn = layers.BatchNormalization(name='backbone_dense_bn')(shared_features)\n",
    "    shared_features_drop = layers.Dropout(0.5, name='backbone_dense_drop')(shared_features_bn)\n",
    "\n",
    "    # --- Task-Specific Output Heads ---\n",
    "    plant_output = layers.Dense(64, activation='relu')(shared_features_drop)\n",
    "    plant_output = layers.Dense(n_plant, activation='softmax', name='plant_output')(plant_output)\n",
    "\n",
    "    age_output = layers.Dense(32, activation='relu')(shared_features_drop)\n",
    "    age_output = layers.Dense(n_age, activation='softmax', name='age_output')(age_output)\n",
    "\n",
    "    part_output = layers.Dense(32, activation='relu')(shared_features_drop)\n",
    "    part_output = layers.Dense(n_part, activation='softmax', name='part_output')(part_output)\n",
    "\n",
    "    health_output = layers.Dense(32, activation='relu')(shared_features_drop)\n",
    "    health_output = layers.Dense(n_health, activation='softmax', name='health_output')(health_output)\n",
    "\n",
    "    lifecycle_output = layers.Dense(32, activation='relu')(shared_features_drop)\n",
    "    lifecycle_output = layers.Dense(n_lifecycle, activation='softmax', name='lifecycle_output')(lifecycle_output)\n",
    "\n",
    "    # --- Build and return the final model ---\n",
    "    model = keras.Model(\n",
    "        inputs=inputs,\n",
    "        outputs={\n",
    "            'plant_output': plant_output,\n",
    "            'age_output': age_output,\n",
    "            'part_output': part_output,\n",
    "            'health_output': health_output,\n",
    "            'lifecycle_output': lifecycle_output\n",
    "        },\n",
    "        name=\"multitask_cnn\"\n",
    "    )\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_and_report(model, X_test_scaled, y_test_orig, y_test_dict_keras, sample_weights_test, scenario_name):\n",
    "    \"\"\"\n",
    "    Generates predictions, detailed classification reports, and confusion matrices for a trained model.\n",
    "    Saves reports to CSV files.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Evaluating on Test Set for {scenario_name} ---\")\n",
    "\n",
    "    # --- 1. Keras Evaluation (Quick Summary) ---\n",
    "    print(\"\\nRunning model.evaluate() for a quick summary...\")\n",
    "    results = model.evaluate(\n",
    "        X_test_scaled,\n",
    "        y_test_dict_keras,\n",
    "        sample_weight=sample_weights_test,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=0,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "    print(\"\\nTest Set Keras Evaluation Results:\")\n",
    "    print(f\"Overall Loss (Weighted Sum): {results['loss']:.4f}\")\n",
    "    print(\"\\nTest Weighted Metrics (Accuracy ignoring invalid samples):\")\n",
    "    for name in OUTPUT_NAMES:\n",
    "        metric_key = f\"{name}_weighted_sparse_categorical_accuracy\"\n",
    "        if metric_key in results:\n",
    "            print(f\"  {name.replace('_output', '').capitalize()}: {results[metric_key]:.4f}\")\n",
    "        else:\n",
    "            # Fallback for slightly different key names\n",
    "            metric_key_alt = f\"weighted_{name}_sparse_categorical_accuracy\"\n",
    "            if metric_key_alt in results:\n",
    "                print(f\"  {name.replace('_output', '').capitalize()} (alt key): {results[metric_key_alt]:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {name.replace('_output', '').capitalize()}: Weighted metric key not found in results.\")\n",
    "\n",
    "    # --- 2. Generate Predictions for Detailed Reports ---\n",
    "    print(\"\\n\\n--- Generating Detailed Reports and Visualizations ---\")\n",
    "    y_pred_probs = model.predict(X_test_scaled, batch_size=BATCH_SIZE, verbose=0)\n",
    "    \n",
    "    # Ensure predictions is a dictionary (Keras usually returns dict for multi-output)\n",
    "    if not isinstance(y_pred_probs, dict):\n",
    "        y_pred_probs = dict(zip(model.output_names, y_pred_probs))\n",
    "\n",
    "    y_pred_labels = {name: np.argmax(y_pred_probs[name], axis=1) for name in OUTPUT_NAMES}\n",
    "    \n",
    "    overall_scores = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "    # --- 3. Generate a Detailed Report for Each Task ---\n",
    "    for task_name in TASK_NAMES:\n",
    "        output_name = f\"{task_name}_output\"\n",
    "        \n",
    "        print(f\"\\n\\n{'='*45}\\nMetrics for: {task_name.capitalize()}\\n{'='*45}\")\n",
    "\n",
    "        # Get true labels, predicted labels, and sample weights\n",
    "        y_true_all = y_test_orig[task_name]\n",
    "        y_pred_all = y_pred_labels[output_name]\n",
    "        weights = sample_weights_test[output_name]\n",
    "        original_labels = label_maps[task_name]\n",
    "\n",
    "        # Conditional Filtering: Special handling for 'lifecycle' vs. other tasks\n",
    "        if task_name == 'lifecycle':\n",
    "            # Include 'N' category for lifecycle task\n",
    "            y_true_valid = y_true_all\n",
    "            y_pred_valid = y_pred_all\n",
    "            valid_indices = list(range(len(original_labels)))\n",
    "            valid_string_labels = list(original_labels)\n",
    "            print(f\"Including 'N' category. Evaluating on {len(y_true_valid)} samples.\")\n",
    "        else:\n",
    "            # Exclude 'N' category for all other tasks using the weights\n",
    "            valid_mask = (weights == 1.0)\n",
    "            y_true_valid = y_true_all[valid_mask]\n",
    "            y_pred_valid = y_pred_all[valid_mask]\n",
    "            \n",
    "            try:\n",
    "                n_index = np.where(original_labels == 'N')[0][0]\n",
    "                valid_indices = [i for i in range(len(original_labels)) if i != n_index]\n",
    "                valid_string_labels = [label for label in original_labels if label != 'N']\n",
    "            except IndexError: # 'N' category not found\n",
    "                valid_indices = list(range(len(original_labels)))\n",
    "                valid_string_labels = list(original_labels)\n",
    "            print(f\"Excluding 'N' category. Evaluating on {len(y_true_valid)} samples.\")\n",
    "\n",
    "        if len(y_true_valid) == 0:\n",
    "            print(f\"No valid samples to evaluate for task '{task_name}'. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        # Determine the final set of labels present in the data for robust reporting\n",
    "        present_labels_in_data = np.unique(np.concatenate((y_true_valid, y_pred_valid)))\n",
    "        final_cm_indices = [idx for idx in valid_indices if idx in present_labels_in_data]\n",
    "        final_cm_labels = [label for idx, label in zip(valid_indices, valid_string_labels) if idx in final_cm_indices]\n",
    "        \n",
    "        if not final_cm_labels:\n",
    "            print(f\"No valid classes remain for task '{task_name}' after filtering. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate overall metrics for this task\n",
    "        accuracy = accuracy_score(y_true_valid, y_pred_valid)\n",
    "        precision = precision_score(y_true_valid, y_pred_valid, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_true_valid, y_pred_valid, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_true_valid, y_pred_valid, average='weighted', zero_division=0)\n",
    "        \n",
    "        overall_scores['accuracy'].append(accuracy)\n",
    "        overall_scores['precision'].append(precision)\n",
    "        overall_scores['recall'].append(recall)\n",
    "        overall_scores['f1'].append(f1)\n",
    "\n",
    "        # Generate, display, and save the DataFrame report\n",
    "        print(\"\\n--- DataFrame Report ---\")\n",
    "        report_dict = classification_report(\n",
    "            y_true_valid,\n",
    "            y_pred_valid,\n",
    "            labels=final_cm_indices,\n",
    "            target_names=final_cm_labels,\n",
    "            zero_division=0,\n",
    "            output_dict=True\n",
    "        )\n",
    "        report_df = pd.DataFrame(report_dict).transpose()\n",
    "        display(report_df)\n",
    "\n",
    "        if EXPORT_TO_CSV:\n",
    "            csv_filename = f'reports/classification_report_{scenario_name}_{task_name}.csv'\n",
    "            report_df.to_csv(csv_filename)\n",
    "            print(f\"DataFrame report saved to '{csv_filename}'\")\n",
    "\n",
    "        # Calculate and plot the Confusion Matrix\n",
    "        print(\"\\n--- Confusion Matrix ---\")\n",
    "        cm = confusion_matrix(y_true_valid, y_pred_valid, labels=final_cm_indices)\n",
    "        plt.figure(figsize=(max(8, len(final_cm_labels)*0.8), max(6, len(final_cm_labels)*0.6)))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=final_cm_labels,\n",
    "                    yticklabels=final_cm_labels)\n",
    "        plt.title(f'Confusion Matrix - Task: {task_name.capitalize()}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "            \n",
    "    # --- 4. Display and Save Overall Model Performance ---\n",
    "    print(f\"\\n\\n{'='*55}\\n--- Overall Model Performance (Averaged Across Tasks) ---\\n{'='*55}\")\n",
    "    overall_metrics_dict = {\n",
    "        'Overall Accuracy': [np.mean(overall_scores['accuracy'])],\n",
    "        'Overall Precision (Weighted Avg)': [np.mean(overall_scores['precision'])],\n",
    "        'Overall Recall (Weighted Avg)': [np.mean(overall_scores['recall'])],\n",
    "        'Overall F1-Score (Weighted Avg)': [np.mean(overall_scores['f1'])]\n",
    "    }\n",
    "    overall_df = pd.DataFrame(overall_metrics_dict).T\n",
    "    overall_df.columns = ['Average Score']\n",
    "    display(overall_df)\n",
    "    \n",
    "    if EXPORT_TO_CSV:\n",
    "        overall_csv_filename = f'reports/classification_report_{scenario_name}_overall.csv'\n",
    "        overall_df.to_csv(overall_csv_filename)\n",
    "        print(f\"\\nOverall report for {scenario_name} saved to '{overall_csv_filename}'\")\n",
    "\n",
    "    # --- 5. Optional: Labeled Codes and Counts in Test Set for Each Species ---\n",
    "    print(f\"\\n\\n{'='*60}\\n--- Detailed Label Counts in Test Set by Plant Species ---\\n{'='*60}\")\n",
    "    from collections import Counter\n",
    "\n",
    "    for i, plant_name in enumerate(label_maps['plant']):\n",
    "        print(f\"\\n--- Analysis for Species: {plant_name} ---\")\n",
    "        \n",
    "        # Find indices corresponding to the current plant species\n",
    "        selected_indices = np.where(y_test_orig['plant'] == i)[0]\n",
    "        \n",
    "        if len(selected_indices) == 0:\n",
    "            print(\"No test samples found for this species.\")\n",
    "            continue\n",
    "            \n",
    "        # Filter the original label dictionaries for this species\n",
    "        y_test_filtered_by_species = {key: arr[selected_indices] for key, arr in y_test_orig.items()}\n",
    "        \n",
    "        # Count the occurrences of each label for each task\n",
    "        for task, labels in y_test_filtered_by_species.items():\n",
    "            if task == 'plant': continue # Skip counting the plant itself\n",
    "            \n",
    "            print(f\"  Task '{task}':\")\n",
    "            counts = Counter(labels)\n",
    "            for value_idx, count in sorted(counts.items()):\n",
    "                # Map integer index back to string label\n",
    "                if value_idx == IGNORE_VALUE:\n",
    "                    label_str = \"IGNORED\"\n",
    "                else:\n",
    "                    label_str = label_maps[task][value_idx]\n",
    "                print(f\"    {label_str}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA SPLITTING AND PREP FOR FINE-TUNING (NANO IMAGERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4. DATA SPLITTING AND PREP FOR FINE-TUNING (NANO IMAGERY)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nSplitting nano-imagery data for fine-tuning...\")\n",
    "\n",
    "nano_indices = np.arange(len(d_spectra))\n",
    "train_idx, test_idx = train_test_split(nano_indices, test_size=0.20, random_state=42)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.20, random_state=42) # 20% of the 80% is 16%\n",
    "\n",
    "X_nano_train, X_nano_val, X_nano_test = d_spectra[train_idx], d_spectra[val_idx], d_spectra[test_idx]\n",
    "\n",
    "# Split the original integer labels (which include the -1 ignore value)\n",
    "y_nano_train_orig = {task: y_nano_tasks_orig[task][train_idx] for task in TASK_NAMES}\n",
    "y_nano_val_orig = {task: y_nano_tasks_orig[task][val_idx] for task in TASK_NAMES}\n",
    "y_nano_test_orig = {task: y_nano_tasks_orig[task][test_idx] for task in TASK_NAMES}\n",
    "\n",
    "# Create Keras-ready labels (replace -1 with 0 for loss calculation)\n",
    "y_nano_train_keras = {f\"{t}_output\": np.maximum(0, y_nano_train_orig[t]) for t in TASK_NAMES}\n",
    "y_nano_val_keras = {f\"{t}_output\": np.maximum(0, y_nano_val_orig[t]) for t in TASK_NAMES}\n",
    "y_nano_test_keras = {f\"{t}_output\": np.maximum(0, y_nano_test_orig[t]) for t in TASK_NAMES}\n",
    "\n",
    "# Create sample weights (0 for ignored labels, 1 for valid labels)\n",
    "sample_weights_train = {f\"{t}_output\": (y_nano_train_orig[t] != IGNORE_VALUE).astype(np.float32) for t in TASK_NAMES}\n",
    "sample_weights_val = {f\"{t}_output\": (y_nano_val_orig[t] != IGNORE_VALUE).astype(np.float32) for t in TASK_NAMES}\n",
    "sample_weights_test = {f\"{t}_output\": (y_nano_test_orig[t] != IGNORE_VALUE).astype(np.float32) for t in TASK_NAMES}\n",
    "\n",
    "print(f\"Nano data split: Train={len(X_nano_train)}, Val={len(X_nano_val)}, Test={len(X_nano_test)}\")\n",
    "\n",
    "# Define the shared EarlyStopping callback\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',         # Monitor validation loss\n",
    "    patience=PATIENCE,          # Number of epochs with no improvement to wait\n",
    "    restore_best_weights=True,  # Restore weights from the epoch with the best val_loss\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCENARIO 2.2a: BASELINE TRANSFER LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5. SCENARIO 2.2a: BASELINE TRANSFER LEARNING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"--- Starting Scenario 2.2a: Baseline Transfer Learning ---\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# --- 2.2a Step 1: Pre-train on down-sampled library data ---\n",
    "print(\"Step 2.2a.1: Preparing and splitting library data for pre-training...\")\n",
    "\n",
    "# --- Split the library data into training and validation sets ---\n",
    "library_indices = np.arange(len(spectra_resampled))\n",
    "lib_train_idx, lib_val_idx = train_test_split(library_indices, test_size=0.15, random_state=42)\n",
    "\n",
    "X_lib_train = spectra_resampled[lib_train_idx]\n",
    "X_lib_val = spectra_resampled[lib_val_idx]\n",
    "\n",
    "# --- Scale the data ---\n",
    "\n",
    "scaler_2a_pretrain = StandardScaler()\n",
    "\n",
    "X_lib_train_scaled = scaler_2a_pretrain.fit_transform(X_lib_train)[..., np.newaxis]\n",
    "X_lib_val_scaled = scaler_2a_pretrain.transform(X_lib_val)[..., np.newaxis]\n",
    "input_shape_259 = X_lib_train_scaled.shape[1:]\n",
    "\n",
    "# --- Prepare labels and sample weights for both training and validation sets ---\n",
    "y_lib_train_orig = {task: y_library_tasks_orig[task][lib_train_idx] for task in TASK_NAMES}\n",
    "y_lib_val_orig = {task: y_library_tasks_orig[task][lib_val_idx] for task in TASK_NAMES}\n",
    "\n",
    "y_lib_train_keras = {f\"{t}_output\": np.maximum(0, y_lib_train_orig[t]) for t in TASK_NAMES}\n",
    "y_lib_val_keras = {f\"{t}_output\": np.maximum(0, y_lib_val_orig[t]) for t in TASK_NAMES}\n",
    "\n",
    "sample_weights_lib_train = {f\"{t}_output\": (y_lib_train_orig[t] != IGNORE_VALUE).astype(np.float32) for t in TASK_NAMES}\n",
    "sample_weights_lib_val = {f\"{t}_output\": (y_lib_val_orig[t] != IGNORE_VALUE).astype(np.float32) for t in TASK_NAMES}\n",
    "\n",
    "\n",
    "pretrain_model_2a = build_multitask_cnn(\n",
    "    input_shape_259, n_plant_classes, n_age_classes, n_part_classes, n_health_classes, n_lifecycle_classes\n",
    ")\n",
    "\n",
    "if np.isnan(X_lib_train_scaled).any():\n",
    "    print(\"FATAL ERROR: NaN values found in the scaled training data!\")\n",
    "\n",
    "# --- Define the explicit, multi-output losses and metrics (from your original code) ---\n",
    "losses = {name: 'sparse_categorical_crossentropy' for name in OUTPUT_NAMES}\n",
    "metrics = {name: 'sparse_categorical_accuracy' for name in OUTPUT_NAMES}\n",
    "\n",
    "pretrain_model_2a.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=losses,\n",
    "    loss_weights=LOSS_WEIGHTS,\n",
    "    metrics=metrics,\n",
    "    weighted_metrics=metrics\n",
    ")\n",
    "\n",
    "print(\"Step 2.2a.2: Pre-training on 259-band library data...\")\n",
    "\n",
    "pretrain_model_2a.fit(\n",
    "    X_lib_train_scaled,\n",
    "    y_lib_train_keras,\n",
    "    sample_weight=sample_weights_lib_train,\n",
    "    epochs=PRETRAIN_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_lib_val_scaled, y_lib_val_keras, sample_weights_lib_val),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Save the pre-trained model and the scaler ---\n",
    "print(\"\\nStep 2.2a.3: Saving pre-trained model and scaler...\")\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('scalers', exist_ok=True)\n",
    "pretrain_model_2a.save('models/pretrain_model_2a.keras')\n",
    "with open('scalers/scaler_2a_pretrain.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_2a_pretrain, f)\n",
    "print(\"Pre-trained model and scaler saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5. SCENARIO 2.2a: OPTIONALLY LOAD PRE-TRAINED MODEL\n",
    "# ==============================================================================\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define file paths\n",
    "pretrain_model_path = 'models/pretrain_model_2a.keras'\n",
    "scaler_path = 'scalers/scaler_2a_pretrain.pkl'\n",
    "\n",
    "# Check if the pre-trained model file exists and load it\n",
    "if os.path.exists(pretrain_model_path) and os.path.exists(scaler_path):\n",
    "    print(f\"Found pre-trained model at: {pretrain_model_path}\")\n",
    "    print(\"Loading model and scaler from files...\")\n",
    "    pretrain_model_2a = keras.models.load_model(pretrain_model_path)\n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler_2a_pretrain = pickle.load(f)\n",
    "    print(\"Successfully loaded pre-trained model and scaler.\")\n",
    "    # Also reload input shape from the loaded scaler and model\n",
    "    input_shape_259 = pretrain_model_2a.input_shape[1:]\n",
    "\n",
    "else:\n",
    "    print(\"Pre-trained model file not found.\")\n",
    "    print(\"Proceeding with the model just trained in the previous step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.2a Step 2: Build fine-tuning model ---\n",
    "\n",
    "print(\"\\nStep 2.2a.2: Building the fine-tuning model...\")\n",
    "\n",
    "backbone_2a = keras.Model(\n",
    "    inputs=pretrain_model_2a.input,\n",
    "    outputs=pretrain_model_2a.get_layer('backbone_dense_drop').output,\n",
    "    name=\"pretrained_backbone_259\"\n",
    ")\n",
    "backbone_2a.trainable = False  # Freeze the backbone\n",
    "\n",
    "# Build new model with frozen backbone and new, trainable heads\n",
    "nano_inputs_2a = keras.Input(shape=input_shape_259, name='nano_input_259')\n",
    "features_2a = backbone_2a(nano_inputs_2a, training=False)\n",
    "\n",
    "plant_head_2a = layers.Dense(64, activation='relu')(features_2a)\n",
    "plant_output_2a = layers.Dense(n_plant_classes, activation='softmax', name='plant_output')(plant_head_2a)\n",
    "\n",
    "age_head_2a = layers.Dense(32, activation='relu')(features_2a)\n",
    "age_output_2a = layers.Dense(n_age_classes, activation='softmax', name='age_output')(age_head_2a)\n",
    "\n",
    "part_head_2a = layers.Dense(32, activation='relu')(features_2a)\n",
    "part_output_2a = layers.Dense(n_part_classes, activation='softmax', name='part_output')(part_head_2a)\n",
    "\n",
    "health_head_2a = layers.Dense(32, activation='relu')(features_2a)\n",
    "health_output_2a = layers.Dense(n_health_classes, activation='softmax', name='health_output')(health_head_2a)\n",
    "\n",
    "lifecycle_head_2a = layers.Dense(32, activation='relu')(features_2a)\n",
    "lifecycle_output_2a = layers.Dense(n_lifecycle_classes, activation='softmax', name='lifecycle_output')(lifecycle_head_2a)\n",
    "\n",
    "finetune_model_2a = keras.Model(\n",
    "    inputs=nano_inputs_2a,\n",
    "    outputs={\n",
    "        'plant_output': plant_output_2a,\n",
    "        'age_output': age_output_2a,\n",
    "        'part_output': part_output_2a,\n",
    "        'health_output': health_output_2a,\n",
    "        'lifecycle_output': lifecycle_output_2a\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- 2.2a Step 3: Compile and fine-tune on nano imagery ---\n",
    "print(\"\\nStep 2.2a.3: Fine-tuning on nano imagery...\")\n",
    "\n",
    "# Use the loaded or just-trained pre-training scaler to transform the new data\n",
    "X_nano_train_scaled = scaler_2a_pretrain.transform(X_nano_train)[..., np.newaxis]\n",
    "X_nano_val_scaled = scaler_2a_pretrain.transform(X_nano_val)[..., np.newaxis]\n",
    "X_nano_test_scaled = scaler_2a_pretrain.transform(X_nano_test)[..., np.newaxis]\n",
    "\n",
    "# Use the same explicit losses and metrics dictionaries as in the pre-training step\n",
    "losses = {name: 'sparse_categorical_crossentropy' for name in OUTPUT_NAMES}\n",
    "metrics = {name: 'sparse_categorical_accuracy' for name in OUTPUT_NAMES}\n",
    "\n",
    "finetune_model_2a.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=FT_LEARNING_RATE),\n",
    "    loss=losses,\n",
    "    loss_weights=LOSS_WEIGHTS,\n",
    "    metrics=metrics,\n",
    "    weighted_metrics=metrics\n",
    ")\n",
    "\n",
    "print(\"\\nFine-tuning model compiled with weighted metrics.\")\n",
    "\n",
    "finetune_model_2a.fit(\n",
    "    X_nano_train_scaled,\n",
    "    y_nano_train_keras,\n",
    "    sample_weight=sample_weights_train,\n",
    "    validation_data=(X_nano_val_scaled, y_nano_val_keras, sample_weights_val),\n",
    "    epochs=FINETUNE_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- 2.2a Step 4: Full Evaluation and Saving ---\n",
    "evaluate_and_report(\n",
    "    finetune_model_2a, \n",
    "    X_nano_test_scaled, \n",
    "    y_nano_test_orig, \n",
    "    y_nano_test_keras, \n",
    "    sample_weights_test, \n",
    "    \"Scenario_2a\"\n",
    ")\n",
    "\n",
    "# The final fine-tuned model and the scaler used for it are saved.\n",
    "finetune_model_2a.save('models/finetune_model_2a.keras')\n",
    "with open('scalers/scaler_2a_finetune.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_2a_pretrain, f)\n",
    "\n",
    "print(\"\\n--- Scenario 2.2a Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCENARIO 2.2b: REFINED TRANSFER LEARNING WITH ADAPTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6. SCENARIO 2.2b: REFINED TRANSFER LEARNING WITH ADAPTER\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"--- Starting Scenario 2.2b: Refined Transfer Learning with Adapter ---\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# --- 2.2b Step 1: Pre-train on high-resolution data ---\n",
    "print(\"Step 2.2b.1: Preparing and splitting high-resolution library data for pre-training...\")\n",
    "\n",
    "# Find indices for the wavelength range 425nm to 900nm\n",
    "index_425nm = np.where(wl_lib == 425)[0][0]\n",
    "index_900nm = np.where(wl_lib == 900)[0][0]\n",
    "\n",
    "# --- Split the high-resolution library data into training and validation sets ---\n",
    "X_library_cropped_highres = spectra[:, index_425nm:index_900nm]\n",
    "library_indices_highres = np.arange(len(X_library_cropped_highres))\n",
    "lib_train_idx, lib_val_idx = train_test_split(library_indices_highres, test_size=0.15, random_state=42)\n",
    "\n",
    "X_lib_train_highres = X_library_cropped_highres[lib_train_idx]\n",
    "X_lib_val_highres = X_library_cropped_highres[lib_val_idx]\n",
    "\n",
    "# --- Scale the data (Fit ONLY on training data) ---\n",
    "scaler_2b_pretrain = StandardScaler()\n",
    "X_lib_train_scaled_highres = scaler_2b_pretrain.fit_transform(X_lib_train_highres)[..., np.newaxis]\n",
    "X_lib_val_scaled_highres = scaler_2b_pretrain.transform(X_lib_val_highres)[..., np.newaxis]\n",
    "input_shape_475 = X_lib_train_scaled_highres.shape[1:]\n",
    "\n",
    "# --- Prepare labels and sample weights for both splits ---\n",
    "y_lib_train_orig = {task: y_library_tasks_orig[task][lib_train_idx] for task in TASK_NAMES}\n",
    "y_lib_val_orig = {task: y_library_tasks_orig[task][lib_val_idx] for task in TASK_NAMES}\n",
    "y_lib_train_keras = {f\"{t}_output\": np.maximum(0, y_lib_train_orig[t]) for t in TASK_NAMES}\n",
    "y_lib_val_keras = {f\"{t}_output\": np.maximum(0, y_lib_val_orig[t]) for t in TASK_NAMES}\n",
    "sample_weights_lib_train = {f\"{t}_output\": (y_lib_train_orig[t] != IGNORE_VALUE).astype(np.float32) for t in TASK_NAMES}\n",
    "sample_weights_lib_val = {f\"{t}_output\": (y_lib_val_orig[t] != IGNORE_VALUE).astype(np.float32) for t in TASK_NAMES}\n",
    "\n",
    "# --- Build, Compile, and Fit ---\n",
    "pretrain_model_2b = build_multitask_cnn(\n",
    "    input_shape_475, n_plant_classes, n_age_classes, n_part_classes, n_health_classes, n_lifecycle_classes\n",
    ")\n",
    "\n",
    "# Use the explicit compile call\n",
    "losses = {name: 'sparse_categorical_crossentropy' for name in OUTPUT_NAMES}\n",
    "metrics = {name: 'sparse_categorical_accuracy' for name in OUTPUT_NAMES}\n",
    "pretrain_model_2b.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=losses,\n",
    "    loss_weights=LOSS_WEIGHTS,\n",
    "    metrics=metrics,\n",
    "    weighted_metrics=metrics\n",
    ")\n",
    "\n",
    "print(\"Step 2.2b.2: Pre-training on 475-band library data...\")\n",
    "# Use the explicit validation_data tuple\n",
    "pretrain_model_2b.fit(\n",
    "    X_lib_train_scaled_highres,\n",
    "    y_lib_train_keras,\n",
    "    sample_weight=sample_weights_lib_train,\n",
    "    epochs=PRETRAIN_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_lib_val_scaled_highres, y_lib_val_keras, sample_weights_lib_val),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Save the pre-trained model and the scaler ---\n",
    "print(\"\\nStep 2.2b.3: Saving pre-trained model and scaler...\")\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('scalers', exist_ok=True)\n",
    "pretrain_model_2b.save('models/pretrain_model_2b.keras')\n",
    "with open('scalers/scaler_2b_pretrain.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_2b_pretrain, f)\n",
    "print(\"Pre-trained model and scaler saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6. SCENARIO 2.2b: OPTIONALLY LOAD PRE-TRAINED MODEL\n",
    "# ==============================================================================\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define file paths\n",
    "pretrain_model_path = 'models/pretrain_model_2b.keras'\n",
    "scaler_path = 'scalers/scaler_2b_pretrain.pkl'\n",
    "\n",
    "# Check if the pre-trained model file exists and load it\n",
    "if os.path.exists(pretrain_model_path) and os.path.exists(scaler_path):\n",
    "    print(f\"Found pre-trained model at: {pretrain_model_path}\")\n",
    "    print(\"Loading model and scaler from files...\")\n",
    "    pretrain_model_2b = keras.models.load_model(pretrain_model_path)\n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler_2b_pretrain = pickle.load(f)\n",
    "    print(\"Successfully loaded pre-trained model and scaler.\")\n",
    "    # Also reload input shape from the loaded model\n",
    "    input_shape_475 = pretrain_model_2b.input_shape[1:]\n",
    "\n",
    "else:\n",
    "    print(\"Pre-trained model file not found.\")\n",
    "    print(\"Proceeding with the model just trained in the previous step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.2b Step 2: Build fine-tuning model with an Adapter Layer ---\n",
    "print(\"\\nStep 2.2b.4: Building the fine-tuning model with an Adapter Layer...\")\n",
    "\n",
    "backbone_475 = keras.Model(\n",
    "    inputs=pretrain_model_2b.input,\n",
    "    outputs=pretrain_model_2b.get_layer('backbone_dense_drop').output,\n",
    "    name=\"pretrained_backbone_475\"\n",
    ")\n",
    "backbone_475.trainable = False # Freeze the high-res backbone\n",
    "\n",
    "# Build the adapter and new heads\n",
    "nano_inputs_2b = keras.Input(shape=(259, 1), name='nano_input_259')\n",
    "\n",
    "# This is the trainable adapter: it learns to map from 259 to 475 bands\n",
    "flatten_layer = layers.Flatten()(nano_inputs_2b)\n",
    "adapter_output = layers.Dense(475, activation='relu', name='adapter_259_to_475')(flatten_layer)\n",
    "reshaped_for_cnn = layers.Reshape((475, 1))(adapter_output)\n",
    "\n",
    "# Pass the adapted input through the frozen backbone\n",
    "features_2b = backbone_475(reshaped_for_cnn, training=False)\n",
    "\n",
    "# Add new, trainable output heads\n",
    "plant_head_2b = layers.Dense(64, activation='relu')(features_2b)\n",
    "plant_output_2b = layers.Dense(n_plant_classes, activation='softmax', name='plant_output')(plant_head_2b)\n",
    "\n",
    "age_head_2b = layers.Dense(32, activation='relu')(features_2b)\n",
    "age_output_2b = layers.Dense(n_age_classes, activation='softmax', name='age_output')(age_head_2b)\n",
    "\n",
    "part_head_2b = layers.Dense(32, activation='relu')(features_2b)\n",
    "part_output_2b = layers.Dense(n_part_classes, activation='softmax', name='part_output')(part_head_2b)\n",
    "\n",
    "health_head_2b = layers.Dense(32, activation='relu')(features_2b)\n",
    "health_output_2b = layers.Dense(n_health_classes, activation='softmax', name='health_output')(health_head_2b)\n",
    "\n",
    "lifecycle_head_2b = layers.Dense(32, activation='relu')(features_2b)\n",
    "lifecycle_output_2b = layers.Dense(n_lifecycle_classes, activation='softmax', name='lifecycle_output')(lifecycle_head_2b)\n",
    "\n",
    "finetune_model_2b = keras.Model(\n",
    "    inputs=nano_inputs_2b,\n",
    "    outputs={\n",
    "        'plant_output': plant_output_2b,\n",
    "        'age_output': age_output_2b,\n",
    "        'part_output': part_output_2b,\n",
    "        'health_output': health_output_2b,\n",
    "        'lifecycle_output': lifecycle_output_2b\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- 2.2b Step 3: Compile and fine-tune on nano imagery ---\n",
    "print(\"\\nStep 2.2b.5: Fine-tuning on nano imagery...\")\n",
    "\n",
    "# A new scaler is needed because the input data (259 bands) is different from pre-training (475 bands)\n",
    "scaler_2b_finetune = StandardScaler()\n",
    "X_nano_train_scaled_2b = scaler_2b_finetune.fit_transform(X_nano_train)[..., np.newaxis]\n",
    "X_nano_val_scaled_2b = scaler_2b_finetune.transform(X_nano_val)[..., np.newaxis]\n",
    "X_nano_test_scaled_2b = scaler_2b_finetune.transform(X_nano_test)[..., np.newaxis]\n",
    "\n",
    "losses = {name: 'sparse_categorical_crossentropy' for name in OUTPUT_NAMES}\n",
    "metrics = {name: 'sparse_categorical_accuracy' for name in OUTPUT_NAMES}\n",
    "\n",
    "finetune_model_2b.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=FT_LEARNING_RATE),\n",
    "    loss=losses,\n",
    "    loss_weights=LOSS_WEIGHTS,\n",
    "    metrics=metrics,\n",
    "    weighted_metrics=metrics\n",
    ")\n",
    "\n",
    "finetune_model_2b.fit(\n",
    "    X_nano_train_scaled_2b,\n",
    "    y_nano_train_keras,\n",
    "    sample_weight=sample_weights_train,\n",
    "    validation_data=(X_nano_val_scaled_2b, y_nano_val_keras, sample_weights_val),\n",
    "    epochs=FINETUNE_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# --- 2.2b Step 4: Full Evaluation and Saving ---\n",
    "evaluate_and_report(\n",
    "    finetune_model_2b, \n",
    "    X_nano_test_scaled_2b, \n",
    "    y_nano_test_orig, \n",
    "    y_nano_test_keras, \n",
    "    sample_weights_test, \n",
    "    \"Scenario_2b\"\n",
    ")\n",
    "\n",
    "finetune_model_2b.save('models/finetune_model_2b.keras')\n",
    "with open('scalers/scaler_2b_finetune.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_2b_finetune, f)\n",
    "\n",
    "print(\"\\n--- Scenario 2.2b Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
