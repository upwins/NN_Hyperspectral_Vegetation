{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and initialize TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import spectral\n",
    "import shelve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Configuration ---\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 600 # Set a large number, EarlyStopping will find the best\n",
    "LEARNING_RATE = 1e-4\n",
    "PATIENCE = 30 # For Early Stopping\n",
    "IGNORE_VALUE = -1 # Integer value to represent 'N' or ignored labels\n",
    "\n",
    "# Task names (consistent keys/output layer names)\n",
    "TASK_NAMES = ['plant', 'age', 'part', 'health', 'lifecycle']\n",
    "OUTPUT_NAMES = [f\"{task}_output\" for task in TASK_NAMES]\n",
    "\n",
    "# Loss weights (weighting between tasks, start equal)\n",
    "LOSS_WEIGHTS = {\n",
    "    'plant_output': 1.0,\n",
    "    'age_output': 1.0,\n",
    "    'part_output': 1.0,\n",
    "    'health_output': 1.0,\n",
    "    'lifecycle_output': 1.0\n",
    "}\n",
    "\n",
    "print(f\"Using TensorFlow version: {tf.__version__}\")\n",
    "# Optional: Configure GPU memory growth if needed\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Configured memory growth for {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load full spectral library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('util/')\n",
    "import importlib\n",
    "\n",
    "import util_scripts as util\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "MONGO_DBR_URI = os.getenv('MONGO_DBR_URI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_data_driver = False\n",
    "\n",
    "if (reload_data_driver):\n",
    "\n",
    "    records = []\n",
    "\n",
    "    # Create a new client and connect to the server\n",
    "    client = MongoClient(MONGO_DBR_URI, server_api=ServerApi('1'))\n",
    "\n",
    "    # Send a ping to confirm a successful connection\n",
    "    try:\n",
    "        client.admin.command('ping')\n",
    "        print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    db = client[\"upwins_db\"]\n",
    "    view_name = \"spectral_library\"\n",
    "    spectral_library = db[view_name]\n",
    "\n",
    "    records = spectral_library.find()\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_pickle('data/pkl/library_with_Genus_species.pkl')\n",
    "\n",
    "else:\n",
    "    df = pd.read_pickle('data/pkl/library_with_Genus_species.pkl')\n",
    "    #df = pd.read_pickle('data/pkl/library.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = util.SpectralCollection(df)\n",
    "wl_lib = sc.wl\n",
    "name = sc.name\n",
    "spectra = sc.spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sc.spectra.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names = np.unique(name)\n",
    "print(unique_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Nano imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spectral\n",
    "\n",
    "#fname = 'data/morven_4000/raw_4000_or_ref.img'\n",
    "#fname_hdr = 'data/morven_4000/raw_4000_or_ref.hdr'\n",
    "\n",
    "fname = 'data/Greenhead_aug/temp/raw_56319_or_ref.img'\n",
    "fname_hdr = 'data/Greenhead_aug/temp/raw_56319_or_ref.hdr'\n",
    "\n",
    "#fname = 'data/5-8-2025/100133_Allied_05_08_2025_2015_06_04_17_50_15/raw_0_ref'\n",
    "#fname_hdr = 'data/5-8-2025/100133_Allied_05_08_2025_2015_06_04_17_50_15/raw_0_ref.hdr'\n",
    "\n",
    "# Open the image and read into an array\n",
    "im = spectral.envi.open(fname_hdr, fname)\n",
    "wl_img = np.asarray(im.bands.centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image into memory\n",
    "im.Arr = im.load()\n",
    "print(f'Shape of Im.Arr = {im.Arr.shape}')\n",
    "im.List = np.reshape(im.Arr, (im.nrows*im.ncols, im.nbands))\n",
    "print(f'Shape of im.List = {im.List.shape}')\n",
    "\n",
    "valid_pixel_mask = np.sum(im.List, axis=1)>0\n",
    "\n",
    "dataList = im.List[valid_pixel_mask, :]\n",
    "print(f'Shape of dataList = {dataList.shape}')\n",
    "nr = im.nrows\n",
    "nc = im.ncols\n",
    "nb = im.nbands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: if target spectra different from source spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESTINATION\n",
    "\n",
    "fname2 = 'data/5-8-2025/100133_Allied_05_08_2025_2015_06_04_17_50_15/raw_0_ref'\n",
    "fname_hdr2 = 'data/5-8-2025/100133_Allied_05_08_2025_2015_06_04_17_50_15/raw_0_ref.hdr'\n",
    "\n",
    "# Open the image and read into an array\n",
    "im2 = spectral.envi.open(fname_hdr2, fname2)\n",
    "wl_img2 = np.asarray(im2.bands.centers)\n",
    "# Load the image into memory\n",
    "im2.Arr = im2.load()\n",
    "print(f'Shape of Im2.Arr = {im2.Arr.shape}')\n",
    "im2.List = np.reshape(im2.Arr, (im2.nrows*im2.ncols, im2.nbands))\n",
    "print(f'Shape of im2.List = {im2.List.shape}')\n",
    "dataList2 = im2.List[np.sum(im2.List, axis=1)>0, :]\n",
    "print(f'Shape of dataList2 = {dataList2.shape}')\n",
    "nr2 = im2.nrows\n",
    "nc2 = im2.ncols\n",
    "nb2 = im2.nbands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BandResampler, which is a function that resamples spectra from one source to match a different source.\n",
    "# See: https://www.spectralpython.net/class_func_ref.html?highlight=resampling#spectral.algorithms.resampling.BandResampler\n",
    "# Inputs: the first input is the wavelengths for the spectra that you are going to resample, the second input is the wavelengths that you want to resample to.\n",
    "resampler = spectral.BandResampler(wl_img2, wl_img)\n",
    "dataList2_resampled = resampler(dataList2.T).T\n",
    "\n",
    "print(f'The shape of the image is {dataList2_resampled.shape}.')\n",
    "print(f'({dataList2_resampled.shape[0]} spectra with {dataList2_resampled.shape[1]} bands.)')\n",
    "\n",
    "#spectra = spectra_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wl_lib.shape)\n",
    "print(wl_img.shape)\n",
    "\n",
    "#print(wl_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample library to match input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BandResampler, which is a function that resamples spectra from one source to match a different source.\n",
    "# See: https://www.spectralpython.net/class_func_ref.html?highlight=resampling#spectral.algorithms.resampling.BandResampler\n",
    "# Inputs: the first input is the wavelengths for the spectra that you are going to resample, the second input is the wavelengths that you want to resample to.\n",
    "resampler = spectral.BandResampler(wl_lib, wl_img)\n",
    "spectra_resampled = resampler(spectra.T).T\n",
    "\n",
    "print(f'The shape of the spectral library is {spectra_resampled.shape}.')\n",
    "print(f'({spectra_resampled.shape[0]} spectra with {spectra_resampled.shape[1]} bands.)')\n",
    "\n",
    "spectra = spectra_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip the next two cells unless there is a need to filter inputs\n",
    "\n",
    "filter = {\n",
    "    'name': 'Chasmanthium_latifolium',\n",
    "    #'fname': '',\n",
    "    #'genus': '',\n",
    "    #'species': '',\n",
    "    'age': 'M',\n",
    "    'health': 'H',\n",
    "    'part': 'L',\n",
    "    #'type': '',\n",
    "    #'lifecycle': '',\n",
    "    #'date': ''\n",
    "}\n",
    "\n",
    "prediction_class = 'age'\n",
    "\n",
    "selected_indices = sc.select_indicies_with_filter(filter)\n",
    "\n",
    "nSpec = len(selected_indices)\n",
    "print(f\"Number of Spectra: {nSpec}\")\n",
    "\n",
    "print(sc.age[selected_indices])\n",
    "print(sc.principle_part[selected_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = {}\n",
    "class_spectra = spectra[selected_indices]\n",
    "class_nSpec = class_spectra.shape[0]\n",
    "\n",
    "plt.figure(figsize=(10,6)) \n",
    "for i in range(class_nSpec):\n",
    "    plt.plot(wl,class_spectra[i,:].flatten(), color='b', alpha=0.05)\n",
    "means[filter['name']] = np.mean(class_spectra, axis=0)\n",
    "plt.plot(wl, means[filter['name']], color='r')\n",
    "plt.minorticks_on()\n",
    "plt.grid(True, which='major', c='k')\n",
    "plt.grid(True, which='minor', linewidth=0.25)\n",
    "plt.title(f'Spectra for {filter['name']}')\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Reflectance')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare ROI data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def find_roi_files(root_dir):\n",
    "    string_list = ['.pkl', 'roi']\n",
    "    \n",
    "    matching_files = []\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        for filename in files:\n",
    "            if all(string in filename.lower() for string in string_list):\n",
    "                matching_files.append(os.path.join(root, filename))\n",
    "    return matching_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project codes for labeling ROI data\n",
    "# **IMPORTANT**: ROIs should be named using the **same** naming convention used to label ASD files \n",
    "\n",
    "plant_codes = {\n",
    "    'Ammo_bre': ['Ammophila', 'breviligulata', 'American Beachgrass', 'grass', 'https://en.wikipedia.org/wiki/Ammophila_breviligulata'],\n",
    "    'Chas_lat': ['Chasmanthium', 'latifolium', 'River Oats', 'grass', 'https://en.wikipedia.org/wiki/Chasmanthium_latifolium'],\n",
    "    'Pani_ama': ['Panicum', 'amarum', 'Coastal Panic Grass', 'grass', 'https://en.wikipedia.org/wiki/Panicum_amarum'],\n",
    "    'Pani_vir': ['Panicum', 'virgatum', 'Switch Grass', 'grass', 'https://en.wikipedia.org/wiki/Panicum_virgatum'],\n",
    "    'Soli_sem': ['Solidago', 'sempervirens', 'Seaside Goldenrod', 'succulent', 'https://en.wikipedia.org/wiki/Chasmanthium_latifolium'],\n",
    "    'Robi_his': ['Robinia', 'hispida', 'Bristly locust', 'shrub', 'https://en.wikipedia.org/wiki/Robinia_hispida'],\n",
    "    'More_pen': ['Morella', 'pennsylvanica', 'Bristly locust', 'shrub', 'https://en.wikipedia.org/wiki/Myrica_pensylvanica'],    \n",
    "    'Rosa_rug': ['Rosa', 'rugosa', 'Sandy Beach Rose', 'shrub', 'https://en.wikipedia.org/wiki/Rosa_rugosa'],\n",
    "    'Cham_fas': ['Chamaecrista', 'fasciculata', 'Partridge Pea', 'legume', 'https://en.wikipedia.org/wiki/Chamaecrista_fasciculata'],\n",
    "    'Soli_rug': ['Solidago', 'rugosa', 'Wrinkleleaf goldenrod', 'shrub', 'https://en.wikipedia.org/wiki/Solidago_rugosa'],\n",
    "    'Bacc_hal': ['Baccharis', 'halimifolia', 'Groundseltree', 'shrub', 'https://en.wikipedia.org/wiki/Baccharis_halimifolia'],\n",
    "    'Iva_fru_': ['Iva', 'frutescens', 'Jesuits Bark ', 'shrub', 'https://en.wikipedia.org/wiki/Iva_frutescens'],\n",
    "    'Ilex_vom': ['Ilex', 'vomitoria', 'Yaupon Holly', 'evergreen shrub', 'https://en.wikipedia.org/wiki/Ilex_vomitoria'],\n",
    "    'Genus_spe': ['Genus', 'species', 'vegetation', 'background', '']\n",
    "}  \n",
    "age_codes = {  \n",
    "    'PE': ['Post Germination Emergence', 'PE'],\n",
    "\t#'RE': ['Re-emergence', 'RE'],\n",
    "    #'RE': ['Year 1 growth', '1G'],\n",
    "\t#'E': ['Emergence (from seed)', 'E'],\n",
    "    'E': ['Post Germination Emergence', 'PE'],\n",
    "\t#'D': ['Dormant', 'D'],\n",
    "\t'1G': ['Year 1 growth', '1G'],\n",
    "    '2G': ['Year 2 growth', '2G'],\n",
    "\t#'1F': ['Year 1 Flowering', '1F'],\n",
    "    'J': ['Juvenile', 'J'],\n",
    "\t'M': ['Mature', 'M']\n",
    "}\n",
    "principal_part_codes = {  \n",
    "    'MX': ['Mix', 'MX'],\n",
    "    #'S': ['Seed', 'SE'],\n",
    "\t#'SA': ['Shoot Apex', 'SA'],\n",
    "    'SA': ['Internode Stem', 'ST'],\n",
    "\t'L': ['Leaf/Blade', 'L'],\n",
    "\t#'IS': ['Internode Stem', 'IS'],\n",
    "    'ST': ['Internode Stem', 'ST'],\n",
    "    'SP': ['Sprout', 'SP'],\n",
    "\t#'CS': ['Colar Sprout', 'CS'],\n",
    "    'CS': ['Sprout', 'SP'],\n",
    "\t#'RS': ['Root Sprout', 'RS'],\n",
    "    'RS': ['Sprout', 'SP'],\n",
    "\t'LG': ['Lignin', 'LG'],\n",
    "\t'FL': ['Flower', 'FL'],\n",
    "    #'B': ['Blade', 'B'],\n",
    "\t'B': ['Leaf/Blade', 'L'],\n",
    "    'FR': ['Fruit', 'FR'],\n",
    "\t#'S': ['Seed', 'SE'], #moved above because 'S' is in other codes; this is an old code\n",
    "    'SE': ['Seed', 'SE'],\n",
    "\t#'St': ['Stalk', 'St']\n",
    "}\n",
    "health_codes = {\n",
    "    'MH': ['Healthy/Unhealthy Mix', 'MH'],\n",
    "\t'DS': ['Drought Stress', 'DS'],\n",
    "\t'SS': ['Salt Stress (soak)', 'SS'],\n",
    "    'SY': ['Salt Stress (spray)', 'SY'],\n",
    "\t'S': ['Stressed', 'S'],\n",
    "    'LLRZ': ['LLRZ Lab Stress', 'LLRZ'],\n",
    "\t#'D': ['Dormant', 'D'],\n",
    "    'R': ['Rust', 'R'],\n",
    "    'H': ['Healthy', 'H']\n",
    "}\n",
    "\n",
    "lifecycle_codes = { \n",
    "\t'D': ['Dormant', 'D'],\n",
    "    'RE': ['Re-emergence', 'RE'],\n",
    "    'FLG': ['Flowering', 'FLG'],\n",
    "    'FRG': ['Fruiting', 'FRG'],\n",
    "    \"FFG\": ['Fruiting and Flowering', 'FFG'],\n",
    "    'N': ['Neither', 'N']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data lists\n",
    "\n",
    "d_spectra = []\n",
    "d_plant = []\n",
    "d_part = []\n",
    "d_health = []\n",
    "d_age = []\n",
    "d_lifecycle = []\n",
    "\n",
    "yd_all_dict = {\n",
    "    'plant': d_plant,\n",
    "    'age': d_age,\n",
    "    'part': d_part,\n",
    "    'health': d_health,\n",
    "    'lifecycle': d_lifecycle\n",
    "}\n",
    "\n",
    "code_category_dict = {\n",
    "    'plant': plant_codes,\n",
    "    'age': age_codes,\n",
    "    'part': principal_part_codes,\n",
    "    'health': health_codes,\n",
    "    'lifecycle': lifecycle_codes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find ROI data\n",
    "\n",
    "roi_files = find_roi_files('data/pkl/rois_labeled')\n",
    "\n",
    "print(f\"Number of ROI files found: {len(roi_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare ROI data\n",
    "\n",
    "resample_rois = True\n",
    "\n",
    "for roi_filename in roi_files:\n",
    "   # Unpickling the dictionary\n",
    "    with open(roi_filename, 'rb') as f:\n",
    "        roiData = pickle.load(f)\n",
    "        roi_df = roiData.df # a DataFrame holding all the data for the ROI\n",
    "\n",
    "    roi_spectra = roi_df.to_numpy()[:,4:]\n",
    "    roi_spectra = roi_spectra.astype(np.float32)\n",
    "\n",
    "    if resample_rois:\n",
    "        wl_roi = roi_df.columns.to_numpy()[4:]\n",
    "        resampler_roi = spectral.BandResampler(wl_roi, wl_img)\n",
    "        roi_spectra_resampled = resampler_roi(roi_spectra.T).T\n",
    "        roi_spectra = roi_spectra_resampled\n",
    "\n",
    "    roi_spectra_names = roi_df['Name'].to_numpy()\n",
    "\n",
    "    roi_names = roiData.names # the names of the ROIs\n",
    "\n",
    "    print(f\"Number of ROIs found in {roi_filename}: {len(roi_names)}\")\n",
    "    print(roi_names)\n",
    "\n",
    "    for name in roi_names:\n",
    "        roi_class_spectra = roi_spectra[roi_spectra_names==name]\n",
    "\n",
    "        if name == 'Genus_spe_N_N_N_N':\n",
    "            new_name = 'Genus_spe_MX_N_N_N'\n",
    "            print(f\"Renaming {name} to {new_name}\")\n",
    "            name = new_name\n",
    "\n",
    "        if name[-1] != '_':\n",
    "            name = name + '_'\n",
    "\n",
    "        # parse name for metadata\n",
    "        class_data_dict = {}          \n",
    "        \n",
    "        for cat, codes in code_category_dict.items():\n",
    "            class_data_dict[cat] = 'N'\n",
    "            for key, value in codes.items():\n",
    "                if cat == 'plant':\n",
    "                    if (name[:8].lower()==key.lower()) or (name[:9].lower()==key.lower()):\n",
    "                        class_data_dict[cat] = value[0] + '_' + value[1]\n",
    "                else:\n",
    "                    if '_'+key+'_' in name:\n",
    "                        class_data_dict[cat] = value[1]\n",
    "                        #print(key, class_data_dict[cat])\n",
    "                        #print(class_data_dict)\n",
    "            \n",
    "            #if class_data_dict[cat] == 'N':\n",
    "            #    print(cat, name)\n",
    "\n",
    "        #print(class_data_dict)\n",
    "\n",
    "        # for each spectrum in class_spectra, append to each list\n",
    "        for roi_spectrum in roi_class_spectra:\n",
    "\n",
    "            d_spectra.append(roi_spectrum)\n",
    "            \n",
    "            for key in yd_all_dict:\n",
    "                #print(key)\n",
    "                #print(len(yd_all_dict[key]))\n",
    "                #print(yd_all_dict[key])\n",
    "                yd_all_dict[key].append(class_data_dict[key])\n",
    "                #print(len(yd_all_dict[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_spectra = np.asarray(d_spectra)\n",
    "print(d_spectra.shape)\n",
    "\n",
    "for key in yd_all_dict:\n",
    "    yd_all_dict[key] = np.asarray(yd_all_dict[key])\n",
    "    print(key, yd_all_dict[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(d_age))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select subset of ROI spectra and append to library spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_select = 300  # Here (250 before) randomly selected ROI spectra are added to the full spectral library\n",
    "\n",
    "total_indices = d_spectra.shape[0]\n",
    "\n",
    "selected_indices = np.random.choice(total_indices, size=num_to_select, replace=False)\n",
    "\n",
    "selected_d_spectra = d_spectra[selected_indices]\n",
    "print(\"\\nNew d_spectra shape:\", selected_d_spectra.shape)\n",
    "\n",
    "selected_yd_all_dict = {}\n",
    "for key in yd_all_dict:\n",
    "    # Select the corresponding rows/elements from the original array\n",
    "    selected_yd_all_dict[key] = yd_all_dict[key][selected_indices]\n",
    "    print(f\"New {key} shape: {selected_yd_all_dict[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra = np.concatenate((spectra, selected_d_spectra))\n",
    "\n",
    "sc.name = np.concatenate((sc.name, selected_yd_all_dict['plant']))\n",
    "sc.age = np.concatenate((sc.age, selected_yd_all_dict['age']))\n",
    "sc.principle_part = np.concatenate((sc.principle_part, selected_yd_all_dict['part']))\n",
    "sc.health = np.concatenate((sc.health, selected_yd_all_dict['health']))\n",
    "sc.lifecycle = np.concatenate((sc.lifecycle, selected_yd_all_dict['lifecycle']))\n",
    "print(f\"New spectra shape: {spectra.shape}\")\n",
    "print(f\"New sc.name shape: {sc.name.shape}\")\n",
    "print(f\"New sc.age shape: {sc.age.shape}\")\n",
    "print(f\"New sc.principle_part shape: {sc.principle_part.shape}\")\n",
    "print(f\"New sc.health shape: {sc.health.shape}\")\n",
    "print(f\"New sc.lifecycle shape: {sc.lifecycle.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(sc.lifecycle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_integer_labels(data_array, label_array):\n",
    "    \"\"\"\n",
    "    Assigns integer values from a label array to corresponding values in a data array.\n",
    "\n",
    "    Args:\n",
    "        data_array:  The array containing the string or categorical data (e.g., Yn_o, Ypp_o, etc.).\n",
    "        label_array: The array of unique labels (e.g., y_plant_labels, y_part_labels, etc.).\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array containing the integer representations of the data based on the labels.\n",
    "    \"\"\"\n",
    "    mapping = {label: i for i, label in enumerate(label_array)}\n",
    "    integer_array = np.array([mapping[x] for x in data_array])\n",
    "    return integer_array\n",
    "\n",
    "\n",
    "X_all = spectra\n",
    "\n",
    "y_plant_labels = np.unique(sc.name)\n",
    "y_part_labels = np.unique(sc.principle_part)\n",
    "y_health_labels = np.unique(sc.health)\n",
    "y_age_labels = np.unique(sc.age)\n",
    "y_lifecycle_labels = np.unique(sc.lifecycle)\n",
    "\n",
    "label_maps = {'plant': y_plant_labels, 'age': y_age_labels, 'part': y_part_labels, 'health': y_health_labels, 'lifecycle': y_lifecycle_labels}\n",
    "\n",
    "# Store the label_maps dictionary in a shelve key-value store\n",
    "with shelve.open('data/shelve/label_maps_store') as db:\n",
    "    db['label_maps'] = label_maps\n",
    "\n",
    "# Number of classes for each task\n",
    "n_plant_classes = len(y_plant_labels)\n",
    "n_age_classes = len(y_age_labels)\n",
    "n_part_classes = len(y_part_labels)\n",
    "n_health_classes = len(y_health_labels)\n",
    "n_lifecycle_classes = len(y_lifecycle_labels)\n",
    "\n",
    "Yn_int = assign_integer_labels(sc.name, y_plant_labels)\n",
    "Yp_int = assign_integer_labels(sc.principle_part, y_part_labels)\n",
    "Yh_int = assign_integer_labels(sc.health, y_health_labels)\n",
    "Ya_int = assign_integer_labels(sc.age, y_age_labels)\n",
    "Yl_int = assign_integer_labels(sc.lifecycle, y_lifecycle_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_n_with_ignore_val(integer_array, label_array):\n",
    "    \"\"\"\n",
    "    Replaces integer values with -1 in an integer array where the corresponding\n",
    "    label in the label array is 'N'.\n",
    "\n",
    "    Args:\n",
    "        integer_array: The integer-encoded data array (e.g., Yn_int, Ypp_int, etc.).\n",
    "        label_array: The array of unique labels (e.g., y_plant_labels, y_part_labels, etc.).\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array with values replaced by -1 where the label is 'N'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        n_index = np.where(label_array == 'N')[0][0]  # Find the index of 'N'\n",
    "        integer_array[integer_array == n_index] = IGNORE_VALUE  # Replace values\n",
    "        return integer_array\n",
    "    except IndexError:\n",
    "        print(\"'N' not found in the label array.  No replacements made.\")\n",
    "        return integer_array\n",
    "\n",
    "\n",
    "y_plant = replace_n_with_ignore_val(Yn_int, y_plant_labels)\n",
    "y_part = replace_n_with_ignore_val(Yp_int, y_part_labels)\n",
    "y_age = replace_n_with_ignore_val(Ya_int, y_age_labels)\n",
    "y_health = replace_n_with_ignore_val(Yh_int, y_health_labels)\n",
    "\n",
    "y_lifecycle = Yl_int\n",
    "\n",
    "y_all_dict_original = {'plant': y_plant, 'part': y_part, 'age': y_age, 'health': y_health, 'lifecycle': y_lifecycle}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Splitting (Train/Validation/Test on features and ORIGINAL labels) ---\n",
    "indices = np.arange(len(X_all))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.15, random_state=42)\n",
    "train_indices, val_indices = train_test_split(train_indices, test_size=0.1765, random_state=42)\n",
    "X_train, X_val, X_test = X_all[train_indices], X_all[val_indices], X_all[test_indices]\n",
    "y_train_dict_orig = {task: y_all_dict_original[task][train_indices] for task in TASK_NAMES}\n",
    "y_val_dict_orig = {task: y_all_dict_original[task][val_indices] for task in TASK_NAMES}\n",
    "y_test_dict_orig = {task: y_all_dict_original[task][test_indices] for task in TASK_NAMES}\n",
    "print(f\"Data split sizes: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sc.spectra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices_library = test_indices[test_indices < len(sc.spectra)]\n",
    "print(\"Number of library spectra in test set: \", len(test_indices_library))\n",
    "test_indices_rois = test_indices[test_indices >= len(sc.spectra)]\n",
    "print(\"Number of ROI spectra in test set: \", len(test_indices_rois))\n",
    "\n",
    "X_test_library = X_all[test_indices_library]\n",
    "X_test_rois = X_all[test_indices_rois]\n",
    "\n",
    "y_test_dict_library = {task: y_all_dict_original[task][test_indices_library] for task in TASK_NAMES}\n",
    "y_test_dict_rois = {task: y_all_dict_original[task][test_indices_rois] for task in TASK_NAMES}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preprocessing (Standardization) ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "scaler_filename = \"data/pkl/scaler.pkl\"\n",
    "with open(scaler_filename, 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "print(f\"Scaler saved to {scaler_filename}\")\n",
    "\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Reshape for Conv1D: (batch, steps, channels=1) ---\n",
    "X_train_scaled = X_train_scaled[..., np.newaxis]\n",
    "X_val_scaled = X_val_scaled[..., np.newaxis]\n",
    "X_test_scaled = X_test_scaled[..., np.newaxis]\n",
    "#X_train_scaled = X_train[..., np.newaxis]\n",
    "#X_val_scaled = X_val[..., np.newaxis]\n",
    "#X_test_scaled = X_test[..., np.newaxis]\n",
    "\n",
    "print(f\"Feature shapes: Train={X_train_scaled.shape}, Val={X_val_scaled.shape}, Test={X_test_scaled.shape}\")\n",
    "\n",
    "# --- Prepare Labels for Keras Fit/Evaluate (Replace IGNORE_VALUE with 0) ---\n",
    "y_train_dict_keras = {f\"{task}_output\": np.maximum(0, y_train_dict_orig[task]) for task in TASK_NAMES}\n",
    "y_val_dict_keras = {f\"{task}_output\": np.maximum(0, y_val_dict_orig[task]) for task in TASK_NAMES}\n",
    "y_test_dict_keras = {f\"{task}_output\": np.maximum(0, y_test_dict_orig[task]) for task in TASK_NAMES}\n",
    "print(f\"Example Keras 'age_output' labels for training: {y_train_dict_keras['age_output'][:20]}\")\n",
    "\n",
    "# --- Create Sample Weight Dictionaries ---\n",
    "sample_weights_train = {f\"{task}_output\": (y_train_dict_orig[task] != IGNORE_VALUE).astype(np.float32) for task in TASK_NAMES}\n",
    "sample_weights_val = {f\"{task}_output\": (y_val_dict_orig[task] != IGNORE_VALUE).astype(np.float32) for task in TASK_NAMES}\n",
    "sample_weights_test = {f\"{task}_output\": (y_test_dict_orig[task] != IGNORE_VALUE).astype(np.float32) for task in TASK_NAMES}\n",
    "print(f\"Example train sample weights for 'age_output': {sample_weights_train['age_output'][:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_library_scaled = scaler.transform(X_test_library)\n",
    "X_test_library_scaled = X_test_library_scaled[..., np.newaxis]\n",
    "\n",
    "X_test_rois_scaled = scaler.transform(X_test_rois)\n",
    "X_test_rois_scaled = X_test_rois_scaled[..., np.newaxis]\n",
    "\n",
    "y_test_dict_library_keras = {f\"{task}_output\": np.maximum(0, y_test_dict_library[task]) for task in TASK_NAMES}\n",
    "y_test_dict_rois_keras = {f\"{task}_output\": np.maximum(0, y_test_dict_rois[task]) for task in TASK_NAMES}\n",
    "\n",
    "sample_weights_test_library = {f\"{task}_output\": (y_test_dict_library[task] != IGNORE_VALUE).astype(np.float32) for task in TASK_NAMES}\n",
    "sample_weights_test_rois = {f\"{task}_output\": (y_test_dict_rois[task] != IGNORE_VALUE).astype(np.float32) for task in TASK_NAMES}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Load existing NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('data/checkpoints/model_9-10-25_0.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Build NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a 1D CNN to capture local patterns (like absorption peaks or slopes across adjacent bands)\n",
    "\n",
    "def build_spectral_cnn(input_shape, n_plant, n_age, n_part, n_health, n_lifecycle):\n",
    "    inputs = keras.Input(shape=input_shape, name='spectrum_input')\n",
    "\n",
    "    # --- Shared Feature Extractor (1D CNN Backbone) ---\n",
    "    x = layers.Conv1D(filters=32, kernel_size=7, activation='relu', padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=3)(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    x = layers.Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=3)(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    x = layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=3)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    shared_features = layers.Flatten()(x)\n",
    "\n",
    "    # Optional shared dense layer\n",
    "    shared_features = layers.Dense(128, activation='relu')(shared_features)\n",
    "    shared_features = layers.BatchNormalization()(shared_features)\n",
    "    shared_features = layers.Dropout(0.5)(shared_features)\n",
    "\n",
    "    # --- Output Heads (One per task) ---\n",
    "    plant_output = layers.Dense(64, activation='relu')(shared_features)\n",
    "    plant_output = layers.Dense(n_plant, activation='softmax', name='plant_output')(plant_output)\n",
    "\n",
    "    age_output = layers.Dense(32, activation='relu')(shared_features)\n",
    "    age_output = layers.Dense(n_age, activation='softmax', name='age_output')(age_output)\n",
    "\n",
    "    part_output = layers.Dense(32, activation='relu')(shared_features)\n",
    "    part_output = layers.Dense(n_part, activation='softmax', name='part_output')(part_output)\n",
    "\n",
    "    health_output = layers.Dense(32, activation='relu')(shared_features)\n",
    "    health_output = layers.Dense(n_health, activation='softmax', name='health_output')(health_output)\n",
    "\n",
    "    lifecycle_output = layers.Dense(32, activation='relu')(shared_features)\n",
    "    lifecycle_output = layers.Dense(n_lifecycle, activation='softmax', name='lifecycle_output')(lifecycle_output)\n",
    "\n",
    "    # --- Build the Model ---\n",
    "    model = keras.Model(\n",
    "        inputs=inputs,\n",
    "        outputs={\n",
    "            'plant_output': plant_output,\n",
    "            'age_output': age_output,\n",
    "            'part_output': part_output,\n",
    "            'health_output': health_output,\n",
    "            'lifecycle_output': lifecycle_output\n",
    "        },\n",
    "        name=\"spectral_multi_task_cnn\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "input_shape = X_train_scaled.shape[1:]\n",
    "model = build_spectral_cnn(input_shape, n_plant_classes, n_age_classes, n_part_classes, n_health_classes, n_lifecycle_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "losses = {name: 'sparse_categorical_crossentropy' for name in OUTPUT_NAMES}\n",
    "metrics = {name: 'sparse_categorical_accuracy' for name in OUTPUT_NAMES}\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=losses,\n",
    "    loss_weights=LOSS_WEIGHTS,\n",
    "    metrics=metrics, # Standard metrics\n",
    "    weighted_metrics=metrics # Weighted metrics\n",
    ")\n",
    "\n",
    "print(\"\\nModel Compiled.\")\n",
    "print(f\"Losses: {model.loss}\")\n",
    "print(f\"Metrics: {model.metrics_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load weights from a previous training checkpoint\n",
    "\n",
    "#model.load_weights('data/checkpoints/best_spectral_model.weights_standard_dense_lib_resampled_5-16-3.h5')\n",
    "model.load_weights('data/checkpoints/best_spectral_model.weights_9-10-25_0.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NN if weights are not loaded in the previous step\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=PATIENCE,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_dict_keras,           # Labels dictionary (keys match output names)\n",
    "    sample_weight=sample_weights_train, # Sample weights dictionary (keys match output names)\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val_scaled, y_val_dict_keras, sample_weights_val), # Also pass val weights\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save weights after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "\n",
    "checkpoint_filepath = 'data/checkpoints/best_spectral_model.weights_9-10-25_0.h5'\n",
    "\n",
    "model.save_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('data/checkpoints/model_9-10-25_0.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_maps = {\n",
    "    'plant': y_plant_labels,\n",
    "    'age': y_age_labels,\n",
    "    'part': y_part_labels,\n",
    "    'health': y_health_labels,\n",
    "    'lifecycle': y_lifecycle_labels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\nEvaluating on Test Set with Best Model (using sample weights)...\")\n",
    "\n",
    "results = model.evaluate(\n",
    "    X_test_scaled,\n",
    "    y_test_dict_keras,\n",
    "    sample_weight=sample_weights_test,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=0,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "print(f\"Overall Loss (Weighted Sum): {results['loss']:.4f}\")\n",
    "\n",
    "print(\"\\nTest Weighted Metrics (Accuracy ignoring invalid samples):\")\n",
    "for name in OUTPUT_NAMES:\n",
    "    metric_key = f\"{name}_weighted_sparse_categorical_accuracy\"\n",
    "    if metric_key in results:\n",
    "        print(f\"  {name.replace('_output', '').capitalize()}: {results[metric_key]:.4f}\")\n",
    "    else:\n",
    "         metric_key_alt = f\"weighted_{name}_sparse_categorical_accuracy\"\n",
    "         if metric_key_alt in results:\n",
    "              print(f\"  {name.replace('_output', '').capitalize()} (alt key): {results[metric_key_alt]:.4f}\")\n",
    "         else:\n",
    "              print(f\"  {name.replace('_output', '').capitalize()}: Weighted metric key not found in results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Test Set from Spectral Library only\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\nEvaluating on Test Set (Library) with Best Model (using sample weights)...\")\n",
    "\n",
    "results = model.evaluate(\n",
    "    X_test_library_scaled,\n",
    "    y_test_dict_library_keras,\n",
    "    sample_weight=sample_weights_test_library,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=0,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "print(f\"Overall Loss (Weighted Sum): {results['loss']:.4f}\")\n",
    "\n",
    "print(\"\\nTest Weighted Metrics (Accuracy ignoring invalid samples):\")\n",
    "for name in OUTPUT_NAMES:\n",
    "    metric_key = f\"{name}_weighted_sparse_categorical_accuracy\"\n",
    "    if metric_key in results:\n",
    "        print(f\"  {name.replace('_output', '').capitalize()}: {results[metric_key]:.4f}\")\n",
    "    else:\n",
    "         metric_key_alt = f\"weighted_{name}_sparse_categorical_accuracy\"\n",
    "         if metric_key_alt in results:\n",
    "              print(f\"  {name.replace('_output', '').capitalize()} (alt key): {results[metric_key_alt]:.4f}\")\n",
    "         else:\n",
    "              print(f\"  {name.replace('_output', '').capitalize()}: Weighted metric key not found in results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Test Set from ROIs only\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\nEvaluating on Test Set (ROIs) with Best Model (using sample weights)...\")\n",
    "\n",
    "results = model.evaluate(\n",
    "    X_test_rois_scaled,\n",
    "    y_test_dict_rois_keras,\n",
    "    sample_weight=sample_weights_test_rois,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=0,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "print(f\"Overall Loss (Weighted Sum): {results['loss']:.4f}\")\n",
    "\n",
    "print(\"\\nTest Weighted Metrics (Accuracy ignoring invalid samples):\")\n",
    "for name in OUTPUT_NAMES:\n",
    "    metric_key = f\"{name}_weighted_sparse_categorical_accuracy\"\n",
    "    if metric_key in results:\n",
    "        print(f\"  {name.replace('_output', '').capitalize()}: {results[metric_key]:.4f}\")\n",
    "    else:\n",
    "         metric_key_alt = f\"weighted_{name}_sparse_categorical_accuracy\"\n",
    "         if metric_key_alt in results:\n",
    "              print(f\"  {name.replace('_output', '').capitalize()} (alt key): {results[metric_key_alt]:.4f}\")\n",
    "         else:\n",
    "              print(f\"  {name.replace('_output', '').capitalize()}: Weighted metric key not found in results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Print Labeled Codes and Counts in Test Set for Each Species (checking for a balanced test set)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for i, val in enumerate(y_plant_labels):\n",
    "    \n",
    "    selected_indices = np.arange(len(y_test_dict_orig['plant']))\n",
    "    selected_indices = selected_indices[y_test_dict_orig['plant'] == i]\n",
    "\n",
    "    y_test_dict_keras_filtered = {key : y_test_dict_keras[key][selected_indices] for key in y_test_dict_keras.keys()}\n",
    "    sample_weights_test_filtered = {key : sample_weights_test[key][selected_indices] for key in sample_weights_test.keys()}\n",
    "\n",
    "    #print labels for values for all tasks of y_test_dict_keras['plant_output'][selected_indices]\n",
    "\n",
    "    #counts_dict = dict(zip(np.unique(y_test_dict_keras_filtered, return_counts=True)))\n",
    "\n",
    "    value_counts = {}\n",
    "\n",
    "    for key, arr in y_test_dict_keras_filtered.items():\n",
    "        value_counts[key] = Counter(arr)\n",
    "\n",
    "    print(\"Labeled Codes and Counts in Test Set for Each Species\")\n",
    "\n",
    "    for key, counts in value_counts.items():\n",
    "\n",
    "        key = key.replace('_output', '')\n",
    "        \n",
    "        print(f\" '{key}':\")\n",
    "        for value, count in counts.items():\n",
    "            print(f\"    {label_maps[key][value]}: {count} {'spectra' if key == 'plant' else ''}\")\n",
    "\n",
    "    results = model.evaluate(\n",
    "        X_test_scaled[selected_indices],\n",
    "        y_test_dict_keras_filtered,\n",
    "        sample_weight=sample_weights_test_filtered,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=0,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTest Set Evaluation Results for plant={val}:\")\n",
    "    print(f\"Overall Loss (Weighted Sum): {results['loss']:.4f}\")\n",
    "\n",
    "    print(\"\\nTest Weighted Metrics (Accuracy ignoring invalid samples):\")\n",
    "    for name in OUTPUT_NAMES:\n",
    "        metric_key = f\"{name}_weighted_sparse_categorical_accuracy\"\n",
    "        if metric_key in results:\n",
    "            print(f\"  {name.replace('_output', '').capitalize()}: {results[metric_key]:.4f}\")\n",
    "        else:\n",
    "            metric_key_alt = f\"weighted_{name}_sparse_categorical_accuracy\"\n",
    "            if metric_key_alt in results:\n",
    "                print(f\"  {name.replace('_output', '').capitalize()} (alt key): {results[metric_key_alt]:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {name.replace('_output', '').capitalize()}: Weighted metric key not found in results.\")\n",
    "    print(\"\\n____________________________________________________________\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Show confusion matrices for each task\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np # Ensure numpy is imported\n",
    "\n",
    "# --- Predictions on the Test Set ---\n",
    "print(\"\\nGenerating predictions on the test set...\")\n",
    "predictions = model.predict(X_test_scaled, batch_size=BATCH_SIZE)\n",
    "print(\"Predictions generated.\")\n",
    "\n",
    "# Ensure predictions is a dictionary if it's not already (Keras usually returns dict for multi-output)\n",
    "if not isinstance(predictions, dict):\n",
    "     # If the model output is a list, convert it back to a dict based on output names\n",
    "     output_layer_names = model.output_names # Or use OUTPUT_NAMES if they match exactly\n",
    "     predictions = dict(zip(output_layer_names, predictions))\n",
    "\n",
    "\n",
    "# # --- Create a mapping from task name to its labels ---\n",
    "# # Make sure these label arrays are accessible here\n",
    "# label_maps = {\n",
    "#     'plant': y_plant_labels,\n",
    "#     'age': y_age_labels,\n",
    "#     'part': y_part_labels,\n",
    "#     'health': y_health_labels,\n",
    "#     'lifecycle': y_lifecycle_labels\n",
    "# }\n",
    "\n",
    "# --- Generate and Plot Confusion Matrices ---\n",
    "print(\"\\nGenerating Confusion Matrices and Classification Reports...\")\n",
    "\n",
    "for task in TASK_NAMES:\n",
    "    output_name = f\"{task}_output\" # e.g., 'plant_output'\n",
    "\n",
    "    print(f\"\\n--- Task: {task.capitalize()} ---\")\n",
    "\n",
    "    # 1. Get True Labels (Original, includes integer mapping for 'N')\n",
    "    y_true_all = y_test_dict_orig[task]\n",
    "\n",
    "    # 2. Get Predicted Labels (Integers)\n",
    "    if output_name not in predictions:\n",
    "        print(f\"Warning: Output key '{output_name}' not found in model predictions. Skipping task '{task}'.\")\n",
    "        continue\n",
    "    y_pred_probs = predictions[output_name]\n",
    "    y_pred_all = np.argmax(y_pred_probs, axis=-1) # Get the class index with the highest probability\n",
    "\n",
    "    # 3. Get Sample Weights\n",
    "    weights = sample_weights_test[output_name]\n",
    "\n",
    "    # 4. Get the corresponding class names (labels)\n",
    "    original_labels = label_maps[task] # e.g., y_lifecycle_labels\n",
    "\n",
    "    # 5. *Conditional* Filtering and Label Definition\n",
    "    if task == 'lifecycle':\n",
    "        # --- Lifecycle Task: Include 'N' ---\n",
    "        # We don't filter samples based on weight, as weight=0 specifically marks 'N' here\n",
    "        # We want to see how 'N' is classified.\n",
    "        y_true_valid = y_true_all\n",
    "        y_pred_valid = y_pred_all\n",
    "\n",
    "        # Define the labels for the CM to include *all* original categories\n",
    "        # Note: y_true_valid might contain IGNORE_VALUE (-1) if 'N' was mapped to it.\n",
    "        # The confusion_matrix function handles this gracefully if -1 isn't in `labels`.\n",
    "        # So, we define `valid_indices` based on the original mapping.\n",
    "        valid_indices = list(range(len(original_labels))) # 0, 1, 2,... N_classes-1\n",
    "        valid_string_labels = list(original_labels) # Includes 'N' string\n",
    "\n",
    "        print(f\"Including 'N' category for {task.capitalize()}. Evaluating on {len(y_true_valid)} samples.\")\n",
    "\n",
    "    else:\n",
    "        # --- Other Tasks: Exclude 'N' ---\n",
    "        # Filter out ignored samples using the weights\n",
    "        valid_mask = (weights == 1.0) # Mask for samples that should NOT be ignored\n",
    "        y_true_valid = y_true_all[valid_mask]\n",
    "        y_pred_valid = y_pred_all[valid_mask]\n",
    "\n",
    "        # Define the labels for the CM *excluding* the 'N' category\n",
    "        try:\n",
    "            # Find the integer index originally assigned to 'N'\n",
    "            n_index = np.where(original_labels == 'N')[0][0]\n",
    "            valid_indices = [i for i in range(len(original_labels)) if i != n_index]\n",
    "            valid_string_labels = [label for i, label in enumerate(original_labels) if i != n_index]\n",
    "        except IndexError:\n",
    "             # 'N' category doesn't exist for this task, include all labels\n",
    "             print(f\"Note: 'N' category not found in labels for task '{task}'. Including all labels.\")\n",
    "             valid_indices = list(range(len(original_labels)))\n",
    "             valid_string_labels = list(original_labels)\n",
    "\n",
    "        print(f\"Excluding 'N' category for {task.capitalize()}. Evaluating on {len(y_true_valid)} samples.\")\n",
    "\n",
    "\n",
    "    # 6. Check if there are any valid samples left for this task\n",
    "    if len(y_true_valid) == 0:\n",
    "        print(f\"No samples to evaluate for task '{task}' after filtering (if applicable). Skipping CM.\")\n",
    "        continue\n",
    "\n",
    "    # 7. Determine the final set of labels/indices *present* in the data for the CM\n",
    "    # This is important because even if we define all labels, some might not\n",
    "    # appear in the specific y_true_valid/y_pred_valid subset.\n",
    "    present_true_labels = np.unique(y_true_valid)\n",
    "    present_pred_labels = np.unique(y_pred_valid)\n",
    "\n",
    "    # Combine present labels, but only keep those that were in our initial 'valid_indices' list\n",
    "    # (This prevents including the IGNORE_VALUE index (-1) if 'N' was mapped to it for lifecycle)\n",
    "    all_present_indices_in_data = np.unique(np.concatenate((present_true_labels, present_pred_labels)))\n",
    "    final_cm_indices = [idx for idx in valid_indices if idx in all_present_indices_in_data]\n",
    "    final_cm_labels = [label for idx, label in zip(valid_indices, valid_string_labels) if idx in final_cm_indices]\n",
    "\n",
    "\n",
    "    # Handle case where after filtering, no valid classes remain\n",
    "    if not final_cm_indices:\n",
    "         print(f\"No valid classes found in true/predicted labels for task '{task}' for CM. Skipping CM.\")\n",
    "         continue\n",
    "\n",
    "    # 8. Calculate Confusion Matrix using the *final* determined indices\n",
    "    # `labels=final_cm_indices` ensures the matrix axes match `final_cm_labels`\n",
    "    cm = confusion_matrix(y_true_valid, y_pred_valid, labels=final_cm_indices)\n",
    "\n",
    "    # 9. Plot Confusion Matrix\n",
    "    plt.figure(figsize=(max(8, len(final_cm_labels)*0.8), max(6, len(final_cm_labels)*0.6))) # Adjust size based on num labels\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=final_cm_labels,\n",
    "                yticklabels=final_cm_labels)\n",
    "    plt.title(f'Confusion Matrix - Task: {task.capitalize()}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 10. Print Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    try:\n",
    "        # Use zero_division=0 or 1 for handling classes with no true samples\n",
    "        report = classification_report(y_true_valid, y_pred_valid,\n",
    "                                   labels=final_cm_indices,\n",
    "                                   target_names=final_cm_labels,\n",
    "                                   zero_division=0)\n",
    "        print(report)\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not generate classification report for {task}: {e}\")\n",
    "        print(\"This might happen if predicted values contain labels not present in true values after filtering.\")\n",
    "        print(f\"Unique True values considered: {np.unique(y_true_valid)}\")\n",
    "        print(f\"Unique Pred values considered: {np.unique(y_pred_valid)}\")\n",
    "        print(f\"Indices used for report: {final_cm_indices}\")\n",
    "        print(f\"Labels used for report: {final_cm_labels}\")\n",
    "\n",
    "\n",
    "print(\"\\nFinished generating visualizations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference function to predict task classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_spectra(new_spectra, model, scaler, label_maps, task_names):\n",
    "    \"\"\"\n",
    "    Predicts classifications for multiple tasks for one or more input spectra.\n",
    "\n",
    "    Args:\n",
    "        new_spectra (np.ndarray): A NumPy array containing the spectrum/spectra.\n",
    "                                   Shape should be (num_bands,) for a single spectrum,\n",
    "                                   or (num_samples, num_bands) for multiple spectra.\n",
    "        model (tf.keras.Model): The trained Keras model.\n",
    "        scaler (sklearn.preprocessing.StandardScaler): The StandardScaler *already fitted*\n",
    "                                                      on the training data.\n",
    "        label_maps (dict): Dictionary mapping task names (e.g., 'plant') to their\n",
    "                           corresponding array of string labels (e.g., y_plant_labels).\n",
    "        task_names (list): List of task names (e.g., ['plant', 'age', ...]).\n",
    "\n",
    "    Returns:\n",
    "        list or dict:\n",
    "            - If a single spectrum was input: A dictionary where keys are task names\n",
    "              and values are the predicted string labels (e.g., {'plant': 'Rosa_rugosa', 'age': 'M', ...}).\n",
    "            - If multiple spectra were input: A list of dictionaries, where each\n",
    "              dictionary represents the predictions for one input spectrum.\n",
    "        None: If input shape is invalid.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the number of bands in new_spectra doesn't match the scaler.\n",
    "    \"\"\"\n",
    "    # --- Input Validation and Preparation ---\n",
    "    if not isinstance(new_spectra, np.ndarray):\n",
    "        new_spectra = np.array(new_spectra)\n",
    "\n",
    "    if new_spectra.ndim == 1:\n",
    "        # Single spectrum provided, reshape to (1, num_bands) for scaler and model\n",
    "        num_bands = new_spectra.shape[0]\n",
    "        spectra_batch = new_spectra.reshape(1, -1)\n",
    "        single_input = True\n",
    "    elif new_spectra.ndim == 2:\n",
    "        # Batch of spectra provided\n",
    "        num_bands = new_spectra.shape[1]\n",
    "        spectra_batch = new_spectra\n",
    "        single_input = False\n",
    "    else:\n",
    "        print(f\"Error: Input spectra must be 1D or 2D, but got {new_spectra.ndim} dimensions.\")\n",
    "        return None\n",
    "\n",
    "    # Check if number of bands matches the scaler\n",
    "    if num_bands != scaler.n_features_in_:\n",
    "        raise ValueError(f\"Input spectrum has {num_bands} bands, but the model/scaler \"\n",
    "                         f\"was trained with {scaler.n_features_in_} bands.\")\n",
    "\n",
    "    # --- Preprocessing ---\n",
    "    # 1. Scale using the *fitted* scaler\n",
    "    spectra_scaled = scaler.transform(spectra_batch)\n",
    "\n",
    "    # 2. Reshape for Conv1D input: (batch_size, steps=num_bands, channels=1)\n",
    "    spectra_reshaped = spectra_scaled[..., np.newaxis]\n",
    "\n",
    "    # --- Prediction ---\n",
    "    # Get raw probability outputs from the model\n",
    "    predictions_raw = model.predict(spectra_reshaped)\n",
    "    # Ensure predictions_raw is a dict (it should be for multi-output)\n",
    "    if not isinstance(predictions_raw, dict):\n",
    "         output_layer_names = model.output_names\n",
    "         predictions_raw = dict(zip(output_layer_names, predictions_raw))\n",
    "\n",
    "\n",
    "    # --- Output Processing ---\n",
    "    results = []\n",
    "    num_samples = spectra_reshaped.shape[0]\n",
    "\n",
    "    for i in range(num_samples): # Loop through each spectrum in the batch\n",
    "        sample_predictions = {}\n",
    "        for task in task_names:\n",
    "            output_name = f\"{task}_output\" # e.g., 'plant_output'\n",
    "\n",
    "            if output_name not in predictions_raw:\n",
    "                 print(f\"Warning: Output key '{output_name}' not found in model predictions for task '{task}'. Skipping.\")\n",
    "                 sample_predictions[task] = \"Error: Output not found\"\n",
    "                 continue\n",
    "\n",
    "            # Get probabilities for the current task and current sample\n",
    "            task_probs = predictions_raw[output_name][i]\n",
    "\n",
    "            # Find the index of the highest probability\n",
    "            predicted_index = np.argmax(task_probs)\n",
    "\n",
    "            # Convert index back to string label\n",
    "            try:\n",
    "                predicted_label = label_maps[task][predicted_index]\n",
    "            except IndexError:\n",
    "                predicted_label = f\"Error: Index {predicted_index} out of bounds for task '{task}' labels\"\n",
    "            except KeyError:\n",
    "                predicted_label = f\"Error: Task '{task}' not found in label_maps\"\n",
    "\n",
    "            sample_predictions[task] = predicted_label\n",
    "            # Optional: Add the probability of the predicted class\n",
    "            sample_predictions[f\"{task}_probability\"] = float(task_probs[predicted_index])\n",
    "\n",
    "        results.append(sample_predictions)\n",
    "\n",
    "    # Return a single dict if only one spectrum was input, otherwise the list\n",
    "    return results[0] if single_input else results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage ---\n",
    "\n",
    "# Make sure your trained `model`, fitted `scaler`, `label_maps`, and `task_names`\n",
    "# are available in the current scope.\n",
    "\n",
    "# Example 1: Predict a single spectrum from the test set\n",
    "sample_index = 0 # Choose a sample from your test set\n",
    "single_spectrum_original = X_test[sample_index] # Get the *original*, unscaled data\n",
    "\n",
    "print(f\"Predicting for a single spectrum (Sample {sample_index} from original test set)...\")\n",
    "prediction_single = predict_spectra(single_spectrum_original, model, scaler, label_maps, TASK_NAMES)\n",
    "print(\"\\nPrediction Results (Single):\")\n",
    "import json # For pretty printing the dictionary\n",
    "#print(json.dumps(prediction_single, indent=2))\n",
    "print(prediction_single)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Example 2: Predict a small batch of spectra from the test set\n",
    "batch_spectra_original = X_test[5:8] # Get samples 5, 6, 7 (original, unscaled)\n",
    "\n",
    "print(f\"\\nPredicting for a batch of {len(batch_spectra_original)} spectra...\")\n",
    "predictions_batch = predict_spectra(batch_spectra_original, model, scaler, label_maps, TASK_NAMES)\n",
    "print(\"\\nPrediction Results (Batch):\")\n",
    "for i, pred in enumerate(predictions_batch):\n",
    "    print(f\"\\n--- Spectrum {i+1} ---\")\n",
    "    print(json.dumps(pred, indent=2))\n",
    "\n",
    "# Example 3: Using dummy data (ensure it has the correct number of bands)\n",
    "# num_bands_expected = scaler.n_features_in_\n",
    "# dummy_spectrum = np.random.rand(num_bands_expected) * 0.5 + 0.1 # Example random spectrum\n",
    "\n",
    "# print(\"\\nPredicting for a dummy spectrum...\")\n",
    "# prediction_dummy = predict_spectra(dummy_spectrum, model, scaler, label_maps, TASK_NAMES)\n",
    "# print(\"\\nPrediction Results (Dummy):\")\n",
    "# print(json.dumps(prediction_dummy, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the labeled data for the test spectrum\n",
    "for key, value in y_test_dict_orig.items():\n",
    "    if value.any():  # Check if the list is not empty\n",
    "        print(f\"{key}: {label_maps[key][value[sample_index]]}\")\n",
    "    else:\n",
    "        print(f\"{key} has an empty list.\")\n",
    "\n",
    "plt.plot(wl, X_test[sample_index], lw=1)\n",
    "plt.ylim(0, 1) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function to read a single ASD file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import specdal\n",
    "def read(filepath, jump_correct = False):\n",
    "    # Reads a single ASD file with metadata.\n",
    "    \n",
    "    # check data\n",
    "    if filepath[-4:] != '.asd':\n",
    "        print(f'WARNING: File {fname} does not appear to be an ASD file.')\n",
    "        return -1\n",
    "    \n",
    "    # read the asd file with specdal and asdreader\n",
    "    s = specdal.Spectrum(filepath=filepath)\n",
    "    fname = os.path.basename(filepath)\n",
    "\n",
    "    if (jump_correct):\n",
    "        wl = s.measurement.index\n",
    "\n",
    "        # Fix 1: shift 0<wl<1000 range up/down to smooth jump at 1000\n",
    "        i1 = np.where(wl==1000)[0][0]\n",
    "        if not np.isnan(s.measurement.iloc[i1]):\n",
    "            dp = ( ((s.measurement.iloc[i1+1]-s.measurement.iloc[i1+2]) + (s.measurement.iloc[i1-1]-s.measurement.iloc[i1]))/2 )\n",
    "            d1 = (s.measurement.iloc[i1+1]-s.measurement.iloc[i1])\n",
    "            s.measurement.iloc[:(i1+1)] = s.measurement.iloc[:(i1+1)] + dp + d1\n",
    "        # Fix 2: shift 1800<wl<2500 range up/down to smooth jump at 1800\n",
    "        i2 = np.where(wl==1800)[0][0]\n",
    "        if not np.isnan(s.measurement.iloc[i2]):\n",
    "            dp = ( ((s.measurement.iloc[i2+1]-s.measurement.iloc[i2+2]) + (s.measurement.iloc[i2-1]-s.measurement.iloc[i2]))/2 )\n",
    "            d1 = (s.measurement.iloc[i2+1]-s.measurement.iloc[i2])\n",
    "            s.measurement.iloc[(i2+1):] = s.measurement.iloc[(i2+1):] - dp - d1\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function to build an array of ASD spectra for batch prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def get_file_paths_single_level(directory):\n",
    "    file_paths = []\n",
    "    for entry in os.listdir(directory):\n",
    "        full_path = os.path.join(directory, entry)\n",
    "        if os.path.isfile(full_path):\n",
    "            file_paths.append(full_path)\n",
    "    return file_paths\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"/workspaces/NN_Hyperspectral_Vegetation/data/asd\"\n",
    "files = get_file_paths_single_level(directory_path)\n",
    "\n",
    "spectra_array = [read(filepath, True).measurement for filepath in files]\n",
    "\n",
    "spectra_array = np.asarray(spectra_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nPredicting for a batch of {len(spectra_array)} spectra...\")\n",
    "predictions_batch = predict_spectra(spectra_array, model, scaler, label_maps, TASK_NAMES)\n",
    "print(\"\\nPrediction Results (Batch):\")\n",
    "for i, pred in enumerate(predictions_batch):\n",
    "    print(f\"\\n--- Spectrum {i} ---\")\n",
    "    print(f\"Filepath: {files[i]}\")\n",
    "    plt.plot(wl, spectra_array[i], label=f'{files[i]}', lw=1)\n",
    "    plt.ylim(0, 1) \n",
    "    plt.show()\n",
    "    print(json.dumps(pred, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "fname_roi = 'data/pkl/ROIs_4-25_Ilex_vom.pkl'\n",
    "\n",
    "# Unpickling the dictionary\n",
    "with open(fname_roi, 'rb') as f:\n",
    "    roiData = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "fname_roi = 'data/pkl/ROI_Soli_sem.pkl'\n",
    "\n",
    "# Unpickling the dictionary\n",
    "with open(fname_roi, 'rb') as f:\n",
    "    roiData = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "fname_roi = 'data/pkl/ROIs_4-10.pkl'\n",
    "\n",
    "# Unpickling the dictionary\n",
    "with open(fname_roi, 'rb') as f:\n",
    "    roiData = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = roiData.df # a DataFrame holding all the data for the ROIs\n",
    "roi_names = roiData.names # the names of the ROIs\n",
    "roi_colors = roiData.colors # a Python dictionary of colors, where colors[class_name] will provide the color for class class_name\n",
    "roi_masks = roiData.masks # a Python dictionary of location masks, where colors[class_name] will provide the numpy array for the mask\n",
    "nClasses = len(roi_names)\n",
    "\n",
    "# Notes:\n",
    "#   Each mask is an array of bollean values of the same dimensions as the image, \n",
    "#   and True represents a pixel in the ROI while False represents not in the ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine how many rows of images (assuming each row has 2 masks)\n",
    "nRowsPlots = int(np.ceil(nClasses/2)) # (change size of figure as needed)\n",
    "plt.figure(figsize=(8, nRowsPlots*2))\n",
    "for i in range(nClasses):\n",
    "    name = roi_names[i]\n",
    "    # create the subplot\n",
    "    plt.subplot(nRowsPlots, 2, i+1)    \n",
    "    plt.imshow(roi_masks[name])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(f'ROI Mask for {name}');\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Imports\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import spectral\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import pandas\n",
    "import sklearn\n",
    "\n",
    "\n",
    "def make_rgb(imArr, wl, stretch=[2,98]):\n",
    "    \n",
    "    def stretch_arr(arr, stretch):\n",
    "        low_thresh_val = np.percentile(arr, stretch[0])\n",
    "        high_thresh_val = np.percentile(arr, stretch[1])\n",
    "        arr = np.clip(arr, a_min=low_thresh_val, a_max=high_thresh_val)\n",
    "        arr = arr - np.min(arr)\n",
    "        arr = arr/np.max(arr)\n",
    "        return arr\n",
    "    \n",
    "    # get the dimensions of the image aarray\n",
    "    nr,nc,nb = imArr.shape\n",
    "        \n",
    "    # determine the indices for the red, green, and blue bands\n",
    "    index_red_band = np.argmin(np.abs(wl-650))\n",
    "    index_green_band = np.argmin(np.abs(wl-550))\n",
    "    index_blue_band = np.argmin(np.abs(wl-460))  \n",
    "    \n",
    "    imRGB = np.zeros((nr,nc,3))\n",
    "    imRGB[:,:,0] = stretch_arr(np.squeeze(imArr[:,:,index_red_band]), stretch)\n",
    "    imRGB[:,:,1] = stretch_arr(np.squeeze(imArr[:,:,index_green_band]), stretch)\n",
    "    imRGB[:,:,2] = stretch_arr( np.squeeze(imArr[:,:,index_blue_band]), stretch)\n",
    "    \n",
    "    return imRGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the locations\n",
    "# create an RGB image\n",
    "RGB_image_with_ROIs = make_rgb(im.Arr, wl_img, stretch = [2,98])\n",
    "# Add the ROI locations\n",
    "for name in roi_names:\n",
    "    # get the color for this ROI, converted from hex to (r,g,b)\n",
    "    color = colors.to_rgb(roi_colors[name])\n",
    "    mask = roi_masks[name]\n",
    "    # set the pixel values for this ROI to (0,0,0)\n",
    "    for i in range(3):\n",
    "        RGB_image_with_ROIs[:,:,i] = RGB_image_with_ROIs[:,:,i]*(mask==False)\n",
    "    # set the pixel values for this ROI to the color for this ROI\n",
    "    for i in range(3):\n",
    "        RGB_image_with_ROIs[:,:,i] = RGB_image_with_ROIs[:,:,i] + mask*color[i]\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(RGB_image_with_ROIs)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(os.path.basename(fname));\n",
    "# Create legend patches: each patch is a colored box with the ROI name.\n",
    "legend_patches = [mpatches.Patch(color=roi_colors[name], label=name) for name in roi_names]\n",
    "# Add the legend to the right of the image\n",
    "plt.legend(handles=legend_patches, loc='center left', bbox_to_anchor=(1, 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_img = df.to_numpy()[:,4:] # an array that holds all the spectra (each row is a spectrum)\n",
    "spectra_img = spectra_img.astype(np.float32)\n",
    "spectra_img_names = df['Name'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(spectra_img_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean (red) and all individual spectra (blue) of each class\n",
    "# we save the mean for each class for use later\n",
    "means = {}\n",
    "for name in roi_names:\n",
    "    class_spectra = spectra_img[spectra_img_names==name,:]\n",
    "    class_nSpec = class_spectra.shape[0]\n",
    "    plt.figure(figsize=(10,6)) \n",
    "    for i in range(class_nSpec):\n",
    "        plt.plot(wl_img,class_spectra[i,:].flatten(), color='b', alpha=0.05)\n",
    "    means[name] = np.mean(class_spectra, axis=0)\n",
    "    plt.plot(wl_img, means[name], color='r')\n",
    "    plt.minorticks_on()\n",
    "    plt.grid(True, which='major', c='k')\n",
    "    plt.grid(True, which='minor', linewidth=0.25)\n",
    "    plt.title(f'ROI Spectra for {name}')\n",
    "    plt.xlabel('Wavelength (nm)')\n",
    "    plt.ylabel('Reflectance')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean (red) and all individual spectra (blue) of each class, scaling all spectra to have mean value of 1\n",
    "for name in roi_names:\n",
    "    class_spectra = spectra_img[spectra_img_names==name,:]\n",
    "    class_nSpec = class_spectra.shape[0]\n",
    "    plt.figure(figsize=(10,6)) \n",
    "    for i in range(class_nSpec):\n",
    "        plt.plot(wl_img,class_spectra[i,:].flatten()/np.mean(class_spectra[i,:].flatten()), color='b', alpha=0.05)\n",
    "plt.minorticks_on()\n",
    "plt.grid(True, which='major', c='k')\n",
    "plt.grid(True, which='minor', linewidth=0.25)\n",
    "plt.plot(wl_img, means[name]/np.mean(means[name]), color='r')\n",
    "plt.title(f'ROI Spectra for {name}, Normalized to Mean=1')\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Reflectance')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the means together\n",
    "plt.figure(figsize=(10,6)) \n",
    "for name in roi_names:\n",
    "    plt.plot(wl_img, means[name], label=name, color=roi_colors[name])\n",
    "plt.minorticks_on()\n",
    "plt.grid(True, which='major', c='k')\n",
    "plt.grid(True, which='minor', linewidth=0.25)\n",
    "plt.title(f'ROI Spectra for {name}')\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Reflectance')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the means together\n",
    "plt.figure(figsize=(10,6)) \n",
    "for name in roi_names:\n",
    "    plt.plot(wl_img, means[name]/np.mean(means[name]), label=name, color=roi_colors[name])\n",
    "plt.title(f'ROI Spectra for {name}, Normalized to Mean=1')\n",
    "plt.minorticks_on()\n",
    "plt.grid(True, which='major', c='k')\n",
    "plt.grid(True, which='minor', linewidth=0.25)\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Reflectance')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spectra_img.shape)\n",
    "print(spectra_img_names.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing predict on ROI spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "img_test_spec = means['Soli_sem']\n",
    "\n",
    "print(f\"Predicting for a single spectrum...\")\n",
    "prediction_single = predict_spectra(img_test_spec, model, scaler, label_maps, TASK_NAMES)\n",
    "print(\"\\nPrediction Results (Single):\")\n",
    "import json # For pretty printing the dictionary\n",
    "#print(json.dumps(prediction_single, indent=2))\n",
    "print(prediction_single)\n",
    "\n",
    "plt.plot(wl_img, img_test_spec, lw=1)\n",
    "plt.ylim(0, 1) \n",
    "plt.show()\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spectra_img_names[2069])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spectra_img[2069])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on an individual ROI pixel/spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with and without smoothing\n",
    "\n",
    "import warnings\n",
    "import scipy\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# parameters for smoothing the neurons\n",
    "sigma = 3\n",
    "window = 3\n",
    "\n",
    "\n",
    "img_test_spec1 = spectra_img[2069]\n",
    "\n",
    "#smooth_spectra1 = scipy.ndimage.gaussian_filter1d(img_test_spec1, sigma)\n",
    "\n",
    "#smooth_spectra1 = scipy.signal.savgol_filter(smooth_spectra1, window, 2)\n",
    "#smooth_spectra = scipy.signal.savgol_filter(neuronACt, window, 3)\n",
    "\n",
    "print(f\"Predicting for a single spectrum...\")\n",
    "prediction_single = predict_spectra(img_test_spec1, model, scaler, label_maps, TASK_NAMES)\n",
    "print(\"\\nPrediction Results (Single):\")\n",
    "import json # For pretty printing the dictionary\n",
    "#print(json.dumps(prediction_single, indent=2))\n",
    "print(prediction_single)\n",
    "\n",
    "plt.plot(wl_img, img_test_spec1, lw=1)\n",
    "plt.ylim(0, 1) \n",
    "plt.show()\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spectra_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on a batch of ROI spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with and without smoothing\n",
    "\n",
    "import scipy\n",
    "\n",
    "# parameters for smoothing the neurons\n",
    "sigma = 1.5\n",
    "window = 3\n",
    "\n",
    "print(roi_names)\n",
    "print(means)\n",
    "\n",
    "batch_ROI_means = np.empty([len(roi_names), len(wl_img)])\n",
    "for i, name in enumerate(roi_names):\n",
    "\n",
    "    smooth_mean = scipy.ndimage.gaussian_filter1d(means[name], sigma)\n",
    "    #smooth_mean = scipy.signal.savgol_filter(smooth_mean, window, 2)\n",
    "\n",
    "    #batch_ROI_means[i] = means[name]\n",
    "    batch_ROI_means[i] = smooth_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"\\nPredicting for a batch of {len(batch_ROI_means)} spectra...\")\n",
    "predictions_batch = predict_spectra(batch_ROI_means, model, scaler, label_maps, TASK_NAMES)\n",
    "\n",
    "# \n",
    "for i, pred in enumerate(predictions_batch):\n",
    "    print(f\"\\n--- Spectrum {i} ---\")\n",
    "    plt.plot(wl_img, batch_ROI_means[i], lw=1)\n",
    "    plt.ylim(0, 1) \n",
    "    plt.show()\n",
    "    print(json.dumps(pred, indent=2))\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the image into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image into memory\n",
    "im.Arr = im.load()\n",
    "print(f'Shape of Im.Arr = {im.Arr.shape}')\n",
    "im.List = np.reshape(im.Arr, (im.nrows*im.ncols, im.nbands))\n",
    "print(f'Shape of im.List = {im.List.shape}')\n",
    "\n",
    "valid_pixel_mask = np.sum(im.List, axis=1)>0\n",
    "\n",
    "dataList = im.List[valid_pixel_mask, :]\n",
    "print(f'Shape of dataList = {dataList.shape}')\n",
    "nr = im.nrows\n",
    "nc = im.ncols\n",
    "nb = im.nbands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify and plot image (memory-efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# =====================================================================================\n",
    "# --- SCRIPT TO CLASSIFY AND PLOT FULL IMAGE (MEMORY-EFFICIENT BATCH PROCESSING) ---\n",
    "# This script processes the image in smaller batches to avoid running out of RAM,\n",
    "# making it suitable for very large hyperspectral images.\n",
    "# =====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- Starting Full Image Classification (Memory-Efficient) ---\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Configuration for Batch Processing ---\n",
    "# Adjust this batch size based on your available RAM and GPU memory.\n",
    "# A larger batch size is faster but uses more memory. Start with this value.\n",
    "PREDICTION_BATCH_SIZE = 32768\n",
    "\n",
    "# --- 1. Prepare Data and Pre-allocate Output Maps ---\n",
    "\n",
    "# Create label maps (string label to integer ID) for efficient conversion later.\n",
    "label_maps = {\n",
    "    'plant': y_plant_labels,\n",
    "    'age': y_age_labels,\n",
    "    'part': y_part_labels,\n",
    "    'health': y_health_labels,\n",
    "    'lifecycle': y_lifecycle_labels\n",
    "}\n",
    "label_to_int_maps = {task: {label: i for i, label in enumerate(labels)}\n",
    "                     for task, labels in label_maps.items()}\n",
    "\n",
    "# Identify valid (non-background) pixels.\n",
    "#valid_pixel_mask = np.sum(im.List, axis=1) > 0\n",
    "#valid_spectra = im.List[valid_pixel_mask]\n",
    "\n",
    "valid_spectra = dataList\n",
    "\n",
    "valid_pixel_indices = np.where(valid_pixel_mask)[0]\n",
    "\n",
    "\n",
    "n_valid_pixels = len(valid_spectra)\n",
    "print(f\"Found {n_valid_pixels} valid pixels to classify.\")\n",
    "\n",
    "# Pre-allocate flat arrays to store the integer classification results.\n",
    "# We initialize with -1 for the background.\n",
    "# Using np.int16 is more memory-efficient than the default int64.\n",
    "classification_maps_flat = {\n",
    "    task: np.full(im.nrows * im.ncols, -1, dtype=np.int16)\n",
    "    for task in TASK_NAMES\n",
    "}\n",
    "\n",
    "# --- 2. Run Prediction in Batches ---\n",
    "\n",
    "print(f\"Starting prediction with a batch size of {PREDICTION_BATCH_SIZE}...\")\n",
    "num_batches = int(np.ceil(n_valid_pixels / PREDICTION_BATCH_SIZE))\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * PREDICTION_BATCH_SIZE\n",
    "    end_idx = min((i + 1) * PREDICTION_BATCH_SIZE, n_valid_pixels)\n",
    "\n",
    "    print(f\"  Processing batch {i+1}/{num_batches} (pixels {start_idx} to {end_idx-1})...\")\n",
    "\n",
    "    # Get the current batch of spectra\n",
    "    batch_spectra = valid_spectra[start_idx:end_idx]\n",
    "\n",
    "    # Predict on the small batch\n",
    "    batch_predictions = predict_spectra(batch_spectra, model, scaler, label_maps, TASK_NAMES)\n",
    "\n",
    "    # Get the original global indices for this batch's pixels\n",
    "    global_indices_for_batch = valid_pixel_indices[start_idx:end_idx]\n",
    "\n",
    "    # Populate the pre-allocated maps with the results for this batch\n",
    "    for task in TASK_NAMES:\n",
    "        # Get the integer-to-label map for the current task\n",
    "        label_to_int = label_to_int_maps[task]\n",
    "\n",
    "        # Extract predicted string labels and convert to integers\n",
    "        predicted_labels = [p[task] for p in batch_predictions]\n",
    "        predicted_ints = np.array([label_to_int.get(label, -1) for label in predicted_labels], dtype=np.int16)\n",
    "        \n",
    "        # Place the integer results into the correct locations in the final flat map\n",
    "        classification_maps_flat[task][global_indices_for_batch] = predicted_ints\n",
    "\n",
    "print(\"\\nAll batches processed. Prediction complete.\")\n",
    "\n",
    "\n",
    "# --- 3. Plot the Final Classification Maps ---\n",
    "\n",
    "print(\"\\nGenerating classification plots...\")\n",
    "for task in TASK_NAMES:\n",
    "    print(f\"  Plotting map for task: '{task}'...\")\n",
    "\n",
    "    # Reshape the final flat map back to the original 2D image dimensions\n",
    "    classification_image = classification_maps_flat[task].reshape((im.nrows, im.ncols))\n",
    "\n",
    "    # --- Plotting (same logic as before) ---\n",
    "    labels = label_maps[task]\n",
    "    n_classes = len(labels)\n",
    "\n",
    "    # Create a discrete colormap with black for the background (-1)\n",
    "    base_cmap = plt.cm.get_cmap('tab20', n_classes)\n",
    "    custom_colors = np.vstack(([0, 0, 0, 1], base_cmap(np.arange(n_classes))))\n",
    "    custom_cmap = colors.ListedColormap(custom_colors)\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    ax.imshow(classification_image, cmap=custom_cmap, vmin=-1, vmax=n_classes - 1, interpolation='none')\n",
    "    ax.set_title(f'Pixel Classification Map: {task.capitalize()}', fontsize=16)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Create and place the legend\n",
    "    legend_patches = [mpatches.Patch(color=custom_cmap(i + 1), label=label)\n",
    "                      for i, label in enumerate(labels)]\n",
    "    ax.legend(handles=legend_patches, bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- All classification maps have been generated. ---\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot images only (using a different colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating classification plots...\")\n",
    "for task in TASK_NAMES:\n",
    "    print(f\"  Plotting map for task: '{task}'...\")\n",
    "\n",
    "    # Reshape the final flat map back to the original 2D image dimensions\n",
    "    classification_image = classification_maps_flat[task].reshape((im.nrows, im.ncols))\n",
    "\n",
    "    # --- Plotting (same logic as before) ---\n",
    "    labels = label_maps[task]\n",
    "    n_classes = len(labels)\n",
    "\n",
    "    # Create a discrete colormap with black for the background (-1)\n",
    "    base_cmap = plt.cm.get_cmap('Paired', n_classes)\n",
    "    custom_colors = np.vstack(([0, 0, 0, 1], base_cmap(np.arange(n_classes))))\n",
    "    custom_cmap = colors.ListedColormap(custom_colors)\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    ax.imshow(classification_image, cmap=custom_cmap, vmin=-1, vmax=n_classes - 1, interpolation='none')\n",
    "    ax.set_title(f'Pixel Classification Map: {task.capitalize()}', fontsize=16)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Create and place the legend\n",
    "    legend_patches = [mpatches.Patch(color=custom_cmap(i + 1), label=label)\n",
    "                      for i, label in enumerate(labels)]\n",
    "    ax.legend(handles=legend_patches, bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- All classification maps have been generated. ---\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Older code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# Make sure your trained `model`, fitted `scaler`, `label_maps`, and `task_names`\n",
    "# are available in the current scope.\n",
    "\n",
    "# Example 1: Predict a single spectrum\n",
    "sample_index = 0 # Choose a sample\n",
    "pixel = dataList[0] # Get the *original*, unscaled data\n",
    "\n",
    "print(f\"Predicting for a single spectrum (Sample {sample_index} from image)...\")\n",
    "prediction_single = predict_spectra(pixel, model, scaler, label_maps, TASK_NAMES)\n",
    "print(\"\\nPrediction Results (Single):\")\n",
    "import json # For pretty printing the dictionary\n",
    "#print(json.dumps(prediction_single, indent=2))\n",
    "print(prediction_single)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "plt.plot(wl_img, pixel, lw=1)\n",
    "plt.ylim(0, 1) \n",
    "plt.show()\n",
    "\n",
    "# Example 2: Predict a small batch of spectra\n",
    "batch_pixels = dataList[1:100] # Get samples 5, 6, 7 (original, unscaled)\n",
    "\n",
    "print(f\"\\nPredicting for a batch of {len(batch_pixels)} spectra...\")\n",
    "predictions_batch = predict_spectra(batch_pixels, model, scaler, label_maps, TASK_NAMES)\n",
    "print(\"\\nPrediction Results (Batch):\")\n",
    "# for i, pred in enumerate(predictions_batch):\n",
    "#     print(f\"\\n--- Spectrum {i+1} ---\")\n",
    "#     print(json.dumps(pred, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# =====================================================================================\n",
    "# --- SCRIPT TO CLASSIFY AND PLOT FULL IMAGE ---\n",
    "# This script uses the trained model to predict the class for each pixel\n",
    "# in the hyperspectral image and then generates a classification map for each task.\n",
    "# =====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- Starting Full Image Classification and Plotting ---\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 1. Prepare Data for Prediction ---\n",
    "\n",
    "# Create the label_maps dictionary required by the predict_spectra function.\n",
    "# This maps each task to its corresponding array of string labels.\n",
    "label_maps = {\n",
    "    'plant': y_plant_labels,\n",
    "    'age': y_age_labels,\n",
    "    'part': y_part_labels,\n",
    "    'health': y_health_labels,\n",
    "    'lifecycle': y_lifecycle_labels\n",
    "}\n",
    "\n",
    "# The variable `dataList` from your setup code already contains the spectra\n",
    "# for non-background pixels. We will predict on this for efficiency.\n",
    "# The `im.List` variable contains all pixels, including background (zero-spectra).\n",
    "#valid_pixel_mask = np.sum(im.List, axis=1) > 0\n",
    "#valid_spectra = im.List[valid_pixel_mask]\n",
    "\n",
    "#valid_spectra = dataList2_resampled\n",
    "valid_spectra = dataList\n",
    "\n",
    "# --- 2. Run Prediction on All Valid Pixels ---\n",
    "\n",
    "print(f\"Predicting classifications for {len(valid_spectra)} valid pixels in the image...\")\n",
    "# This may take a few moments depending on your hardware and image size.\n",
    "image_predictions = predict_spectra(valid_spectra, model, scaler, label_maps, TASK_NAMES)\n",
    "print(\"Prediction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3. Generate and Plot Classification Maps for Each Task ---\n",
    "\n",
    "for task in TASK_NAMES:\n",
    "    print(f\"\\nGenerating plot for task: '{task}'...\")\n",
    "\n",
    "    # Create a map of integer class IDs from the string labels\n",
    "    labels = label_maps[task]\n",
    "    label_to_int = {label: i for i, label in enumerate(labels)}\n",
    "    \n",
    "    # Extract the predicted string labels for the current task from the list of dicts\n",
    "    predicted_labels_for_task = [p[task] for p in image_predictions]\n",
    "    \n",
    "    # Convert string predictions to their corresponding integer IDs\n",
    "    predicted_ints_for_task = np.array([label_to_int.get(label, -1) for label in predicted_labels_for_task])\n",
    "\n",
    "    # --- Create the full-size image map ---\n",
    "    # Initialize a full-size map with a background value (-1)\n",
    "    # The map is initially flat to align with the `valid_pixel_mask`\n",
    "    classification_map_flat = np.full(im.nrows * im.ncols, -1, dtype=int)\n",
    "    \n",
    "    # Place the predicted integer classes into the map at the correct pixel locations\n",
    "    classification_map_flat[valid_pixel_mask] = predicted_ints_for_task\n",
    "    \n",
    "    # Reshape the flat map back to the original 2D image dimensions\n",
    "    classification_image = classification_map_flat.reshape((im.nrows, im.ncols))\n",
    "\n",
    "    # --- Plotting ---\n",
    "    n_classes = len(labels)\n",
    "    \n",
    "    # Create a discrete colormap. We'll add a color for the background (-1).\n",
    "    # 'tab20' is a good choice for categorical data with many classes.\n",
    "    base_cmap = plt.cm.get_cmap('tab20', n_classes)\n",
    "    cmap_colors = base_cmap(np.arange(n_classes))\n",
    "    \n",
    "    # Prepend black for the background class (-1)\n",
    "    custom_colors = np.vstack(([0, 0, 0, 1], cmap_colors)) # [R, G, B, Alpha]\n",
    "    custom_cmap = colors.ListedColormap(custom_colors)\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Display the classification image. vmin=-1 ensures background is mapped to black.\n",
    "    im_plot = ax.imshow(classification_image, cmap=custom_cmap, vmin=-1, vmax=n_classes - 1)\n",
    "    \n",
    "    ax.set_title(f'Pixel Classification Map: {task.capitalize()}', fontsize=16)\n",
    "    ax.axis('off') # Hide the axes ticks and labels\n",
    "\n",
    "    # Create a legend to map colors back to their string labels\n",
    "    legend_patches = [mpatches.Patch(color=custom_cmap(i + 1), label=label) \n",
    "                      for i, label in enumerate(labels)]\n",
    "    \n",
    "    # Place the legend outside the plot area for clarity\n",
    "    ax.legend(handles=legend_patches, bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- All classification maps have been generated. ---\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save classification maps to ENVI files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spectral\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# This script assumes the following variables from your previous code are in memory:\n",
    "# - im: The original spectral image object, containing metadata.\n",
    "# - classification_maps_flat: A dictionary of your flat (1D) classification results.\n",
    "# - TASK_NAMES: A list of the task names (e.g., ['plant', 'age', 'part']).\n",
    "# - label_maps: A dictionary mapping task names to their corresponding list of class labels.\n",
    "# - fname_hdr: The file path to the original image's header file.\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- Saving Classification Maps to ENVI Files ---\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 1. Define Output Directory and Create It ---\n",
    "\n",
    "# Specify the name for the new directory where results will be saved.\n",
    "# This can be a relative path (like below) or an absolute path (e.g., 'C:/Users/YourUser/Desktop/Results').\n",
    "output_dir = 'output/9-10-25'\n",
    "\n",
    "# Create the output directory if it does not already exist.\n",
    "# os.makedirs() is used as it won't raise an error if the directory already exists\n",
    "# and it can create parent directories if needed.\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Output files will be saved to the directory: '{os.path.abspath(output_dir)}'\")\n",
    "\n",
    "# Get the base name of the input file to create unique output filenames.\n",
    "base_name = os.path.splitext(os.path.basename(fname_hdr))[0]\n",
    "\n",
    "# --- 2. Iterate Through Each Classification Task and Save ---\n",
    "\n",
    "for task in TASK_NAMES:\n",
    "    print(f\"\\nProcessing and saving map for task: '{task}'...\")\n",
    "\n",
    "    # --- a. Reshape the Flat Data to a 2D Image ---\n",
    "    # The flat array is reshaped back to the original image's dimensions.\n",
    "    flat_map = classification_maps_flat[task]\n",
    "    reshaped_map = np.reshape(flat_map, (im.nrows, im.ncols))\n",
    "\n",
    "    # --- b. Define the Output Filename within the New Directory ---\n",
    "    # The output filename is now constructed using the specified output_dir.\n",
    "    output_filename = os.path.join(output_dir, f\"{base_name}_{task}_classification.hdr\")\n",
    "\n",
    "    # --- c. Get the Class Names for the Header File ---\n",
    "    # The order of these names corresponds to the integer values (0, 1, 2, ...) in your map.\n",
    "    class_names = label_maps[task]\n",
    "\n",
    "    # --- d. Save the Classification Map ---\n",
    "    # We pass the original image's metadata to preserve the georeferencing data.\n",
    "    # The class_names are also saved into the header for easy interpretation in ENVI software.\n",
    "    spectral.envi.save_classification(\n",
    "        output_filename,\n",
    "        reshaped_map,\n",
    "        metadata=im.metadata,\n",
    "        class_names=class_names\n",
    "    )\n",
    "\n",
    "    print(f\"  -> Successfully saved to: {output_filename}\")\n",
    "\n",
    "print(\"\\nAll classification maps have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display ENVI classification images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spectral\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def display_envi_classification(hdr_path):\n",
    "    \"\"\"\n",
    "    Opens and displays an ENVI classification image with a proper legend.\n",
    "\n",
    "    Args:\n",
    "        hdr_path (str): The full file path to the .hdr file of the classification image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- 1. Open the ENVI Classification File ---\n",
    "        # The spectral.open_image function reads the header and prepares to load data.\n",
    "        img = spectral.open_image(hdr_path)\n",
    "        print(f\"Successfully opened: {hdr_path}\")\n",
    "        print(f\"Image dimensions (Rows, Cols, Bands): {img.shape}\")\n",
    "\n",
    "        # --- 2. Load the Classification Data ---\n",
    "        # .load() reads the entire image into a NumPy array.\n",
    "        # For single-band classification images, the shape will be (rows, cols, 1).\n",
    "        # We use [:, :, 0] or np.squeeze() to get a 2D array for plotting.\n",
    "        class_map = img.load()\n",
    "        if class_map.ndim == 3 and class_map.shape[2] == 1:\n",
    "            class_map = class_map[:, :, 0]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Could not open or read the file '{hdr_path}'.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Extract Metadata for the Legend ---\n",
    "    # The class names are stored in the header's metadata.\n",
    "    # The background value (-1 in our case) is often not in the class names list.\n",
    "    class_names = img.metadata.get('class names', [])\n",
    "    if not class_names:\n",
    "        print(\"Warning: Could not find 'class names' in the header file.\")\n",
    "        # Create generic names if none are found\n",
    "        num_classes = int(class_map.max()) + 1\n",
    "        class_names = [f'Class {i}' for i in range(num_classes)]\n",
    "\n",
    "    # --- 4. Prepare for Plotting ---\n",
    "    \n",
    "    # Get the unique integer values in our map (e.g., -1, 0, 1, 2...).\n",
    "    # The background is -1, and classes start at 0.\n",
    "    class_values = np.unique(class_map)\n",
    "    \n",
    "    # Use a default colormap (like 'jet' or 'viridis') and add a specific color for the background.\n",
    "    # Matplotlib's 'tab20' is good for categorical data.\n",
    "    cmap_colors = plt.cm.get_cmap('Paired', len(class_names))\n",
    "    \n",
    "    # Create a new list of colors. Start with gray for the background (-1).\n",
    "    colors = ['#808080'] + [cmap_colors(i) for i in range(len(class_names))]\n",
    "    \n",
    "    # Create a colormap object from our list of colors.\n",
    "    custom_cmap = ListedColormap(colors)\n",
    "    \n",
    "    # Create a normalization object to map each class value to the correct color.\n",
    "    # The boundaries ensure that each integer value gets a solid color block.\n",
    "    bounds = np.arange(len(colors) + 1) - 1.5 # e.g., [-1.5, -0.5, 0.5, 1.5, ...]\n",
    "    norm = plt.matplotlib.colors.BoundaryNorm(bounds, len(colors))\n",
    "\n",
    "    # --- 5. Create the Plot and Legend ---\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # Display the classification map using our custom colormap and normalization.\n",
    "    im_plot = ax.imshow(class_map, cmap=custom_cmap, norm=norm)\n",
    "\n",
    "    ax.set_title(f\"Classification Map: {img.metadata.get('description', 'Untitled')}\", fontsize=16)\n",
    "    ax.set_xlabel(\"Column\")\n",
    "    ax.set_ylabel(\"Row\")\n",
    "\n",
    "    # Create legend handles manually.\n",
    "    legend_labels = ['Background'] + class_names\n",
    "    patches = [mpatches.Patch(color=colors[i], label=legend_labels[i]) for i in range(len(legend_labels))]\n",
    "\n",
    "    # Add the legend to the plot.\n",
    "    ax.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "    # Improve layout and display the plot.\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make room for legend\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- USAGE EXAMPLE ---\n",
    "# Replace this with the actual path to one of your output classification files.\n",
    "# For example, if you saved a 'plant' classification for 'my_image.hdr'.\n",
    "file_to_display = 'output/9-10-25/raw_39219_or_ref_plant_classification.hdr' \n",
    "\n",
    "# Call the function to display the image\n",
    "display_envi_classification(file_to_display)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
