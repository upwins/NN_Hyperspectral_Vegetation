{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Configuration ---\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 600 # Set a large number, EarlyStopping will find the best\n",
    "LEARNING_RATE = 1e-4\n",
    "PATIENCE = 30 # For Early Stopping\n",
    "IGNORE_VALUE = -1 # Integer value to represent 'N' or ignored labels\n",
    "\n",
    "# Task names (consistent keys/output layer names)\n",
    "TASK_NAMES = ['plant', 'age', 'part', 'health', 'lifecycle']\n",
    "OUTPUT_NAMES = [f\"{task}_output\" for task in TASK_NAMES]\n",
    "\n",
    "# Loss weights (weighting between tasks, start equal)\n",
    "LOSS_WEIGHTS = {\n",
    "    'plant_output': 1.0,\n",
    "    'age_output': 1.0,\n",
    "    'part_output': 1.0,\n",
    "    'health_output': 1.0,\n",
    "    'lifecycle_output': 1.0\n",
    "}\n",
    "\n",
    "print(f\"Using TensorFlow version: {tf.__version__}\")\n",
    "# Optional: Configure GPU memory growth if needed\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Configured memory growth for {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load full spectral library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('util/')\n",
    "import importlib\n",
    "\n",
    "import util_scripts as util\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "MONGO_DBR_URI = os.getenv('MONGO_DBR_URI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_data_driver = False\n",
    "\n",
    "if (reload_data_driver):\n",
    "\n",
    "    records = []\n",
    "\n",
    "    # Create a new client and connect to the server\n",
    "    client = MongoClient(MONGO_DBR_URI, server_api=ServerApi('1'))\n",
    "\n",
    "    # Send a ping to confirm a successful connection\n",
    "    try:\n",
    "        client.admin.command('ping')\n",
    "        print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    db = client[\"upwins_db\"]\n",
    "    view_name = \"spectral_library\"\n",
    "    spectral_library = db[view_name]\n",
    "\n",
    "    records = spectral_library.find()\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_pickle('data/pkl/library.pkl')\n",
    "\n",
    "else:\n",
    "    df = pd.read_pickle('data/pkl/library.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = util.SpectralCollection(df)\n",
    "wl_lib = sc.wl\n",
    "name = sc.name\n",
    "spectra = sc.spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5734, 2151)\n"
     ]
    }
   ],
   "source": [
    "print(sc.spectra.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Nano imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Im.Arr = (1582, 419, 272)\n",
      "Shape of im.List = (662858, 272)\n",
      "Shape of dataList = (662219, 272)\n"
     ]
    }
   ],
   "source": [
    "# Currently this is used only to get wl_img\n",
    "\n",
    "import spectral\n",
    "\n",
    "fname = 'data/5-8-2025/100133_Allied_05_08_2025_2015_06_04_17_50_15/raw_0_ref'\n",
    "\n",
    "# Open the image and read into an array\n",
    "im = spectral.envi.open(fname+'.hdr', fname)\n",
    "wl_img = np.asarray(im.bands.centers)\n",
    "# Load the image into memory\n",
    "im.Arr = im.load()\n",
    "print(f'Shape of Im.Arr = {im.Arr.shape}')\n",
    "im.List = np.reshape(im.Arr, (im.nrows*im.ncols, im.nbands))\n",
    "print(f'Shape of im.List = {im.List.shape}')\n",
    "dataList = im.List[np.sum(im.List, axis=1)>0, :]\n",
    "print(f'Shape of dataList = {dataList.shape}')\n",
    "nr = im.nrows\n",
    "nc = im.ncols\n",
    "nb = im.nbands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2151,)\n",
      "(272,)\n"
     ]
    }
   ],
   "source": [
    "print(wl_lib.shape)\n",
    "print(wl_img.shape)\n",
    "\n",
    "#print(wl_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the spectral library is (5734, 272).\n",
      "(5734 spectra with 272 bands.)\n"
     ]
    }
   ],
   "source": [
    "# Create a BandResampler, which is a function that resamples spectra from one source to match a different source.\n",
    "# See: https://www.spectralpython.net/class_func_ref.html?highlight=resampling#spectral.algorithms.resampling.BandResampler\n",
    "# Inputs: the first input is the wavelengths for the spectra that you are going to resample, the second input is the wavelengths that you want to resample to.\n",
    "resampler = spectral.BandResampler(wl_lib, wl_img)\n",
    "spectra_resampled = resampler(spectra.T).T\n",
    "\n",
    "print(f'The shape of the spectral library is {spectra_resampled.shape}.')\n",
    "print(f'({spectra_resampled.shape[0]} spectra with {spectra_resampled.shape[1]} bands.)')\n",
    "\n",
    "spectra = spectra_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip the next two cells unless there is a need to filter inputs\n",
    "\n",
    "filter = {\n",
    "    'name': 'Chasmanthium_latifolium',\n",
    "    #'fname': '',\n",
    "    #'genus': '',\n",
    "    #'species': '',\n",
    "    'age': 'M',\n",
    "    'health': 'H',\n",
    "    'part': 'L',\n",
    "    #'type': '',\n",
    "    #'lifecycle': '',\n",
    "    #'date': ''\n",
    "}\n",
    "\n",
    "prediction_class = 'age'\n",
    "\n",
    "selected_indices = sc.select_indicies_with_filter(filter)\n",
    "\n",
    "nSpec = len(selected_indices)\n",
    "print(f\"Number of Spectra: {nSpec}\")\n",
    "\n",
    "print(sc.age[selected_indices])\n",
    "print(sc.principle_part[selected_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = {}\n",
    "class_spectra = spectra[selected_indices]\n",
    "class_nSpec = class_spectra.shape[0]\n",
    "\n",
    "plt.figure(figsize=(10,6)) \n",
    "for i in range(class_nSpec):\n",
    "    plt.plot(wl,class_spectra[i,:].flatten(), color='b', alpha=0.05)\n",
    "means[filter['name']] = np.mean(class_spectra, axis=0)\n",
    "plt.plot(wl, means[filter['name']], color='r')\n",
    "plt.minorticks_on()\n",
    "plt.grid(True, which='major', c='k')\n",
    "plt.grid(True, which='minor', linewidth=0.25)\n",
    "plt.title(f'Spectra for {filter['name']}')\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Reflectance')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare ROI data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def find_roi_files(root_dir):\n",
    "    string_list = ['.pkl', 'roi']\n",
    "    \n",
    "    matching_files = []\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        for filename in files:\n",
    "            if all(string in filename.lower() for string in string_list):\n",
    "                matching_files.append(os.path.join(root, filename))\n",
    "    return matching_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project codes for labeling ROI data\n",
    "# **IMPORTANT**: ROIs should be named using the **same** naming convention used to label ASD files \n",
    "\n",
    "plant_codes = {\n",
    "    'Ammo_bre': ['Ammophila', 'breviligulata', 'American Beachgrass', 'grass', 'https://en.wikipedia.org/wiki/Ammophila_breviligulata'],\n",
    "    'Chas_lat': ['Chasmanthium', 'latifolium', 'River Oats', 'grass', 'https://en.wikipedia.org/wiki/Chasmanthium_latifolium'],\n",
    "    'Pani_ama': ['Panicum', 'amarum', 'Coastal Panic Grass', 'grass', 'https://en.wikipedia.org/wiki/Panicum_amarum'],\n",
    "    'Pani_vir': ['Panicum', 'virgatum', 'Switch Grass', 'grass', 'https://en.wikipedia.org/wiki/Panicum_virgatum'],\n",
    "    'Soli_sem': ['Solidago', 'sempervirens', 'Seaside Goldenrod', 'succulent', 'https://en.wikipedia.org/wiki/Chasmanthium_latifolium'],\n",
    "    'Robi_his': ['Robinia', 'hispida', 'Bristly locust', 'shrub', 'https://en.wikipedia.org/wiki/Robinia_hispida'],\n",
    "    'More_pen': ['Morella', 'pennsylvanica', 'Bristly locust', 'shrub', 'https://en.wikipedia.org/wiki/Myrica_pensylvanica'],    \n",
    "    'Rosa_rug': ['Rosa', 'rugosa', 'Sandy Beach Rose', 'shrub', 'https://en.wikipedia.org/wiki/Rosa_rugosa'],\n",
    "    'Cham_fas': ['Chamaecrista', 'fasciculata', 'Partridge Pea', 'legume', 'https://en.wikipedia.org/wiki/Chamaecrista_fasciculata'],\n",
    "    'Soli_rug': ['Solidago', 'rugosa', 'Wrinkleleaf goldenrod', 'shrub', 'https://en.wikipedia.org/wiki/Solidago_rugosa'],\n",
    "    'Bacc_hal': ['Baccharis', 'halimifolia', 'Groundseltree', 'shrub', 'https://en.wikipedia.org/wiki/Baccharis_halimifolia'],\n",
    "    'Iva_fru_': ['Iva', 'frutescens', 'Jesuits Bark ', 'shrub', 'https://en.wikipedia.org/wiki/Iva_frutescens'],\n",
    "    'Ilex_vom': ['Ilex', 'vomitoria', 'Yaupon Holly', 'evergreen shrub', 'https://en.wikipedia.org/wiki/Ilex_vomitoria']\n",
    "}  \n",
    "age_codes = {  \n",
    "    'PE': ['Post Germination Emergence', 'PE'],\n",
    "\t#'RE': ['Re-emergence', 'RE'],\n",
    "    #'RE': ['Year 1 growth', '1G'],\n",
    "\t#'E': ['Emergence (from seed)', 'E'],\n",
    "    'E': ['Post Germination Emergence', 'PE'],\n",
    "\t#'D': ['Dormant', 'D'],\n",
    "\t'1G': ['Year 1 growth', '1G'],\n",
    "    '2G': ['Year 2 growth', '2G'],\n",
    "\t#'1F': ['Year 1 Flowering', '1F'],\n",
    "    'J': ['Juvenile', 'J'],\n",
    "\t'M': ['Mature', 'M']\n",
    "}\n",
    "principal_part_codes = {  \n",
    "    'MX': ['Mix', 'MX'],\n",
    "    #'S': ['Seed', 'SE'],\n",
    "\t#'SA': ['Shoot Apex', 'SA'],\n",
    "    'SA': ['Internode Stem', 'ST'],\n",
    "\t'L': ['Leaf/Blade', 'L'],\n",
    "\t#'IS': ['Internode Stem', 'IS'],\n",
    "    'ST': ['Internode Stem', 'ST'],\n",
    "    'SP': ['Sprout', 'SP'],\n",
    "\t#'CS': ['Colar Sprout', 'CS'],\n",
    "    'CS': ['Sprout', 'SP'],\n",
    "\t#'RS': ['Root Sprout', 'RS'],\n",
    "    'RS': ['Sprout', 'SP'],\n",
    "\t'LG': ['Lignin', 'LG'],\n",
    "\t'FL': ['Flower', 'FL'],\n",
    "    #'B': ['Blade', 'B'],\n",
    "\t'B': ['Leaf/Blade', 'L'],\n",
    "    'FR': ['Fruit', 'FR'],\n",
    "\t#'S': ['Seed', 'SE'], #moved above because 'S' is in other codes; this is an old code\n",
    "    'SE': ['Seed', 'SE'],\n",
    "\t#'St': ['Stalk', 'St']\n",
    "}\n",
    "health_codes = {\n",
    "    'MH': ['Healthy/Unhealthy Mix', 'MH'],\n",
    "\t'DS': ['Drought Stress', 'DS'],\n",
    "\t'SS': ['Salt Stress (soak)', 'SS'],\n",
    "    'SY': ['Salt Stress (spray)', 'SY'],\n",
    "\t'S': ['Stressed', 'S'],\n",
    "    'LLRZ': ['LLRZ Lab Stress', 'LLRZ'],\n",
    "\t#'D': ['Dormant', 'D'],\n",
    "    'R': ['Rust', 'R'],\n",
    "    'H': ['Healthy', 'H']\n",
    "}\n",
    "\n",
    "lifecycle_codes = { \n",
    "\t'D': ['Dormant', 'D'],\n",
    "    'RE': ['Re-emergence', 'RE'],\n",
    "    'FLG': ['Flowering', 'FLG'],\n",
    "    'FRG': ['Fruiting', 'FRG'],\n",
    "    \"FFG\": ['Fruiting and Flowering', 'FFG'],\n",
    "    'N': ['Neither', 'N']\n",
    "}\n",
    "\n",
    "# data lists\n",
    "\n",
    "d_spectra = []\n",
    "d_plant = []\n",
    "d_part = []\n",
    "d_health = []\n",
    "d_age = []\n",
    "d_lifecycle = []\n",
    "\n",
    "yd_all_dict = {\n",
    "    'plant': d_plant,\n",
    "    'age': d_age,\n",
    "    'part': d_part,\n",
    "    'health': d_health,\n",
    "    'lifecycle': d_lifecycle\n",
    "}\n",
    "\n",
    "code_category_dict = {\n",
    "    'plant': plant_codes,\n",
    "    'age': age_codes,\n",
    "    'part': principal_part_codes,\n",
    "    'health': health_codes,\n",
    "    'lifecycle': lifecycle_codes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ROI files found: 3\n",
      "Number of ROIs found in data/pkl/rois/ROIs_5-8-2025_More_pen.pkl: 4\n",
      "['More_pen_L_J_H_N_0', 'More_pen_L_J_H_N_1', 'More_pen_LG_J_H_N_0', 'More_pen_L_M_H_N_0']\n",
      "Number of ROIs found in data/pkl/rois/ROIs_5-8-2025_Chas_lat.pkl: 9\n",
      "['Chas_lat_L_1G_H_N_0', 'Chas_lat_L_1G_H_N_1', 'Chas_lat_L_J_H_N_0', 'Chas_lat_L_J_H_N_1', 'Chas_lat_L_J_H_N_2', 'Chas_lat_L_M_H_FFG_0', 'Chas_lat_L_M_H_FFG_1', 'Chas_lat_FL_M_H_FFG_0', 'Chas_lat_FL_M_H_FFG_1']\n",
      "Number of ROIs found in data/pkl/rois/ROIs_5-8-2025_Rosa_rug.pkl: 10\n",
      "['Rosa_rug_L_1G_H_N_0', 'Rosa_rug_L_1G_H_N_1', 'Rosa_rug_L_1G_H_N_2', 'Rosa_rug_L_J_H_FLG_0', 'Rosa_rug_L_J_H_FLG_1', 'Rosa_rug_FL_J_H_FLG_0', 'Rosa_rug_FL_J_H_FLG_1', 'Rosa_rug_L_J_H_FLG_2', 'Rosa_rug_ST_J_H_FLG_0', 'Rosa_rug_ST_J_H_FLG_1']\n"
     ]
    }
   ],
   "source": [
    "## Prepare ROI data\n",
    "\n",
    "roi_files = find_roi_files('data/pkl/rois/')\n",
    "\n",
    "print(f\"Number of ROI files found: {len(roi_files)}\")\n",
    "\n",
    "for roi_filename in roi_files:\n",
    "   # Unpickling the dictionary\n",
    "    with open(roi_filename, 'rb') as f:\n",
    "        roiData = pickle.load(f)\n",
    "        roi_df = roiData.df # a DataFrame holding all the data for the ROI\n",
    "\n",
    "    roi_spectra = roi_df.to_numpy()[:,4:]\n",
    "    roi_spectra = roi_spectra.astype(np.float32)\n",
    "    roi_spectra_names = roi_df['Name'].to_numpy()\n",
    "\n",
    "    roi_names = roiData.names # the names of the ROIs\n",
    "\n",
    "    print(f\"Number of ROIs found in {roi_filename}: {len(roi_names)}\")\n",
    "    print(roi_names)\n",
    "\n",
    "    for name in roi_names:\n",
    "        roi_class_spectra = roi_spectra[roi_spectra_names==name]\n",
    "\n",
    "        if name[-1] != '_':\n",
    "            name = name + '_'\n",
    "\n",
    "        # parse name for metadata\n",
    "        class_data_dict = {}          \n",
    "        \n",
    "        for cat, codes in code_category_dict.items():\n",
    "            class_data_dict[cat] = '-1'\n",
    "            for key, value in codes.items():\n",
    "                if cat == 'plant':\n",
    "                    if name[:8].lower()==key.lower():\n",
    "                        class_data_dict[cat] = value[0] + '_' + value[1]\n",
    "                else:\n",
    "                    if '_'+key+'_' in name:\n",
    "                        class_data_dict[cat] = value[1]\n",
    "                        #print(key, class_data_dict[cat])\n",
    "                        #print(class_data_dict)\n",
    "\n",
    "        #print(class_data_dict)\n",
    "\n",
    "        # for each spectrum in class_spectra, append to each list\n",
    "        for roi_spectrum in roi_class_spectra:\n",
    "\n",
    "            d_spectra.append(roi_spectrum)\n",
    "            \n",
    "            for key in yd_all_dict:\n",
    "                #print(key)\n",
    "                #print(len(yd_all_dict[key]))\n",
    "                #print(yd_all_dict[key])\n",
    "                yd_all_dict[key].append(class_data_dict[key])\n",
    "                #print(len(yd_all_dict[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13270, 272)\n",
      "plant (13270,)\n",
      "age (13270,)\n",
      "part (13270,)\n",
      "health (13270,)\n",
      "lifecycle (13270,)\n"
     ]
    }
   ],
   "source": [
    "d_spectra = np.asarray(d_spectra)\n",
    "print(d_spectra.shape)\n",
    "\n",
    "for key in yd_all_dict:\n",
    "    yd_all_dict[key] = np.asarray(yd_all_dict[key])\n",
    "    print(key, yd_all_dict[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select subset of ROI spectra and append to library spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New d_spectra shape: (250, 272)\n",
      "New plant shape: (250,)\n",
      "New age shape: (250,)\n",
      "New part shape: (250,)\n",
      "New health shape: (250,)\n",
      "New lifecycle shape: (250,)\n"
     ]
    }
   ],
   "source": [
    "num_to_select = 250  # Here 250 randomly selected ROI spectra are added to the full spectral library\n",
    "\n",
    "total_indices = d_spectra.shape[0]\n",
    "\n",
    "selected_indices = np.random.choice(total_indices, size=num_to_select, replace=False)\n",
    "\n",
    "selected_d_spectra = d_spectra[selected_indices]\n",
    "print(\"\\nNew d_spectra shape:\", selected_d_spectra.shape)\n",
    "\n",
    "selected_yd_all_dict = {}\n",
    "for key in yd_all_dict:\n",
    "    # Select the corresponding rows/elements from the original array\n",
    "    selected_yd_all_dict[key] = yd_all_dict[key][selected_indices]\n",
    "    print(f\"New {key} shape: {selected_yd_all_dict[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New spectra shape: (5984, 272)\n",
      "New sc.name shape: (5984,)\n",
      "New sc.age shape: (5984,)\n",
      "New sc.principle_part shape: (5984,)\n",
      "New sc.health shape: (5984,)\n",
      "New sc.lifecycle shape: (5984,)\n"
     ]
    }
   ],
   "source": [
    "spectra = np.concatenate((spectra, selected_d_spectra))\n",
    "\n",
    "sc.name = np.concatenate((sc.name, selected_yd_all_dict['plant']))\n",
    "sc.age = np.concatenate((sc.age, selected_yd_all_dict['age']))\n",
    "sc.principle_part = np.concatenate((sc.principle_part, selected_yd_all_dict['part']))\n",
    "sc.health = np.concatenate((sc.health, selected_yd_all_dict['health']))\n",
    "sc.lifecycle = np.concatenate((sc.lifecycle, selected_yd_all_dict['lifecycle']))\n",
    "print(f\"New spectra shape: {spectra.shape}\")\n",
    "print(f\"New sc.name shape: {sc.name.shape}\")\n",
    "print(f\"New sc.age shape: {sc.age.shape}\")\n",
    "print(f\"New sc.principle_part shape: {sc.principle_part.shape}\")\n",
    "print(f\"New sc.health shape: {sc.health.shape}\")\n",
    "print(f\"New sc.lifecycle shape: {sc.lifecycle.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_integer_labels(data_array, label_array):\n",
    "    \"\"\"\n",
    "    Assigns integer values from a label array to corresponding values in a data array.\n",
    "\n",
    "    Args:\n",
    "        data_array:  The array containing the string or categorical data (e.g., Yn_o, Ypp_o, etc.).\n",
    "        label_array: The array of unique labels (e.g., y_plant_labels, y_part_labels, etc.).\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array containing the integer representations of the data based on the labels.\n",
    "    \"\"\"\n",
    "    mapping = {label: i for i, label in enumerate(label_array)}\n",
    "    integer_array = np.array([mapping[x] for x in data_array])\n",
    "    return integer_array\n",
    "\n",
    "\n",
    "X_all = spectra\n",
    "\n",
    "y_plant_labels = np.unique(sc.name)\n",
    "y_part_labels = np.unique(sc.principle_part)\n",
    "y_health_labels = np.unique(sc.health)\n",
    "y_age_labels = np.unique(sc.age)\n",
    "y_lifecycle_labels = np.unique(sc.lifecycle)\n",
    "\n",
    "# Number of classes for each task\n",
    "n_plant_classes = len(y_plant_labels)\n",
    "n_age_classes = len(y_age_labels)\n",
    "n_part_classes = len(y_part_labels)\n",
    "n_health_classes = len(y_health_labels)\n",
    "n_lifecycle_classes = len(y_lifecycle_labels)\n",
    "\n",
    "Yn_int = assign_integer_labels(sc.name, y_plant_labels)\n",
    "Yp_int = assign_integer_labels(sc.principle_part, y_part_labels)\n",
    "Yh_int = assign_integer_labels(sc.health, y_health_labels)\n",
    "Ya_int = assign_integer_labels(sc.age, y_age_labels)\n",
    "Yl_int = assign_integer_labels(sc.lifecycle, y_lifecycle_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'N' not found in the label array.  No replacements made.\n"
     ]
    }
   ],
   "source": [
    "def replace_n_with_ignore_val(integer_array, label_array):\n",
    "    \"\"\"\n",
    "    Replaces integer values with -1 in an integer array where the corresponding\n",
    "    label in the label array is 'N'.\n",
    "\n",
    "    Args:\n",
    "        integer_array: The integer-encoded data array (e.g., Yn_int, Ypp_int, etc.).\n",
    "        label_array: The array of unique labels (e.g., y_plant_labels, y_part_labels, etc.).\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array with values replaced by -1 where the label is 'N'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        n_index = np.where(label_array == 'N')[0][0]  # Find the index of 'N'\n",
    "        integer_array[integer_array == n_index] = IGNORE_VALUE  # Replace values\n",
    "        return integer_array\n",
    "    except IndexError:\n",
    "        print(\"'N' not found in the label array.  No replacements made.\")\n",
    "        return integer_array\n",
    "\n",
    "\n",
    "y_plant = replace_n_with_ignore_val(Yn_int, y_plant_labels)\n",
    "y_part = replace_n_with_ignore_val(Yp_int, y_part_labels)\n",
    "y_age = replace_n_with_ignore_val(Ya_int, y_age_labels)\n",
    "y_health = replace_n_with_ignore_val(Yh_int, y_health_labels)\n",
    "\n",
    "y_lifecycle = Yl_int\n",
    "\n",
    "y_all_dict_original = {'plant': y_plant, 'part': y_part, 'age': y_age, 'health': y_health, 'lifecycle': y_lifecycle}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split sizes: Train=4188, Val=898, Test=898\n"
     ]
    }
   ],
   "source": [
    "# --- Data Splitting (Train/Validation/Test on features and ORIGINAL labels) ---\n",
    "indices = np.arange(len(X_all))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.15, random_state=42)\n",
    "train_indices, val_indices = train_test_split(train_indices, test_size=0.1765, random_state=42)\n",
    "X_train, X_val, X_test = X_all[train_indices], X_all[val_indices], X_all[test_indices]\n",
    "y_train_dict_orig = {task: y_all_dict_original[task][train_indices] for task in TASK_NAMES}\n",
    "y_val_dict_orig = {task: y_all_dict_original[task][val_indices] for task in TASK_NAMES}\n",
    "y_test_dict_orig = {task: y_all_dict_original[task][test_indices] for task in TASK_NAMES}\n",
    "print(f\"Data split sizes: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sc.spectra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of library spectra in test set:  867\n",
      "Number of ROI spectra in test set:  31\n"
     ]
    }
   ],
   "source": [
    "test_indices_library = test_indices[test_indices < len(sc.spectra)]\n",
    "print(\"Number of library spectra in test set: \", len(test_indices_library))\n",
    "test_indices_rois = test_indices[test_indices >= len(sc.spectra)]\n",
    "print(\"Number of ROI spectra in test set: \", len(test_indices_rois))\n",
    "\n",
    "X_test_library = X_all[test_indices_library]\n",
    "X_test_rois = X_all[test_indices_rois]\n",
    "\n",
    "y_test_dict_library = {task: y_all_dict_original[task][test_indices_library] for task in TASK_NAMES}\n",
    "y_test_dict_rois = {task: y_all_dict_original[task][test_indices_rois] for task in TASK_NAMES}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shapes: Train=(4188, 272, 1), Val=(898, 272, 1), Test=(898, 272, 1)\n",
      "Example Keras 'age_output' labels for training: [0 1 1 1 2 1 1 2 1 1 2 1 1 1 1 0 1 1 1 0]\n",
      "Example train sample weights for 'age_output': [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# --- Preprocessing (Standardization) ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Reshape for Conv1D: (batch, steps, channels=1) ---\n",
    "X_train_scaled = X_train_scaled[..., np.newaxis]\n",
    "X_val_scaled = X_val_scaled[..., np.newaxis]\n",
    "X_test_scaled = X_test_scaled[..., np.newaxis]\n",
    "#X_train_scaled = X_train[..., np.newaxis]\n",
    "#X_val_scaled = X_val[..., np.newaxis]\n",
    "#X_test_scaled = X_test[..., np.newaxis]\n",
    "\n",
    "print(f\"Feature shapes: Train={X_train_scaled.shape}, Val={X_val_scaled.shape}, Test={X_test_scaled.shape}\")\n",
    "\n",
    "# --- Prepare Labels for Keras Fit/Evaluate (Replace IGNORE_VALUE with 0) ---\n",
    "y_train_dict_keras = {f\"{task}_output\": np.maximum(0, y_train_dict_orig[task]) for task in TASK_NAMES}\n",
    "y_val_dict_keras = {f\"{task}_output\": np.maximum(0, y_val_dict_orig[task]) for task in TASK_NAMES}\n",
    "y_test_dict_keras = {f\"{task}_output\": np.maximum(0, y_test_dict_orig[task]) for task in TASK_NAMES}\n",
    "print(f\"Example Keras 'age_output' labels for training: {y_train_dict_keras['age_output'][:20]}\")\n",
    "\n",
    "# --- Create Sample Weight Dictionaries ---\n",
    "sample_weights_train = {f\"{task}_output\": (y_train_dict_orig[task] != IGNORE_VALUE).astype(np.float32) for task in TASK_NAMES}\n",
    "sample_weights_val = {f\"{task}_output\": (y_val_dict_orig[task] != IGNORE_VALUE).astype(np.float32) for task in TASK_NAMES}\n",
    "sample_weights_test = {f\"{task}_output\": (y_test_dict_orig[task] != IGNORE_VALUE).astype(np.float32) for task in TASK_NAMES}\n",
    "print(f\"Example train sample weights for 'age_output': {sample_weights_train['age_output'][:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_library_scaled = scaler.transform(X_test_library)\n",
    "X_test_library_scaled = X_test_library_scaled[..., np.newaxis]\n",
    "\n",
    "X_test_rois_scaled = scaler.transform(X_test_rois)\n",
    "X_test_rois_scaled = X_test_rois_scaled[..., np.newaxis]\n",
    "\n",
    "y_test_dict_library_keras = {f\"{task}_output\": np.maximum(0, y_test_dict_library[task]) for task in TASK_NAMES}\n",
    "y_test_dict_rois_keras = {f\"{task}_output\": np.maximum(0, y_test_dict_rois[task]) for task in TASK_NAMES}\n",
    "\n",
    "sample_weights_test_library = {f\"{task}_output\": (y_test_dict_library[task] != IGNORE_VALUE).astype(np.float32) for task in TASK_NAMES}\n",
    "sample_weights_test_rois = {f\"{task}_output\": (y_test_dict_rois[task] != IGNORE_VALUE).astype(np.float32) for task in TASK_NAMES}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a 1D CNN to capture local patterns (like absorption peaks or slopes across adjacent bands)\n",
    "\n",
    "def build_spectral_cnn(input_shape, n_plant, n_age, n_part, n_health, n_lifecycle):\n",
    "    inputs = keras.Input(shape=input_shape, name='spectrum_input')\n",
    "\n",
    "    # --- Shared Feature Extractor (1D CNN Backbone) ---\n",
    "    x = layers.Conv1D(filters=32, kernel_size=7, activation='relu', padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=3)(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    x = layers.Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=3)(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    x = layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=3)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    shared_features = layers.Flatten()(x)\n",
    "\n",
    "    # Optional shared dense layer\n",
    "    shared_features = layers.Dense(128, activation='relu')(shared_features)\n",
    "    shared_features = layers.BatchNormalization()(shared_features)\n",
    "    shared_features = layers.Dropout(0.5)(shared_features)\n",
    "\n",
    "    # --- Output Heads (One per task) ---\n",
    "    plant_output = layers.Dense(64, activation='relu')(shared_features)\n",
    "    plant_output = layers.Dense(n_plant, activation='softmax', name='plant_output')(plant_output)\n",
    "\n",
    "    age_output = layers.Dense(32, activation='relu')(shared_features)\n",
    "    age_output = layers.Dense(n_age, activation='softmax', name='age_output')(age_output)\n",
    "\n",
    "    part_output = layers.Dense(32, activation='relu')(shared_features)\n",
    "    part_output = layers.Dense(n_part, activation='softmax', name='part_output')(part_output)\n",
    "\n",
    "    health_output = layers.Dense(32, activation='relu')(shared_features)\n",
    "    health_output = layers.Dense(n_health, activation='softmax', name='health_output')(health_output)\n",
    "\n",
    "    lifecycle_output = layers.Dense(32, activation='relu')(shared_features)\n",
    "    lifecycle_output = layers.Dense(n_lifecycle, activation='softmax', name='lifecycle_output')(lifecycle_output)\n",
    "\n",
    "    # --- Build the Model ---\n",
    "    model = keras.Model(\n",
    "        inputs=inputs,\n",
    "        outputs={\n",
    "            'plant_output': plant_output,\n",
    "            'age_output': age_output,\n",
    "            'part_output': part_output,\n",
    "            'health_output': health_output,\n",
    "            'lifecycle_output': lifecycle_output\n",
    "        },\n",
    "        name=\"spectral_multi_task_cnn\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "input_shape = X_train_scaled.shape[1:]\n",
    "model = build_spectral_cnn(input_shape, n_plant_classes, n_age_classes, n_part_classes, n_health_classes, n_lifecycle_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Compiled.\n",
      "Losses: {'plant_output': 'sparse_categorical_crossentropy', 'age_output': 'sparse_categorical_crossentropy', 'part_output': 'sparse_categorical_crossentropy', 'health_output': 'sparse_categorical_crossentropy', 'lifecycle_output': 'sparse_categorical_crossentropy'}\n",
      "Metrics: []\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "losses = {name: 'sparse_categorical_crossentropy' for name in OUTPUT_NAMES}\n",
    "metrics = {name: 'sparse_categorical_accuracy' for name in OUTPUT_NAMES}\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=losses,\n",
    "    loss_weights=LOSS_WEIGHTS,\n",
    "    metrics=metrics, # Standard metrics\n",
    "    weighted_metrics=metrics # Weighted metrics\n",
    ")\n",
    "\n",
    "print(\"\\nModel Compiled.\")\n",
    "print(f\"Losses: {model.loss}\")\n",
    "print(f\"Metrics: {model.metrics_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load weights from a previous training checkpoint\n",
    "\n",
    "model.load_weights('best_spectral_model.weights_standard_dense_lib_resampled_5-16-3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NN if weights are not loaded in the previous step\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=PATIENCE,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_dict_keras,           # Labels dictionary (keys match output names)\n",
    "    sample_weight=sample_weights_train, # Sample weights dictionary (keys match output names)\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val_scaled, y_val_dict_keras, sample_weights_val), # Also pass val weights\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save weights after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "\n",
    "checkpoint_filepath = 'best_spectral_model.weights_standard_dense_lib_resampled_5-16-3.h5'\n",
    "\n",
    "model.save_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_maps = {\n",
    "    'plant': y_plant_labels,\n",
    "    'age': y_age_labels,\n",
    "    'part': y_part_labels,\n",
    "    'health': y_health_labels,\n",
    "    'lifecycle': y_lifecycle_labels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\nEvaluating on Test Set with Best Model (using sample weights)...\")\n",
    "\n",
    "results = model.evaluate(\n",
    "    X_test_scaled,\n",
    "    y_test_dict_keras,\n",
    "    sample_weight=sample_weights_test,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=0,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "print(f\"Overall Loss (Weighted Sum): {results['loss']:.4f}\")\n",
    "\n",
    "print(\"\\nTest Weighted Metrics (Accuracy ignoring invalid samples):\")\n",
    "for name in OUTPUT_NAMES:\n",
    "    metric_key = f\"{name}_weighted_sparse_categorical_accuracy\"\n",
    "    if metric_key in results:\n",
    "        print(f\"  {name.replace('_output', '').capitalize()}: {results[metric_key]:.4f}\")\n",
    "    else:\n",
    "         metric_key_alt = f\"weighted_{name}_sparse_categorical_accuracy\"\n",
    "         if metric_key_alt in results:\n",
    "              print(f\"  {name.replace('_output', '').capitalize()} (alt key): {results[metric_key_alt]:.4f}\")\n",
    "         else:\n",
    "              print(f\"  {name.replace('_output', '').capitalize()}: Weighted metric key not found in results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Test Set from Spectral Library only\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\nEvaluating on Test Set (Library) with Best Model (using sample weights)...\")\n",
    "\n",
    "results = model.evaluate(\n",
    "    X_test_library_scaled,\n",
    "    y_test_dict_library_keras,\n",
    "    sample_weight=sample_weights_test_library,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=0,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "print(f\"Overall Loss (Weighted Sum): {results['loss']:.4f}\")\n",
    "\n",
    "print(\"\\nTest Weighted Metrics (Accuracy ignoring invalid samples):\")\n",
    "for name in OUTPUT_NAMES:\n",
    "    metric_key = f\"{name}_weighted_sparse_categorical_accuracy\"\n",
    "    if metric_key in results:\n",
    "        print(f\"  {name.replace('_output', '').capitalize()}: {results[metric_key]:.4f}\")\n",
    "    else:\n",
    "         metric_key_alt = f\"weighted_{name}_sparse_categorical_accuracy\"\n",
    "         if metric_key_alt in results:\n",
    "              print(f\"  {name.replace('_output', '').capitalize()} (alt key): {results[metric_key_alt]:.4f}\")\n",
    "         else:\n",
    "              print(f\"  {name.replace('_output', '').capitalize()}: Weighted metric key not found in results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Test Set from ROIs only\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\nEvaluating on Test Set (ROIs) with Best Model (using sample weights)...\")\n",
    "\n",
    "results = model.evaluate(\n",
    "    X_test_rois_scaled,\n",
    "    y_test_dict_rois_keras,\n",
    "    sample_weight=sample_weights_test_rois,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=0,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "print(f\"Overall Loss (Weighted Sum): {results['loss']:.4f}\")\n",
    "\n",
    "print(\"\\nTest Weighted Metrics (Accuracy ignoring invalid samples):\")\n",
    "for name in OUTPUT_NAMES:\n",
    "    metric_key = f\"{name}_weighted_sparse_categorical_accuracy\"\n",
    "    if metric_key in results:\n",
    "        print(f\"  {name.replace('_output', '').capitalize()}: {results[metric_key]:.4f}\")\n",
    "    else:\n",
    "         metric_key_alt = f\"weighted_{name}_sparse_categorical_accuracy\"\n",
    "         if metric_key_alt in results:\n",
    "              print(f\"  {name.replace('_output', '').capitalize()} (alt key): {results[metric_key_alt]:.4f}\")\n",
    "         else:\n",
    "              print(f\"  {name.replace('_output', '').capitalize()}: Weighted metric key not found in results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Print Labeled Codes and Counts in Test Set for Each Species (checking for a balanced test set)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for i, val in enumerate(y_plant_labels):\n",
    "    \n",
    "    selected_indices = np.arange(len(y_test_dict_orig['plant']))\n",
    "    selected_indices = selected_indices[y_test_dict_orig['plant'] == i]\n",
    "\n",
    "    y_test_dict_keras_filtered = {key : y_test_dict_keras[key][selected_indices] for key in y_test_dict_keras.keys()}\n",
    "    sample_weights_test_filtered = {key : sample_weights_test[key][selected_indices] for key in sample_weights_test.keys()}\n",
    "\n",
    "    #print labels for values for all tasks of y_test_dict_keras['plant_output'][selected_indices]\n",
    "\n",
    "    #counts_dict = dict(zip(np.unique(y_test_dict_keras_filtered, return_counts=True)))\n",
    "\n",
    "    value_counts = {}\n",
    "\n",
    "    for key, arr in y_test_dict_keras_filtered.items():\n",
    "        value_counts[key] = Counter(arr)\n",
    "\n",
    "    print(\"Labeled Codes and Counts in Test Set for Each Species\")\n",
    "\n",
    "    for key, counts in value_counts.items():\n",
    "\n",
    "        key = key.replace('_output', '')\n",
    "        \n",
    "        print(f\" '{key}':\")\n",
    "        for value, count in counts.items():\n",
    "            print(f\"    {label_maps[key][value]}: {count} {'spectra' if key == 'plant' else ''}\")\n",
    "\n",
    "    results = model.evaluate(\n",
    "        X_test_scaled[selected_indices],\n",
    "        y_test_dict_keras_filtered,\n",
    "        sample_weight=sample_weights_test_filtered,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=0,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTest Set Evaluation Results for plant={val}:\")\n",
    "    print(f\"Overall Loss (Weighted Sum): {results['loss']:.4f}\")\n",
    "\n",
    "    print(\"\\nTest Weighted Metrics (Accuracy ignoring invalid samples):\")\n",
    "    for name in OUTPUT_NAMES:\n",
    "        metric_key = f\"{name}_weighted_sparse_categorical_accuracy\"\n",
    "        if metric_key in results:\n",
    "            print(f\"  {name.replace('_output', '').capitalize()}: {results[metric_key]:.4f}\")\n",
    "        else:\n",
    "            metric_key_alt = f\"weighted_{name}_sparse_categorical_accuracy\"\n",
    "            if metric_key_alt in results:\n",
    "                print(f\"  {name.replace('_output', '').capitalize()} (alt key): {results[metric_key_alt]:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {name.replace('_output', '').capitalize()}: Weighted metric key not found in results.\")\n",
    "    print(\"\\n____________________________________________________________\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Show confusion matrices for each task\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np # Ensure numpy is imported\n",
    "\n",
    "# --- Predictions on the Test Set ---\n",
    "print(\"\\nGenerating predictions on the test set...\")\n",
    "predictions = model.predict(X_test_scaled, batch_size=BATCH_SIZE)\n",
    "print(\"Predictions generated.\")\n",
    "\n",
    "# Ensure predictions is a dictionary if it's not already (Keras usually returns dict for multi-output)\n",
    "if not isinstance(predictions, dict):\n",
    "     # If the model output is a list, convert it back to a dict based on output names\n",
    "     output_layer_names = model.output_names # Or use OUTPUT_NAMES if they match exactly\n",
    "     predictions = dict(zip(output_layer_names, predictions))\n",
    "\n",
    "\n",
    "# # --- Create a mapping from task name to its labels ---\n",
    "# # Make sure these label arrays are accessible here\n",
    "# label_maps = {\n",
    "#     'plant': y_plant_labels,\n",
    "#     'age': y_age_labels,\n",
    "#     'part': y_part_labels,\n",
    "#     'health': y_health_labels,\n",
    "#     'lifecycle': y_lifecycle_labels\n",
    "# }\n",
    "\n",
    "# --- Generate and Plot Confusion Matrices ---\n",
    "print(\"\\nGenerating Confusion Matrices and Classification Reports...\")\n",
    "\n",
    "for task in TASK_NAMES:\n",
    "    output_name = f\"{task}_output\" # e.g., 'plant_output'\n",
    "\n",
    "    print(f\"\\n--- Task: {task.capitalize()} ---\")\n",
    "\n",
    "    # 1. Get True Labels (Original, includes integer mapping for 'N')\n",
    "    y_true_all = y_test_dict_orig[task]\n",
    "\n",
    "    # 2. Get Predicted Labels (Integers)\n",
    "    if output_name not in predictions:\n",
    "        print(f\"Warning: Output key '{output_name}' not found in model predictions. Skipping task '{task}'.\")\n",
    "        continue\n",
    "    y_pred_probs = predictions[output_name]\n",
    "    y_pred_all = np.argmax(y_pred_probs, axis=-1) # Get the class index with the highest probability\n",
    "\n",
    "    # 3. Get Sample Weights\n",
    "    weights = sample_weights_test[output_name]\n",
    "\n",
    "    # 4. Get the corresponding class names (labels)\n",
    "    original_labels = label_maps[task] # e.g., y_lifecycle_labels\n",
    "\n",
    "    # 5. *Conditional* Filtering and Label Definition\n",
    "    if task == 'lifecycle':\n",
    "        # --- Lifecycle Task: Include 'N' ---\n",
    "        # We don't filter samples based on weight, as weight=0 specifically marks 'N' here\n",
    "        # We want to see how 'N' is classified.\n",
    "        y_true_valid = y_true_all\n",
    "        y_pred_valid = y_pred_all\n",
    "\n",
    "        # Define the labels for the CM to include *all* original categories\n",
    "        # Note: y_true_valid might contain IGNORE_VALUE (-1) if 'N' was mapped to it.\n",
    "        # The confusion_matrix function handles this gracefully if -1 isn't in `labels`.\n",
    "        # So, we define `valid_indices` based on the original mapping.\n",
    "        valid_indices = list(range(len(original_labels))) # 0, 1, 2,... N_classes-1\n",
    "        valid_string_labels = list(original_labels) # Includes 'N' string\n",
    "\n",
    "        print(f\"Including 'N' category for {task.capitalize()}. Evaluating on {len(y_true_valid)} samples.\")\n",
    "\n",
    "    else:\n",
    "        # --- Other Tasks: Exclude 'N' ---\n",
    "        # Filter out ignored samples using the weights\n",
    "        valid_mask = (weights == 1.0) # Mask for samples that should NOT be ignored\n",
    "        y_true_valid = y_true_all[valid_mask]\n",
    "        y_pred_valid = y_pred_all[valid_mask]\n",
    "\n",
    "        # Define the labels for the CM *excluding* the 'N' category\n",
    "        try:\n",
    "            # Find the integer index originally assigned to 'N'\n",
    "            n_index = np.where(original_labels == 'N')[0][0]\n",
    "            valid_indices = [i for i in range(len(original_labels)) if i != n_index]\n",
    "            valid_string_labels = [label for i, label in enumerate(original_labels) if i != n_index]\n",
    "        except IndexError:\n",
    "             # 'N' category doesn't exist for this task, include all labels\n",
    "             print(f\"Note: 'N' category not found in labels for task '{task}'. Including all labels.\")\n",
    "             valid_indices = list(range(len(original_labels)))\n",
    "             valid_string_labels = list(original_labels)\n",
    "\n",
    "        print(f\"Excluding 'N' category for {task.capitalize()}. Evaluating on {len(y_true_valid)} samples.\")\n",
    "\n",
    "\n",
    "    # 6. Check if there are any valid samples left for this task\n",
    "    if len(y_true_valid) == 0:\n",
    "        print(f\"No samples to evaluate for task '{task}' after filtering (if applicable). Skipping CM.\")\n",
    "        continue\n",
    "\n",
    "    # 7. Determine the final set of labels/indices *present* in the data for the CM\n",
    "    # This is important because even if we define all labels, some might not\n",
    "    # appear in the specific y_true_valid/y_pred_valid subset.\n",
    "    present_true_labels = np.unique(y_true_valid)\n",
    "    present_pred_labels = np.unique(y_pred_valid)\n",
    "\n",
    "    # Combine present labels, but only keep those that were in our initial 'valid_indices' list\n",
    "    # (This prevents including the IGNORE_VALUE index (-1) if 'N' was mapped to it for lifecycle)\n",
    "    all_present_indices_in_data = np.unique(np.concatenate((present_true_labels, present_pred_labels)))\n",
    "    final_cm_indices = [idx for idx in valid_indices if idx in all_present_indices_in_data]\n",
    "    final_cm_labels = [label for idx, label in zip(valid_indices, valid_string_labels) if idx in final_cm_indices]\n",
    "\n",
    "\n",
    "    # Handle case where after filtering, no valid classes remain\n",
    "    if not final_cm_indices:\n",
    "         print(f\"No valid classes found in true/predicted labels for task '{task}' for CM. Skipping CM.\")\n",
    "         continue\n",
    "\n",
    "    # 8. Calculate Confusion Matrix using the *final* determined indices\n",
    "    # `labels=final_cm_indices` ensures the matrix axes match `final_cm_labels`\n",
    "    cm = confusion_matrix(y_true_valid, y_pred_valid, labels=final_cm_indices)\n",
    "\n",
    "    # 9. Plot Confusion Matrix\n",
    "    plt.figure(figsize=(max(8, len(final_cm_labels)*0.8), max(6, len(final_cm_labels)*0.6))) # Adjust size based on num labels\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=final_cm_labels,\n",
    "                yticklabels=final_cm_labels)\n",
    "    plt.title(f'Confusion Matrix - Task: {task.capitalize()}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 10. Print Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    try:\n",
    "        # Use zero_division=0 or 1 for handling classes with no true samples\n",
    "        report = classification_report(y_true_valid, y_pred_valid,\n",
    "                                   labels=final_cm_indices,\n",
    "                                   target_names=final_cm_labels,\n",
    "                                   zero_division=0)\n",
    "        print(report)\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not generate classification report for {task}: {e}\")\n",
    "        print(\"This might happen if predicted values contain labels not present in true values after filtering.\")\n",
    "        print(f\"Unique True values considered: {np.unique(y_true_valid)}\")\n",
    "        print(f\"Unique Pred values considered: {np.unique(y_pred_valid)}\")\n",
    "        print(f\"Indices used for report: {final_cm_indices}\")\n",
    "        print(f\"Labels used for report: {final_cm_labels}\")\n",
    "\n",
    "\n",
    "print(\"\\nFinished generating visualizations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function to read a single ASD file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import specdal\n",
    "def read(filepath, jump_correct = False):\n",
    "    # Reads a single ASD file with metadata.\n",
    "    \n",
    "    # check data\n",
    "    if filepath[-4:] != '.asd':\n",
    "        print(f'WARNING: File {fname} does not appear to be an ASD file.')\n",
    "        return -1\n",
    "    \n",
    "    # read the asd file with specdal and asdreader\n",
    "    s = specdal.Spectrum(filepath=filepath)\n",
    "    fname = os.path.basename(filepath)\n",
    "\n",
    "    if (jump_correct):\n",
    "        wl = s.measurement.index\n",
    "\n",
    "        # Fix 1: shift 0<wl<1000 range up/down to smooth jump at 1000\n",
    "        i1 = np.where(wl==1000)[0][0]\n",
    "        if not np.isnan(s.measurement.iloc[i1]):\n",
    "            dp = ( ((s.measurement.iloc[i1+1]-s.measurement.iloc[i1+2]) + (s.measurement.iloc[i1-1]-s.measurement.iloc[i1]))/2 )\n",
    "            d1 = (s.measurement.iloc[i1+1]-s.measurement.iloc[i1])\n",
    "            s.measurement.iloc[:(i1+1)] = s.measurement.iloc[:(i1+1)] + dp + d1\n",
    "        # Fix 2: shift 1800<wl<2500 range up/down to smooth jump at 1800\n",
    "        i2 = np.where(wl==1800)[0][0]\n",
    "        if not np.isnan(s.measurement.iloc[i2]):\n",
    "            dp = ( ((s.measurement.iloc[i2+1]-s.measurement.iloc[i2+2]) + (s.measurement.iloc[i2-1]-s.measurement.iloc[i2]))/2 )\n",
    "            d1 = (s.measurement.iloc[i2+1]-s.measurement.iloc[i2])\n",
    "            s.measurement.iloc[(i2+1):] = s.measurement.iloc[(i2+1):] - dp - d1\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference function to predict task classifications for one or more input spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_spectra(new_spectra, model, scaler, label_maps, task_names):\n",
    "    \"\"\"\n",
    "    Predicts classifications for multiple tasks for one or more input spectra.\n",
    "\n",
    "    Args:\n",
    "        new_spectra (np.ndarray): A NumPy array containing the spectrum/spectra.\n",
    "                                   Shape should be (num_bands,) for a single spectrum,\n",
    "                                   or (num_samples, num_bands) for multiple spectra.\n",
    "        model (tf.keras.Model): The trained Keras model.\n",
    "        scaler (sklearn.preprocessing.StandardScaler): The StandardScaler *already fitted*\n",
    "                                                      on the training data.\n",
    "        label_maps (dict): Dictionary mapping task names (e.g., 'plant') to their\n",
    "                           corresponding array of string labels (e.g., y_plant_labels).\n",
    "        task_names (list): List of task names (e.g., ['plant', 'age', ...]).\n",
    "\n",
    "    Returns:\n",
    "        list or dict:\n",
    "            - If a single spectrum was input: A dictionary where keys are task names\n",
    "              and values are the predicted string labels (e.g., {'plant': 'Rosa_rugosa', 'age': 'M', ...}).\n",
    "            - If multiple spectra were input: A list of dictionaries, where each\n",
    "              dictionary represents the predictions for one input spectrum.\n",
    "        None: If input shape is invalid.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the number of bands in new_spectra doesn't match the scaler.\n",
    "    \"\"\"\n",
    "    # --- Input Validation and Preparation ---\n",
    "    if not isinstance(new_spectra, np.ndarray):\n",
    "        new_spectra = np.array(new_spectra)\n",
    "\n",
    "    if new_spectra.ndim == 1:\n",
    "        # Single spectrum provided, reshape to (1, num_bands) for scaler and model\n",
    "        num_bands = new_spectra.shape[0]\n",
    "        spectra_batch = new_spectra.reshape(1, -1)\n",
    "        single_input = True\n",
    "    elif new_spectra.ndim == 2:\n",
    "        # Batch of spectra provided\n",
    "        num_bands = new_spectra.shape[1]\n",
    "        spectra_batch = new_spectra\n",
    "        single_input = False\n",
    "    else:\n",
    "        print(f\"Error: Input spectra must be 1D or 2D, but got {new_spectra.ndim} dimensions.\")\n",
    "        return None\n",
    "\n",
    "    # Check if number of bands matches the scaler\n",
    "    if num_bands != scaler.n_features_in_:\n",
    "        raise ValueError(f\"Input spectrum has {num_bands} bands, but the model/scaler \"\n",
    "                         f\"was trained with {scaler.n_features_in_} bands.\")\n",
    "\n",
    "    # --- Preprocessing ---\n",
    "    # 1. Scale using the *fitted* scaler\n",
    "    spectra_scaled = scaler.transform(spectra_batch)\n",
    "\n",
    "    # 2. Reshape for Conv1D input: (batch_size, steps=num_bands, channels=1)\n",
    "    spectra_reshaped = spectra_scaled[..., np.newaxis]\n",
    "\n",
    "    # --- Prediction ---\n",
    "    # Get raw probability outputs from the model\n",
    "    predictions_raw = model.predict(spectra_reshaped)\n",
    "    # Ensure predictions_raw is a dict (it should be for multi-output)\n",
    "    if not isinstance(predictions_raw, dict):\n",
    "         output_layer_names = model.output_names\n",
    "         predictions_raw = dict(zip(output_layer_names, predictions_raw))\n",
    "\n",
    "\n",
    "    # --- Output Processing ---\n",
    "    results = []\n",
    "    num_samples = spectra_reshaped.shape[0]\n",
    "\n",
    "    for i in range(num_samples): # Loop through each spectrum in the batch\n",
    "        sample_predictions = {}\n",
    "        for task in task_names:\n",
    "            output_name = f\"{task}_output\" # e.g., 'plant_output'\n",
    "\n",
    "            if output_name not in predictions_raw:\n",
    "                 print(f\"Warning: Output key '{output_name}' not found in model predictions for task '{task}'. Skipping.\")\n",
    "                 sample_predictions[task] = \"Error: Output not found\"\n",
    "                 continue\n",
    "\n",
    "            # Get probabilities for the current task and current sample\n",
    "            task_probs = predictions_raw[output_name][i]\n",
    "\n",
    "            # Find the index of the highest probability\n",
    "            predicted_index = np.argmax(task_probs)\n",
    "\n",
    "            # Convert index back to string label\n",
    "            try:\n",
    "                predicted_label = label_maps[task][predicted_index]\n",
    "            except IndexError:\n",
    "                predicted_label = f\"Error: Index {predicted_index} out of bounds for task '{task}' labels\"\n",
    "            except KeyError:\n",
    "                predicted_label = f\"Error: Task '{task}' not found in label_maps\"\n",
    "\n",
    "            sample_predictions[task] = predicted_label\n",
    "            # Optional: Add the probability of the predicted class\n",
    "            sample_predictions[f\"{task}_probability\"] = float(task_probs[predicted_index])\n",
    "\n",
    "        results.append(sample_predictions)\n",
    "\n",
    "    # Return a single dict if only one spectrum was input, otherwise the list\n",
    "    return results[0] if single_input else results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage ---\n",
    "\n",
    "# Make sure your trained `model`, fitted `scaler`, `label_maps`, and `task_names`\n",
    "# are available in the current scope.\n",
    "\n",
    "# Example 1: Predict a single spectrum from the test set\n",
    "sample_index = 0 # Choose a sample from your test set\n",
    "single_spectrum_original = X_test[sample_index] # Get the *original*, unscaled data\n",
    "\n",
    "print(f\"Predicting for a single spectrum (Sample {sample_index} from original test set)...\")\n",
    "prediction_single = predict_spectra(single_spectrum_original, model, scaler, label_maps, TASK_NAMES)\n",
    "print(\"\\nPrediction Results (Single):\")\n",
    "import json # For pretty printing the dictionary\n",
    "#print(json.dumps(prediction_single, indent=2))\n",
    "print(prediction_single)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Example 2: Predict a small batch of spectra from the test set\n",
    "batch_spectra_original = X_test[5:8] # Get samples 5, 6, 7 (original, unscaled)\n",
    "\n",
    "print(f\"\\nPredicting for a batch of {len(batch_spectra_original)} spectra...\")\n",
    "predictions_batch = predict_spectra(batch_spectra_original, model, scaler, label_maps, TASK_NAMES)\n",
    "print(\"\\nPrediction Results (Batch):\")\n",
    "for i, pred in enumerate(predictions_batch):\n",
    "    print(f\"\\n--- Spectrum {i+1} ---\")\n",
    "    print(json.dumps(pred, indent=2))\n",
    "\n",
    "# Example 3: Using dummy data (ensure it has the correct number of bands)\n",
    "# num_bands_expected = scaler.n_features_in_\n",
    "# dummy_spectrum = np.random.rand(num_bands_expected) * 0.5 + 0.1 # Example random spectrum\n",
    "\n",
    "# print(\"\\nPredicting for a dummy spectrum...\")\n",
    "# prediction_dummy = predict_spectra(dummy_spectrum, model, scaler, label_maps, TASK_NAMES)\n",
    "# print(\"\\nPrediction Results (Dummy):\")\n",
    "# print(json.dumps(prediction_dummy, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the labeled data for the test spectrum\n",
    "for key, value in y_test_dict_orig.items():\n",
    "    if value.any():  # Check if the list is not empty\n",
    "        print(f\"{key}: {label_maps[key][value[sample_index]]}\")\n",
    "    else:\n",
    "        print(f\"{key} has an empty list.\")\n",
    "\n",
    "plt.plot(wl, X_test[sample_index], lw=1)\n",
    "plt.ylim(0, 1) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function to build an array of ASD spectra for batch prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def get_file_paths_single_level(directory):\n",
    "    file_paths = []\n",
    "    for entry in os.listdir(directory):\n",
    "        full_path = os.path.join(directory, entry)\n",
    "        if os.path.isfile(full_path):\n",
    "            file_paths.append(full_path)\n",
    "    return file_paths\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"/workspaces/NN_Hyperspectral_Vegetation/data/asd\"\n",
    "files = get_file_paths_single_level(directory_path)\n",
    "\n",
    "spectra_array = [read(filepath, True).measurement for filepath in files]\n",
    "\n",
    "spectra_array = np.asarray(spectra_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nPredicting for a batch of {len(spectra_array)} spectra...\")\n",
    "predictions_batch = predict_spectra(spectra_array, model, scaler, label_maps, TASK_NAMES)\n",
    "print(\"\\nPrediction Results (Batch):\")\n",
    "for i, pred in enumerate(predictions_batch):\n",
    "    print(f\"\\n--- Spectrum {i} ---\")\n",
    "    print(f\"Filepath: {files[i]}\")\n",
    "    plt.plot(wl, spectra_array[i], label=f'{files[i]}', lw=1)\n",
    "    plt.ylim(0, 1) \n",
    "    plt.show()\n",
    "    print(json.dumps(pred, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "fname_roi = 'data/pkl/ROIs_4-25_Ilex_vom.pkl'\n",
    "\n",
    "# Unpickling the dictionary\n",
    "with open(fname_roi, 'rb') as f:\n",
    "    roiData = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "fname_roi = 'data/pkl/ROI_Soli_sem.pkl'\n",
    "\n",
    "# Unpickling the dictionary\n",
    "with open(fname_roi, 'rb') as f:\n",
    "    roiData = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "fname_roi = 'data/pkl/ROIs_4-10.pkl'\n",
    "\n",
    "# Unpickling the dictionary\n",
    "with open(fname_roi, 'rb') as f:\n",
    "    roiData = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = roiData.df # a DataFrame holding all the data for the ROIs\n",
    "roi_names = roiData.names # the names of the ROIs\n",
    "roi_colors = roiData.colors # a Python dictionary of colors, where colors[class_name] will provide the color for class class_name\n",
    "roi_masks = roiData.masks # a Python dictionary of location masks, where colors[class_name] will provide the numpy array for the mask\n",
    "nClasses = len(roi_names)\n",
    "\n",
    "# Notes:\n",
    "#   Each mask is an array of bollean values of the same dimensions as the image, \n",
    "#   and True represents a pixel in the ROI while False represents not in the ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine how many rows of images (assuming each row has 2 masks)\n",
    "nRowsPlots = int(np.ceil(nClasses/2)) # (change size of figure as needed)\n",
    "plt.figure(figsize=(8, nRowsPlots*2))\n",
    "for i in range(nClasses):\n",
    "    name = roi_names[i]\n",
    "    # create the subplot\n",
    "    plt.subplot(nRowsPlots, 2, i+1)    \n",
    "    plt.imshow(roi_masks[name])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(f'ROI Mask for {name}');\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the locations\n",
    "# create an RGB image\n",
    "RGB_image_with_ROIs = msf.make_rgb(im.Arr, wl_img, stretch = [2,98])\n",
    "# Add the ROI locations\n",
    "for name in roi_names:\n",
    "    # get the color for this ROI, converted from hex to (r,g,b)\n",
    "    color = colors.to_rgb(roi_colors[name])\n",
    "    mask = roi_masks[name]\n",
    "    # set the pixel values for this ROI to (0,0,0)\n",
    "    for i in range(3):\n",
    "        RGB_image_with_ROIs[:,:,i] = RGB_image_with_ROIs[:,:,i]*(mask==False)\n",
    "    # set the pixel values for this ROI to the color for this ROI\n",
    "    for i in range(3):\n",
    "        RGB_image_with_ROIs[:,:,i] = RGB_image_with_ROIs[:,:,i] + mask*color[i]\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(RGB_image_with_ROIs)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(os.path.basename(fname));\n",
    "# Create legend patches: each patch is a colored box with the ROI name.\n",
    "legend_patches = [mpatches.Patch(color=roi_colors[name], label=name) for name in roi_names]\n",
    "# Add the legend to the right of the image\n",
    "plt.legend(handles=legend_patches, loc='center left', bbox_to_anchor=(1, 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_img = df.to_numpy()[:,4:] # an array that holds all the spectra (each row is a spectrum)\n",
    "spectra_img = spectra_img.astype(np.float32)\n",
    "spectra_img_names = df['Name'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(spectra_img_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean (red) and all individual spectra (blue) of each class\n",
    "# we save the mean for each class for use later\n",
    "means = {}\n",
    "for name in roi_names:\n",
    "    class_spectra = spectra_img[spectra_img_names==name,:]\n",
    "    class_nSpec = class_spectra.shape[0]\n",
    "    plt.figure(figsize=(10,6)) \n",
    "    for i in range(class_nSpec):\n",
    "        plt.plot(wl_img,class_spectra[i,:].flatten(), color='b', alpha=0.05)\n",
    "    means[name] = np.mean(class_spectra, axis=0)\n",
    "    plt.plot(wl_img, means[name], color='r')\n",
    "    plt.minorticks_on()\n",
    "    plt.grid(True, which='major', c='k')\n",
    "    plt.grid(True, which='minor', linewidth=0.25)\n",
    "    plt.title(f'ROI Spectra for {name}')\n",
    "    plt.xlabel('Wavelength (nm)')\n",
    "    plt.ylabel('Reflectance')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean (red) and all individual spectra (blue) of each class, scaling all spectra to have mean value of 1\n",
    "for name in roi_names:\n",
    "    class_spectra = spectra_img[spectra_img_names==name,:]\n",
    "    class_nSpec = class_spectra.shape[0]\n",
    "    plt.figure(figsize=(10,6)) \n",
    "    for i in range(class_nSpec):\n",
    "        plt.plot(wl_img,class_spectra[i,:].flatten()/np.mean(class_spectra[i,:].flatten()), color='b', alpha=0.05)\n",
    "plt.minorticks_on()\n",
    "plt.grid(True, which='major', c='k')\n",
    "plt.grid(True, which='minor', linewidth=0.25)\n",
    "plt.plot(wl_img, means[name]/np.mean(means[name]), color='r')\n",
    "plt.title(f'ROI Spectra for {name}, Normalized to Mean=1')\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Reflectance')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the means together\n",
    "plt.figure(figsize=(10,6)) \n",
    "for name in roi_names:\n",
    "    plt.plot(wl_img, means[name], label=name, color=roi_colors[name])\n",
    "plt.minorticks_on()\n",
    "plt.grid(True, which='major', c='k')\n",
    "plt.grid(True, which='minor', linewidth=0.25)\n",
    "plt.title(f'ROI Spectra for {name}')\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Reflectance')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the means together\n",
    "plt.figure(figsize=(10,6)) \n",
    "for name in roi_names:\n",
    "    plt.plot(wl_img, means[name]/np.mean(means[name]), label=name, color=roi_colors[name])\n",
    "plt.title(f'ROI Spectra for {name}, Normalized to Mean=1')\n",
    "plt.minorticks_on()\n",
    "plt.grid(True, which='major', c='k')\n",
    "plt.grid(True, which='minor', linewidth=0.25)\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Reflectance')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spectra_img.shape)\n",
    "print(spectra_img_names.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing predict on ROI spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "img_test_spec = means['Soli_sem']\n",
    "\n",
    "print(f\"Predicting for a single spectrum...\")\n",
    "prediction_single = predict_spectra(img_test_spec, model, scaler, label_maps, TASK_NAMES)\n",
    "print(\"\\nPrediction Results (Single):\")\n",
    "import json # For pretty printing the dictionary\n",
    "#print(json.dumps(prediction_single, indent=2))\n",
    "print(prediction_single)\n",
    "\n",
    "plt.plot(wl_img, img_test_spec, lw=1)\n",
    "plt.ylim(0, 1) \n",
    "plt.show()\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spectra_img_names[2069])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spectra_img[2069])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on an individual ROI pixel/spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with and without smoothing\n",
    "\n",
    "import warnings\n",
    "import scipy\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# parameters for smoothing the neurons\n",
    "sigma = 3\n",
    "window = 3\n",
    "\n",
    "\n",
    "img_test_spec1 = spectra_img[2069]\n",
    "\n",
    "#smooth_spectra1 = scipy.ndimage.gaussian_filter1d(img_test_spec1, sigma)\n",
    "\n",
    "#smooth_spectra1 = scipy.signal.savgol_filter(smooth_spectra1, window, 2)\n",
    "#smooth_spectra = scipy.signal.savgol_filter(neuronACt, window, 3)\n",
    "\n",
    "print(f\"Predicting for a single spectrum...\")\n",
    "prediction_single = predict_spectra(img_test_spec1, model, scaler, label_maps, TASK_NAMES)\n",
    "print(\"\\nPrediction Results (Single):\")\n",
    "import json # For pretty printing the dictionary\n",
    "#print(json.dumps(prediction_single, indent=2))\n",
    "print(prediction_single)\n",
    "\n",
    "plt.plot(wl_img, img_test_spec1, lw=1)\n",
    "plt.ylim(0, 1) \n",
    "plt.show()\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spectra_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on a batch of ROI spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with and without smoothing\n",
    "\n",
    "import scipy\n",
    "\n",
    "# parameters for smoothing the neurons\n",
    "sigma = 1.5\n",
    "window = 3\n",
    "\n",
    "print(roi_names)\n",
    "print(means)\n",
    "\n",
    "batch_ROI_means = np.empty([len(roi_names), len(wl_img)])\n",
    "for i, name in enumerate(roi_names):\n",
    "\n",
    "    smooth_mean = scipy.ndimage.gaussian_filter1d(means[name], sigma)\n",
    "    #smooth_mean = scipy.signal.savgol_filter(smooth_mean, window, 2)\n",
    "\n",
    "    #batch_ROI_means[i] = means[name]\n",
    "    batch_ROI_means[i] = smooth_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"\\nPredicting for a batch of {len(batch_ROI_means)} spectra...\")\n",
    "predictions_batch = predict_spectra(batch_ROI_means, model, scaler, label_maps, TASK_NAMES)\n",
    "\n",
    "# \n",
    "for i, pred in enumerate(predictions_batch):\n",
    "    print(f\"\\n--- Spectrum {i} ---\")\n",
    "    plt.plot(wl_img, batch_ROI_means[i], lw=1)\n",
    "    plt.ylim(0, 1) \n",
    "    plt.show()\n",
    "    print(json.dumps(pred, indent=2))\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
